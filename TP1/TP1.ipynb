{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2270,
     "status": "ok",
     "timestamp": 1573606004254,
     "user": {
      "displayName": "Benoit FrÃ©dÃ©ric-Moreau",
      "photoUrl": "",
      "userId": "09939474294080590706"
     },
     "user_tz": -60
    },
    "id": "XfkBZ2f9J_20",
    "outputId": "287a12b2-840a-4b29-c37f-b48b495e2c4e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/beuvry_j/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "#from google.colab import drive\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import LSTM, Dropout, Dense, Bidirectional,  Flatten, Input, GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "from unidecode import unidecode\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords=stopwords.words('english')\n",
    "\n",
    "T = TweetTokenizer()\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "#!wget -P /root/input/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
    "EMBEDDING_FILE = '../GoogleNews-vectors-negative300.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tpo2LX6hJ_vg"
   },
   "outputs": [],
   "source": [
    "def value_sentiment(x):\n",
    "  if x == \"neutral\":\n",
    "    return 0\n",
    "  if x == \"positive\":\n",
    "    return 1\n",
    "  if x == \"negative\":\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m6d-_t2NKvQp"
   },
   "outputs": [],
   "source": [
    "def onehot (tweets,length):\n",
    "  onh = []\n",
    "  for tweet in tweets:\n",
    "      onh_seq = np.zeros(length) \n",
    "      for i in tweet:\n",
    "        onh_seq[i -1] = 1\n",
    "      onh.append(onh_seq)\n",
    "  return onh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TlSZY2t6J_gj"
   },
   "outputs": [],
   "source": [
    "#replace emojy\n",
    "emojis = {\n",
    "    u\":â€‘\\)\":\"â˜ºï¸\",\n",
    "    u\":\\)\":\"â˜ºï¸\",\n",
    "    u\":-\\]\":\"â˜ºï¸\",\n",
    "    u\":\\]\":\"â˜ºï¸\",\n",
    "    u\":-3\":\"â˜ºï¸\",\n",
    "    u\":3\":\"â˜ºï¸\",\n",
    "    u\":->\":\"â˜ºï¸\",\n",
    "    u\":>\":\"â˜ºï¸\",\n",
    "    u\"8-\\)\":\"â˜ºï¸\",\n",
    "    u\":o\\)\":\"â˜ºï¸\",\n",
    "    u\":-\\}\":\"â˜ºï¸\",\n",
    "    u\":\\}\":\"â˜ºï¸\",\n",
    "    u\":-\\)\":\"â˜ºï¸\",\n",
    "    u\":c\\)\":\"â˜ºï¸\",\n",
    "    u\":\\^\\)\":\"â˜ºï¸\",\n",
    "    u\"=\\]\":\"â˜ºï¸\",\n",
    "    u\"=\\)\":\"â˜ºï¸\",\n",
    "    u\":â€‘D\":\"ğŸ˜ƒ\",\n",
    "    u\":D\":\"ğŸ˜ƒ\",\n",
    "    u\"8â€‘D\":\"ğŸ˜ƒ\",\n",
    "    u\"8D\":\"ğŸ˜ƒ\",\n",
    "    u\"Xâ€‘D\":\"ğŸ˜ƒ\",\n",
    "    u\"XD\":\"ğŸ˜ƒ\",\n",
    "    u\"=D\":\"ğŸ˜ƒ\",\n",
    "    u\"=3\":\"ğŸ˜ƒ\",\n",
    "    u\"B\\^D\":\"ğŸ˜ƒ\",\n",
    "    u\":-\\)\\)\":\"ğŸ˜ƒ\",\n",
    "    u\":â€‘\\(\":\"â˜¹ï¸\",\n",
    "    u\":-\\(\":\"â˜¹ï¸\",\n",
    "    u\":\\(\":\"â˜¹ï¸\",\n",
    "    u\":â€‘c\":\"â˜¹ï¸\",\n",
    "    u\":c\":\"â˜¹ï¸\",\n",
    "    u\":â€‘<\":\"â˜¹ï¸\",\n",
    "    u\":<\":\"â˜¹ï¸\",\n",
    "    u\":â€‘\\[\":\"â˜¹ï¸\",\n",
    "    u\":\\[\":\"â˜¹ï¸\",\n",
    "    u\":-\\|\\|\":\"â˜¹ï¸\",\n",
    "    u\">:\\[\":\"â˜¹ï¸\",\n",
    "    u\":\\{\":\"â˜¹ï¸\",\n",
    "    u\":@\":\"â˜¹ï¸\",\n",
    "    u\">:\\(\":\"â˜¹ï¸\",\n",
    "    u\":'â€‘\\(\":\"ğŸ˜­\",\n",
    "    u\":'\\(\":\"ğŸ˜­\",\n",
    "    u\":'â€‘\\)\":\"ğŸ˜ƒ\",\n",
    "    u\":'\\)\":\"ğŸ˜ƒ\",\n",
    "    u\"Dâ€‘':\":\"ğŸ˜¨\",\n",
    "    u\"D:<\":\"ğŸ˜¨\",\n",
    "    u\"D:\":\"ğŸ˜§\",\n",
    "    u\"D8\":\"ğŸ˜§\",\n",
    "    u\"D;\":\"ğŸ˜§\",\n",
    "    u\"D=\":\"ğŸ˜§\",\n",
    "    u\"DX\":\"ğŸ˜§\",\n",
    "    u\":â€‘O\":\"ğŸ˜®\",\n",
    "    u\":O\":\"ğŸ˜®\",\n",
    "    u\":â€‘o\":\"ğŸ˜®\",\n",
    "    u\":o\":\"ğŸ˜®\",\n",
    "    u\":-0\":\"ğŸ˜®\",\n",
    "    u\"8â€‘0\":\"ğŸ˜®\",\n",
    "    u\">:O\":\"ğŸ˜®\",\n",
    "    u\":-\\*\":\"ğŸ˜—\",\n",
    "    u\":\\*\":\"ğŸ˜—\",\n",
    "    u\":X\":\"ğŸ˜—\",\n",
    "    u\";â€‘\\)\":\"ğŸ˜‰\",\n",
    "    u\";\\)\":\"ğŸ˜‰\",\n",
    "    u\"\\*-\\)\":\"ğŸ˜‰\",\n",
    "    u\"\\*\\)\":\"ğŸ˜‰\",\n",
    "    u\";â€‘\\]\":\"ğŸ˜‰\",\n",
    "    u\";\\]\":\"ğŸ˜‰\",\n",
    "    u\";\\^\\)\":\"ğŸ˜‰\",\n",
    "    u\":â€‘,\":\"ğŸ˜‰\",\n",
    "    u\";D\":\"ğŸ˜‰\",\n",
    "    u\":â€‘P\":\"ğŸ˜›\",\n",
    "    u\":P\":\"ğŸ˜›\",\n",
    "    u\"Xâ€‘P\":\"ğŸ˜›\",\n",
    "    u\"XP\":\"ğŸ˜›\",\n",
    "    u\":â€‘Ã\":\"ğŸ˜›\",\n",
    "    u\":Ã\":\"ğŸ˜›\",\n",
    "    u\":b\":\"ğŸ˜›\",\n",
    "    u\"d:\":\"ğŸ˜›\",\n",
    "    u\"=p\":\"ğŸ˜›\",\n",
    "    u\">:P\":\"ğŸ˜›\",\n",
    "    u\":â€‘/\":\"ğŸ˜•\",\n",
    "    u\":/\":\"ğŸ˜•\",\n",
    "    u\":-[.]\":\"ğŸ˜•\",\n",
    "    u\">:[(\\\\\\)]\":\"ğŸ˜•\",\n",
    "    u\">:/\":\"ğŸ˜•\",\n",
    "    u\":[(\\\\\\)]\":\"ğŸ˜•\",\n",
    "    u\"=/\":\"ğŸ˜•\",\n",
    "    u\"=[(\\\\\\)]\":\"ğŸ˜•\",\n",
    "    u\":L\":\"ğŸ˜•\",\n",
    "    u\"=L\":\"ğŸ˜•\",\n",
    "    u\":S\":\"ğŸ˜•\",\n",
    "    u\":â€‘\\|\":\"ğŸ˜\",\n",
    "    u\":\\|\":\"ğŸ˜\",\n",
    "    u\":$\":\"ğŸ˜³\",\n",
    "    u\":â€‘x\":\"ğŸ¤\",\n",
    "    u\":x\":\"ğŸ¤\",\n",
    "    u\":â€‘#\":\"ğŸ¤\",\n",
    "    u\":#\":\"ğŸ¤\",\n",
    "    u\":â€‘&\":\"ğŸ¤\",\n",
    "    u\":&\":\"ğŸ¤\",\n",
    "    u\"O:â€‘\\)\":\"ğŸ˜‡\",\n",
    "    u\"O:\\)\":\"ğŸ˜‡\",\n",
    "    u\"0:â€‘3\":\"ğŸ˜‡\",\n",
    "    u\"0:3\":\"ğŸ˜‡\",\n",
    "    u\"0:â€‘\\)\":\"ğŸ˜‡\",\n",
    "    u\"0:\\)\":\"ğŸ˜‡\",\n",
    "    u\":â€‘b\":\"ğŸ˜›\",\n",
    "    u\"0;\\^\\)\":\"ğŸ˜‡\",\n",
    "    u\">:â€‘\\)\":\"ğŸ˜ˆ\",\n",
    "    u\">:\\)\":\"ğŸ˜ˆ\",\n",
    "    u\"\\}:â€‘\\)\":\"ğŸ˜ˆ\",\n",
    "    u\"\\}:\\)\":\"ğŸ˜ˆ\",\n",
    "    u\"3:â€‘\\)\":\"ğŸ˜ˆ\",\n",
    "    u\"3:\\)\":\"ğŸ˜ˆ\",\n",
    "    u\">;\\)\":\"ğŸ˜ˆ\",\n",
    "    u\"\\|;â€‘\\)\":\"ğŸ˜\",\n",
    "    u\"\\|â€‘O\":\"ğŸ˜\",\n",
    "    u\":â€‘J\":\"ğŸ˜\",\n",
    "    u\"%â€‘\\)\":\"ğŸ˜µ\",\n",
    "    u\"%\\)\":\"ğŸ˜µ\",\n",
    "    u\":-###..\":\"ğŸ¤’\",\n",
    "    u\":###..\":\"ğŸ¤’\",\n",
    "    u\"\\(>_<\\)\":\"ğŸ˜£\",\n",
    "    u\"\\(>_<\\)>\":\"ğŸ˜£\",\n",
    "    u\"\\(';'\\)\":\"ğŸ‘¶\",\n",
    "    u\"\\(\\^\\^>``\":\"ğŸ˜“\",\n",
    "    u\"\\(\\^_\\^;\\)\":\"ğŸ˜“\",\n",
    "    u\"\\(-_-;\\)\":\"ğŸ˜“\",\n",
    "    u\"\\(~_~;\\) \\(ãƒ»\\.ãƒ»;\\)\":\"ğŸ˜“\",\n",
    "    u\"\\(-_-\\)zzz\":\"ğŸ˜´\",\n",
    "    u\"\\(\\^_-\\)\":\"ğŸ˜‰\",\n",
    "    u\"\\(\\(\\+_\\+\\)\\)\":\"ğŸ˜•\",\n",
    "    u\"\\(\\+o\\+\\)\":\"ğŸ˜•\",\n",
    "    u\"\\^_\\^\":\"ğŸ˜ƒ\",\n",
    "    u\"\\(\\^_\\^\\)/\":\"ğŸ˜ƒ\",\n",
    "    u\"\\(\\^O\\^\\)ï¼\":\"ğŸ˜ƒ\",\n",
    "    u\"\\(\\^o\\^\\)ï¼\":\"ğŸ˜ƒ\",\n",
    "    u\"\\(__\\)\":\"ğŸ™‡\",\n",
    "    u\"_\\(\\._\\.\\)_\":\"ğŸ™‡\",\n",
    "    u\"<\\(_ _\\)>\":\"ğŸ™‡\",\n",
    "    u\"<m\\(__\\)m>\":\"ğŸ™‡\",\n",
    "    u\"m\\(__\\)m\":\"ğŸ™‡\",\n",
    "    u\"m\\(_ _\\)m\":\"ğŸ™‡\",\n",
    "    u\"\\('_'\\)\":\"ğŸ˜­\",\n",
    "    u\"\\(/_;\\)\":\"ğŸ˜­\",\n",
    "    u\"\\(T_T\\) \\(;_;\\)\":\"ğŸ˜­\",\n",
    "    u\"\\(;_;\":\"ğŸ˜­\",\n",
    "    u\"\\(;_:\\)\":\"ğŸ˜­\",\n",
    "    u\"\\(;O;\\)\":\"ğŸ˜­\",\n",
    "    u\"\\(:_;\\)\":\"ğŸ˜­\",\n",
    "    u\"\\(ToT\\)\":\"ğŸ˜­\",\n",
    "    u\";_;\":\"ğŸ˜­\",\n",
    "    u\";-;\":\"ğŸ˜­\",\n",
    "    u\";n;\":\"ğŸ˜­\",\n",
    "    u\";;\":\"ğŸ˜­\",\n",
    "    u\"Q\\.Q\":\"ğŸ˜­\",\n",
    "    u\"T\\.T\":\"ğŸ˜­\",\n",
    "    u\"QQ\":\"ğŸ˜­\",\n",
    "    u\"Q_Q\":\"ğŸ˜­\",\n",
    "    u\"\\(-\\.-\\)\":\"ğŸ˜\",\n",
    "    u\"\\(-_-\\)\":\"ğŸ˜\",\n",
    "    u\"\\(ä¸€ä¸€\\)\":\"ğŸ˜\",\n",
    "    u\"\\(ï¼›ä¸€_ä¸€\\)\":\"ğŸ˜\",\n",
    "    u\"\\(=_=\\)\":\"ğŸ˜©\",\n",
    "    u\"\\(=\\^\\Â·\\^=\\)\":\"ğŸ˜º\",\n",
    "    u\"\\(=\\^\\Â·\\Â·\\^=\\)\":\"ğŸ˜º\",\n",
    "    u\"=_\\^=\t\":\"ğŸ˜º\",\n",
    "    u\"\\(\\.\\.\\)\":\"ğŸ˜”\",\n",
    "    u\"\\(\\._\\.\\)\":\"ğŸ˜”\",\n",
    "    u\"\\(\\ãƒ»\\ãƒ»?\":\"ğŸ˜•\",\n",
    "    u\"\\(?_?\\)\":\"ğŸ˜•\",\n",
    "    u\">\\^_\\^<\":\"ğŸ˜ƒ\",\n",
    "    u\"<\\^!\\^>\":\"ğŸ˜ƒ\",\n",
    "    u\"\\^/\\^\":\"ğŸ˜ƒ\",\n",
    "    u\"\\ï¼ˆ\\*\\^_\\^\\*ï¼‰\" :\"ğŸ˜ƒ\",\n",
    "    u\"\\(\\^<\\^\\) \\(\\^\\.\\^\\)\":\"ğŸ˜ƒ\",\n",
    "    u\"\\(^\\^\\)\":\"ğŸ˜ƒ\",\n",
    "    u\"\\(\\^\\.\\^\\)\":\"ğŸ˜ƒ\",\n",
    "    u\"\\(\\^_\\^\\.\\)\":\"ğŸ˜ƒ\",\n",
    "    u\"\\(\\^_\\^\\)\":\"ğŸ˜ƒ\",\n",
    "    u\"\\(\\^\\^\\)\":\"ğŸ˜ƒ\",\n",
    "    u\"\\(\\^J\\^\\)\":\"ğŸ˜ƒ\",\n",
    "    u\"\\(\\*\\^\\.\\^\\*\\)\":\"ğŸ˜ƒ\",\n",
    "    u\"\\(\\^â€”\\^\\ï¼‰\":\"ğŸ˜ƒ\",\n",
    "    u\"\\(#\\^\\.\\^#\\)\":\"ğŸ˜ƒ\",\n",
    "    u\"\\ï¼ˆ\\^â€”\\^\\ï¼‰\":\"ğŸ‘‹\",\n",
    "    u\"\\(;_;\\)/~~~\":\"ğŸ‘‹\",\n",
    "    u\"\\(\\^\\.\\^\\)/~~~\":\"ğŸ‘‹\",\n",
    "    u\"\\(T_T\\)/~~~\":\"ğŸ‘‹\",\n",
    "    u\"\\(ToT\\)/~~~\":\"ğŸ‘‹\",\n",
    "    u\"\\(\\*\\^0\\^\\*\\)\":\"ğŸ˜\",\n",
    "    u\"\\(\\*_\\*\\)\":\"ğŸ˜\",\n",
    "    u\"\\(\\*_\\*;\":\"ğŸ˜\",\n",
    "    u\"\\(\\+_\\+\\) \\(@_@\\)\":\"ğŸ˜\",\n",
    "    u\"\\(\\*\\^\\^\\)v\":\"ğŸ˜‚\",\n",
    "    u\"\\(\\^_\\^\\)v\":\"ğŸ˜‚\",\n",
    "    u'\\(-\"-\\)':\"ğŸ˜“\",\n",
    "    u\"\\(ãƒ¼ãƒ¼;\\)\":\"ğŸ˜“\",\n",
    "    u\"\\(\\^0_0\\^\\)\":\"ğŸ˜\",\n",
    "    u\"\\(\\ï¼¾ï½–\\ï¼¾\\)\":\"ğŸ˜€\",\n",
    "    u\"\\(\\ï¼¾ï½•\\ï¼¾\\)\":\"ğŸ˜€\",\n",
    "    u\"\\(\\^\\)o\\(\\^\\)\":\"ğŸ˜€\",\n",
    "    u\"\\(\\^O\\^\\)\":\"ğŸ˜€\",\n",
    "    u\"\\(\\^o\\^\\)\":\"ğŸ˜€\",\n",
    "    u\"\\)\\^o\\^\\(\":\"ğŸ˜€\",\n",
    "    u\":O o_O\":\"ğŸ˜®\",\n",
    "    u\"o_0\":\"ğŸ˜®\",\n",
    "    u\"o\\.O\":\"ğŸ˜®\",\n",
    "    u\"\\(o\\.o\\)\":\"ğŸ˜®\",\n",
    "    u\"oO\":\"ğŸ˜®\",\n",
    "    u\"\\(\\*ï¿£mï¿£\\)\":\"ğŸ˜ \",\n",
    "    u\":â€‘)\":\"â˜ºï¸\",\n",
    "    u\":)\":\"â˜ºï¸\",\n",
    "    u\":-]\":\"â˜ºï¸\",\n",
    "    u\":]\":\"â˜ºï¸\",\n",
    "    u\":-3\":\"â˜ºï¸\",\n",
    "    u\":3\":\"â˜ºï¸\",\n",
    "    u\":->\":\"â˜ºï¸\",\n",
    "    u\":>\":\"â˜ºï¸\",\n",
    "    u\"8-)\":\"â˜ºï¸\",\n",
    "    u\":o)\":\"â˜ºï¸\",\n",
    "    u\":-}\":\"â˜ºï¸\",\n",
    "    u\":}\":\"â˜ºï¸\",\n",
    "    u\":-)\":\"â˜ºï¸\",\n",
    "    u\":c)\":\"â˜ºï¸\",\n",
    "    u\":^)\":\"â˜ºï¸\",\n",
    "    u\"=]\":\"â˜ºï¸\",\n",
    "    u\"=)\":\"â˜ºï¸\",\n",
    "    u\":â€‘D\":\"ğŸ˜ƒ\",\n",
    "    u\":D\":\"ğŸ˜ƒ\",\n",
    "    u\"8â€‘D\":\"ğŸ˜ƒ\",\n",
    "    u\"8D\":\"ğŸ˜ƒ\",\n",
    "    u\"Xâ€‘D\":\"ğŸ˜ƒ\",\n",
    "    u\"XD\":\"ğŸ˜ƒ\",\n",
    "    u\"=D\":\"ğŸ˜ƒ\",\n",
    "    u\"=3\":\"ğŸ˜ƒ\",\n",
    "    u\"B^D\":\"ğŸ˜ƒ\",\n",
    "    u\":-))\":\"ğŸ˜ƒ\",\n",
    "    u\":-(\":\"â˜¹ï¸\",\n",
    "    u\":â€‘(\":\"â˜¹ï¸\",\n",
    "    u\":(\":\"â˜¹ï¸\",\n",
    "    u\":â€‘c\":\"â˜¹ï¸\",\n",
    "    u\":c\":\"â˜¹ï¸\",\n",
    "    u\":â€‘<\":\"â˜¹ï¸\",\n",
    "    u\":<\":\"â˜¹ï¸\",\n",
    "    u\":â€‘[\":\"â˜¹ï¸\",\n",
    "    u\":[\":\"â˜¹ï¸\",\n",
    "    u\":-||\":\"â˜¹ï¸\",\n",
    "    u\">:[\":\"â˜¹ï¸\",\n",
    "    u\":{\":\"â˜¹ï¸\",\n",
    "    u\":@\":\"â˜¹ï¸\",\n",
    "    u\">:(\":\"â˜¹ï¸\",\n",
    "    u\":'â€‘(\":\"ğŸ˜­\",\n",
    "    u\":'(\":\"ğŸ˜­\",\n",
    "    u\":'â€‘)\":\"ğŸ˜ƒ\",\n",
    "    u\":')\":\"ğŸ˜ƒ\",\n",
    "    u\"Dâ€‘':\":\"ğŸ˜§\",\n",
    "    u\"D:<\":\"ğŸ˜¨\",\n",
    "    u\"D:\":\"ğŸ˜§\",\n",
    "    u\"D8\":\"ğŸ˜§\",\n",
    "    u\"D;\":\"ğŸ˜§\",\n",
    "    u\"D=\":\"ğŸ˜§\",\n",
    "    u\"DX\":\"ğŸ˜§\",\n",
    "    u\":â€‘O\":\"ğŸ˜®\",\n",
    "    u\":O\":\"ğŸ˜®\",\n",
    "    u\":â€‘o\":\"ğŸ˜®\",\n",
    "    u\":o\":\"ğŸ˜®\",\n",
    "    u\":-0\":\"ğŸ˜®\",\n",
    "    u\"8â€‘0\":\"ğŸ˜®\",\n",
    "    u\">:O\":\"ğŸ˜®\",\n",
    "    u\":-*\":\"ğŸ˜—\",\n",
    "    u\":*\":\"ğŸ˜—\",\n",
    "    u\":X\":\"ğŸ˜—\",\n",
    "    u\";â€‘)\":\"ğŸ˜‰\",\n",
    "    u\";)\":\"ğŸ˜‰\",\n",
    "    u\"*-)\":\"ğŸ˜‰\",\n",
    "    u\"*)\":\"ğŸ˜‰\",\n",
    "    u\";â€‘]\":\"ğŸ˜‰\",\n",
    "    u\";]\":\"ğŸ˜‰\",\n",
    "    u\";^)\":\"ğŸ˜‰\",\n",
    "    u\":â€‘,\":\"ğŸ˜‰\",\n",
    "    u\";D\":\"ğŸ˜‰\",\n",
    "    u\":â€‘P\":\"ğŸ˜›\",\n",
    "    u\":P\":\"ğŸ˜›\",\n",
    "    u\"Xâ€‘P\":\"ğŸ˜›\",\n",
    "    u\"XP\":\"ğŸ˜›\",\n",
    "    u\":â€‘Ã\":\"ğŸ˜›\",\n",
    "    u\":Ã\":\"ğŸ˜›\",\n",
    "    u\":b\":\"ğŸ˜›\",\n",
    "    u\"d:\":\"ğŸ˜›\",\n",
    "    u\"=p\":\"ğŸ˜›\",\n",
    "    u\">:P\":\"ğŸ˜›\",\n",
    "    u\":â€‘/\":\"ğŸ˜•\",\n",
    "    u\":/\":\"ğŸ˜•\",\n",
    "    u\":-[.]\":\"ğŸ˜•\",\n",
    "    u\">:[(\\)]\":\"ğŸ˜•\",\n",
    "    u\">:/\":\"ğŸ˜•\",\n",
    "    u\":[(\\)]\":\"ğŸ˜•\",\n",
    "    u\"=/\":\"ğŸ˜•\",\n",
    "    u\"=[(\\)]\":\"ğŸ˜•\",\n",
    "    u\":L\":\"ğŸ˜•\",\n",
    "    u\"=L\":\"ğŸ˜•\",\n",
    "    u\":S\":\"ğŸ˜•\",\n",
    "    u\":â€‘|\":\"ğŸ˜\",\n",
    "    u\":|\":\"ğŸ˜\",\n",
    "    u\":$\":\"ğŸ˜³\",\n",
    "    u\":â€‘x\":\"ğŸ¤\",\n",
    "    u\":x\":\"ğŸ¤\",\n",
    "    u\":â€‘#\":\"ğŸ¤\",\n",
    "    u\":#\":\"ğŸ¤\",\n",
    "    u\":â€‘&\":\"ğŸ¤\",\n",
    "    u\":&\":\"ğŸ¤\",\n",
    "    u\"O:â€‘)\":\"ğŸ˜‡\",\n",
    "    u\"O:)\":\"ğŸ˜‡\",\n",
    "    u\"0:â€‘3\":\"ğŸ˜‡\",\n",
    "    u\"0:3\":\"ğŸ˜‡\",\n",
    "    u\"0:â€‘)\":\"ğŸ˜‡\",\n",
    "    u\"0:)\":\"ğŸ˜‡\",\n",
    "    u\":â€‘b\":\"ğŸ˜›\",\n",
    "    u\"0;^)\":\"ğŸ˜‡\",\n",
    "    u\">:â€‘)\":\"ğŸ˜ˆ\",\n",
    "    u\">:)\":\"ğŸ˜ˆ\",\n",
    "    u\"}:â€‘)\":\"ğŸ˜ˆ\",\n",
    "    u\"}:)\":\"ğŸ˜ˆ\",\n",
    "    u\"3:â€‘)\":\"ğŸ˜ˆ\",\n",
    "    u\"3:)\":\"ğŸ˜ˆ\",\n",
    "    u\">;)\":\"ğŸ˜ˆ\",\n",
    "    u\"|;â€‘)\":\"ğŸ˜\",\n",
    "    u\"|â€‘O\":\"ğŸ˜\",\n",
    "    u\":â€‘J\":\"ğŸ˜\",\n",
    "    u\"%â€‘)\":\"ğŸ˜µ\",\n",
    "    u\"%)\":\"ğŸ˜µ\",\n",
    "    u\":-###..\":\"ğŸ¤’\",\n",
    "    u\":###..\":\"ğŸ¤’\",\n",
    "    u\"(>_<)\":\"ğŸ˜£\",\n",
    "    u\"(>_<)>\":\"ğŸ˜£\",\n",
    "    u\"(';')\":\"Baby\",\n",
    "    u\"(^^>``\":\"ğŸ˜“\",\n",
    "    u\"(^_^;)\":\"ğŸ˜“\",\n",
    "    u\"(-_-;)\":\"ğŸ˜“\",\n",
    "    u\"(~_~;) (ãƒ».ãƒ»;)\":\"ğŸ˜“\",\n",
    "    u\"(-_-)zzz\":\"ğŸ˜´\",\n",
    "    u\"(^_-)\":\"ğŸ˜‰\",\n",
    "    u\"((+_+))\":\"ğŸ˜•\",\n",
    "    u\"(+o+)\":\"ğŸ˜•\",\n",
    "    u\"^_^\":\"ğŸ˜ƒ\",\n",
    "    u\"(^_^)/\":\"ğŸ˜ƒ\",\n",
    "    u\"(^O^)ï¼\":\"ğŸ˜ƒ\",\n",
    "    u\"(^o^)ï¼\":\"ğŸ˜ƒ\",\n",
    "    u\"(__)\":\"ğŸ™‡\",\n",
    "    u\"_(._.)_\":\"ğŸ™‡\",\n",
    "    u\"<(_ _)>\":\"ğŸ™‡\",\n",
    "    u\"<m(__)m>\":\"ğŸ™‡\",\n",
    "    u\"m(__)m\":\"ğŸ™‡\",\n",
    "    u\"m(_ _)m\":\"ğŸ™‡\",\n",
    "    u\"('_')\":\"ğŸ˜­\",\n",
    "    u\"(/_;)\":\"ğŸ˜­\",\n",
    "    u\"(T_T) (;_;)\":\"ğŸ˜­\",\n",
    "    u\"(;_;\":\"ğŸ˜­\",\n",
    "    u\"(;_:)\":\"ğŸ˜­\",\n",
    "    u\"(;O;)\":\"ğŸ˜­\",\n",
    "    u\"(:_;)\":\"ğŸ˜­\",\n",
    "    u\"(ToT)\":\"ğŸ˜­\",\n",
    "    u\";_;\":\"ğŸ˜­\",\n",
    "    u\";-;\":\"ğŸ˜­\",\n",
    "    u\";n;\":\"ğŸ˜­\",\n",
    "    u\";;\":\"ğŸ˜­\",\n",
    "    u\"Q.Q\":\"ğŸ˜­\",\n",
    "    u\"T.T\":\"ğŸ˜­\",\n",
    "    u\"QQ\":\"ğŸ˜­\",\n",
    "    u\"Q_Q\":\"ğŸ˜­\",\n",
    "    u\"(-.-)\":\"ğŸ˜\",\n",
    "    u\"(-_-)\":\"ğŸ˜\",\n",
    "    u\"(ä¸€ä¸€)\":\"ğŸ˜\",\n",
    "    u\"(ï¼›ä¸€_ä¸€)\":\"ğŸ˜\",\n",
    "    u\"(=_=)\":\"ğŸ˜©\",\n",
    "    u\"(=^Â·^=)\":\"ğŸ˜º\",\n",
    "    u\"(=^Â·Â·^=)\":\"ğŸ˜º\",\n",
    "    u\"=_^= \":\"ğŸ˜º\",\n",
    "    u\"(..)\":\"ğŸ˜”\",\n",
    "    u\"(._.)\":\"ğŸ˜”\",\n",
    "    u\"(ãƒ»ãƒ»?\":\"ğŸ˜•\",\n",
    "    u\"(?_?)\":\"ğŸ˜•\",\n",
    "    u\">^_^<\":\"ğŸ˜ƒ\",\n",
    "    u\"<^!^>\":\"ğŸ˜ƒ\",\n",
    "    u\"^/^\":\"ğŸ˜ƒ\",\n",
    "    u\"ï¼ˆ*^_^*ï¼‰\" :\"ğŸ˜ƒ\",\n",
    "    u\"(^<^) (^.^)\":\"ğŸ˜ƒ\",\n",
    "    u\"(^^)\":\"ğŸ˜ƒ\",\n",
    "    u\"(^.^)\":\"ğŸ˜ƒ\",\n",
    "    u\"(^_^.)\":\"ğŸ˜ƒ\",\n",
    "    u\"(^_^)\":\"ğŸ˜ƒ\",\n",
    "    u\"(^^)\":\"ğŸ˜ƒ\",\n",
    "    u\"(^J^)\":\"ğŸ˜ƒ\",\n",
    "    u\"(*^.^*)\":\"ğŸ˜ƒ\",\n",
    "    u\"(^â€”^ï¼‰\":\"ğŸ˜ƒ\",\n",
    "    u\"(#^.^#)\":\"ğŸ˜ƒ\",\n",
    "    u\"ï¼ˆ^â€”^ï¼‰\":\"ğŸ‘‹\",\n",
    "    u\"(;_;)/~~~\":\"ğŸ‘‹\",\n",
    "    u\"(^.^)/~~~\":\"ğŸ‘‹\",\n",
    "    u\"(-_-)/~~~ ($Â·Â·)/~~~\":\"ğŸ‘‹\",\n",
    "    u\"(T_T)/~~~\":\"ğŸ‘‹\",\n",
    "    u\"(ToT)/~~~\":\"ğŸ‘‹\",\n",
    "    u\"(*^0^*)\":\"ğŸ˜\",\n",
    "    u\"(*_*)\":\"ğŸ˜\",\n",
    "    u\"(*_*;\":\"ğŸ˜\",\n",
    "    u\"(+_+) (@_@)\":\"ğŸ˜\",\n",
    "    u\"(*^^)v\":\"ğŸ˜‚\",\n",
    "    u\"(^_^)v\":\"ğŸ˜‚\",\n",
    "    u'(-\"-)':\"ğŸ˜“\",\n",
    "    u\"(ãƒ¼ãƒ¼;)\":\"ğŸ˜“\",\n",
    "    u\"(^0_0^)\":\"ğŸ˜\",\n",
    "    u\"(ï¼¾ï½–ï¼¾)\":\"ğŸ˜€\",\n",
    "    u\"(ï¼¾ï½•ï¼¾)\":\"ğŸ˜€\",\n",
    "    u\"(^)o(^)\":\"ğŸ˜€\",\n",
    "    u\"(^O^)\":\"ğŸ˜€\",\n",
    "    u\"(^o^)\":\"ğŸ˜€\",\n",
    "    u\")^o^(\":\"ğŸ˜€\",\n",
    "    u\":O o_O\":\"ğŸ˜®\",\n",
    "    u\"o_0\":\"ğŸ˜®\",\n",
    "    u\"o.O\":\"ğŸ˜®\",\n",
    "    u\"(o.o)\":\"ğŸ˜®\",\n",
    "    u\"oO\":\"ğŸ˜®\",\n",
    "}\n",
    "\n",
    "\n",
    "def str2emoji(tweet):\n",
    "\t\n",
    "\tfor pos,ej in enumerate(tweet):\n",
    "\t\tif ej in emojis:\n",
    "\t\t\ttweet[pos]=emojis[ej]\n",
    "\treturn tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UjpS6y1RKeeZ"
   },
   "outputs": [],
   "source": [
    "def norm_tweet(tweet):\n",
    "  tweet = re.sub(r\"\\\\u2019\", \"'\", tweet)\n",
    "  tweet = re.sub(r\"\\\\u002c\", \",\", tweet)\n",
    "  tweet=' '.join(str2emoji(unidecode(tweet).lower().split()))\n",
    "  tweet = re.sub(r\"\\'ve\", \" have\", tweet)\n",
    "  tweet = re.sub(r\" can\\'t\", \" cannot\", tweet)\n",
    "  tweet = re.sub(r\"n\\'t\", \" not\", tweet)\n",
    "  tweet = re.sub(r\"\\'re\", \" are\", tweet)\n",
    "  tweet = re.sub(r\"\\'d\", \" would\", tweet)\n",
    "  tweet = re.sub(r\"\\'ll\", \" will\", tweet)\n",
    "  tweet = re.sub(r\"\\'s\", \"\", tweet)\n",
    "  tweet = re.sub(r\"\\'n\", \"\", tweet)\n",
    "  tweet = re.sub(r\"\\'m\", \" am\", tweet)\n",
    "  tweet = re.sub(r\"@\\w+\", r' ',tweet)\n",
    "  tweet = re.sub(r\"#\\w+\", r' ',tweet)\n",
    "  tweet = re.sub(r\"[.]+\",\" \",tweet)\n",
    "  #T = tokenizer.TweetTokenizer()\n",
    "  tweet = T.tokenize(tweet)\n",
    "  #tweet = [lemmatizer.lemmatize(i,j[0].lower()) if j[0].lower() in ['a','n','v']  else lemmatizer.lemmatize(i) for i,j in pos_tag(tknzr.tokenize(tweet))]\n",
    "  tweet = [ i for i in tweet if (i not in stopwords) and (i not in string.punctuation ) ]\n",
    "  #tweet = ' '.join(tweet)\n",
    "  return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Rv6ih5wKoHV"
   },
   "outputs": [],
   "source": [
    "def data_preprocessing(data):\n",
    "  data['data'] = data['data'].apply(norm_tweet)\n",
    "  if data['sentiment'].dtypes not in [np.int64]:\n",
    "    data['sentiment']=data['sentiment'].apply(value_sentiment)\n",
    "  return data['data'],data['sentiment'],data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import scipy.stats\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score, jaccard_similarity_score\n",
    "\n",
    "def cohen_kappa_score(y1, y2, labels=None, weights=None):\n",
    "    \"\"\"Cohen's kappa: a statistic that measures inter-annotator agreement.\n",
    "\n",
    "    This function computes Cohen's kappa [1]_, a score that expresses the level\n",
    "    of agreement between two annotators on a classification problem. It is\n",
    "    defined as\n",
    "\n",
    "    .. math::\n",
    "        \\kappa = (p_o - p_e) / (1 - p_e)\n",
    "\n",
    "    where :math:`p_o` is the empirical probability of agreement on the label\n",
    "    assigned to any sample (the observed agreement ratio), and :math:`p_e` is\n",
    "    the expected agreement when both annotators assign labels randomly.\n",
    "    :math:`p_e` is estimated using a per-annotator empirical prior over the\n",
    "    class labels [2]_.\n",
    "\n",
    "    Read more in the :ref:`User Guide <cohen_kappa>`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y1 : array, shape = [n_samples]\n",
    "        Labels assigned by the first annotator.\n",
    "\n",
    "    y2 : array, shape = [n_samples]\n",
    "        Labels assigned by the second annotator. The kappa statistic is\n",
    "        symmetric, so swapping ``y1`` and ``y2`` doesn't change the value.\n",
    "\n",
    "    labels : array, shape = [n_classes], optional\n",
    "        List of labels to index the matrix. This may be used to select a\n",
    "        subset of labels. If None, all labels that appear at least once in\n",
    "        ``y1`` or ``y2`` are used.\n",
    "\n",
    "    weights : str, optional\n",
    "        List of weighting type to calculate the score. None means no weighted;\n",
    "        \"linear\" means linear weighted; \"quadratic\" means quadratic weighted.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    kappa : float\n",
    "        The kappa statistic, which is a number between -1 and 1. The maximum\n",
    "        value means complete agreement; zero or lower means chance agreement.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] J. Cohen (1960). \"A coefficient of agreement for nominal scales\".\n",
    "           Educational and Psychological Measurement 20(1):37-46.\n",
    "           doi:10.1177/001316446002000104.\n",
    "    .. [2] `R. Artstein and M. Poesio (2008). \"Inter-coder agreement for\n",
    "           computational linguistics\". Computational Linguistics 34(4):555-596.\n",
    "           <http://www.mitpressjournals.org/doi/abs/10.1162/coli.07-034-R2#.V0J1MJMrIWo>`_\n",
    "    .. [3] `Wikipedia entry for the Cohen's kappa.\n",
    "            <https://en.wikipedia.org/wiki/Cohen%27s_kappa>`_\n",
    "    \"\"\"\n",
    "    confusion = metrics.confusion_matrix(y1, y2, labels=labels)\n",
    "    n_classes = confusion.shape[0]\n",
    "    sum0 = np.sum(confusion, axis=0)\n",
    "    sum1 = np.sum(confusion, axis=1)\n",
    "    expected = np.outer(sum0, sum1)*1.0 / np.sum(sum0)\n",
    "\n",
    "    if weights is None:\n",
    "        w_mat = np.ones([n_classes, n_classes], dtype=np.int)\n",
    "        w_mat.flat[:: n_classes + 1] = 0\n",
    "    elif weights == \"linear\" or weights == \"quadratic\":\n",
    "        w_mat = np.zeros([n_classes, n_classes], dtype=np.int)\n",
    "        w_mat += np.arange(n_classes)\n",
    "        if weights == \"linear\":\n",
    "            w_mat = np.abs(w_mat - w_mat.T)\n",
    "        else:\n",
    "            w_mat = (w_mat - w_mat.T) ** 2\n",
    "    else:\n",
    "        raise ValueError(\"Unknown kappa weighting type.\")\n",
    "\n",
    "    k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
    "    return 1 - k\n",
    "\n",
    "\n",
    "def evaluate_ei(pred,gold):  \n",
    "    \"\"\"Calculates performance metrics for regression.\n",
    "    \n",
    "    :param pred: the file path of the predictions\n",
    "    :param gold: the filte path withe gold data\n",
    "    :return: a list with performace metrics.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    f=open(pred, \"rb\")\n",
    "    pred_lines=f.readlines()\n",
    "    f.close()\n",
    "    \n",
    "    f=open(gold, \"rb\")\n",
    "    gold_lines=f.readlines()\n",
    "    f.close()    \n",
    "\n",
    "    if(len(pred_lines)==len(gold_lines)):       \n",
    "        # align tweets ids with gold scores and predictions\n",
    "        data_dic={}\n",
    "\n",
    "\n",
    "        header=True        \n",
    "        for line in gold_lines:\n",
    "            #line=line.decode('utf-8')\n",
    "            \n",
    "            if header:\n",
    "                header=False\n",
    "                continue\n",
    "\n",
    "            parts=line.split('\\t')\n",
    "            if len(parts)==4:\n",
    "                # tweet ids containing the word mystery are discarded\n",
    "                if(not 'mystery' in parts[0]):\n",
    "                    data_dic[parts[0]]=[float(line.split('\\t')[3])]\n",
    "            else:\n",
    "                sys.exit('Format problem in '+os.path.basename(gold)+'. Please report this problem to the task organizers.')\n",
    "\n",
    "        header=True        \n",
    "        for line in pred_lines:\n",
    "            \n",
    "            if header:\n",
    "                header=False\n",
    "                continue\n",
    "\n",
    "            parts=line.split('\\t')      \n",
    "            if len(parts)==4:\n",
    "                # tweet ids containing the word mystery are discarded\n",
    "                if(not 'mystery' in parts[0]):\n",
    "                    if parts[0] in data_dic:\n",
    "                        try:\n",
    "                            data_dic[parts[0]].append(float(line.split('\\t')[3]))\n",
    "                        except ValueError:\n",
    "                            # Invalid predictions are replaced by a default value\n",
    "                            data_dic[parts[0]].append(0.5)\n",
    "                    else:\n",
    "                        sys.exit('Invalid tweet id ('+parts[0]+') in '+os.path.basename(pred)+'.')\n",
    "            else:\n",
    "                sys.exit('Format problem in '+os.path.basename(pred)+'.') \n",
    "            \n",
    "        \n",
    "        # lists storing gold and prediction scores\n",
    "        gold_scores=[]  \n",
    "        pred_scores=[]\n",
    "         \n",
    "        \n",
    "        # lists storing gold and prediction scores where gold score >= 0.5\n",
    "        gold_scores_range_05_1=[]\n",
    "        pred_scores_range_05_1=[]\n",
    "            \n",
    "        for id in data_dic:\n",
    "            if(len(data_dic[id])==2):\n",
    "                gold_scores.append(data_dic[id][0])\n",
    "                pred_scores.append(data_dic[id][1])\n",
    "                \n",
    "                if(data_dic[id][0]>=0.5):\n",
    "                    gold_scores_range_05_1.append(data_dic[id][0])\n",
    "                    pred_scores_range_05_1.append(data_dic[id][1])\n",
    "                \n",
    "            else:\n",
    "                sys.exit('Repeated id ('+id+') in '+os.path.basename(pred)+' .')\n",
    "\n",
    "                \n",
    "                    \n",
    "      \n",
    "        # return zero correlation if predictions are constant\n",
    "        if np.std(pred_scores)==0 or np.std(gold_scores)==0:\n",
    "            return (0,0)\n",
    "        \n",
    "\n",
    "        pears_corr=scipy.stats.pearsonr(pred_scores,gold_scores)[0]                                                 \n",
    "        pears_corr_range_05_1=scipy.stats.pearsonr(pred_scores_range_05_1,gold_scores_range_05_1)[0]                         \n",
    "        return (pears_corr,pears_corr_range_05_1)                        \n",
    "                          \n",
    "    else:\n",
    "        sys.exit('Predictions ('+os.path.basename(pred)+') and gold data ('+os.path.basename(gold)+') have different number of lines.')          \n",
    "        \n",
    "        \n",
    "def evaluate_oc(pred,gold):  \n",
    "    \"\"\"Calculates performance metrics for ordinal classification.\n",
    "    \n",
    "    :param pred: the file path of the predictions\n",
    "    :param gold: the filte path withe gold data\n",
    "    :return: a list with performace metrics.\n",
    "    \n",
    "    \"\"\"\n",
    "  \n",
    "   \n",
    "    f=open(pred, \"rb\")\n",
    "    pred_lines=f.readlines()\n",
    "    f.close()\n",
    "    \n",
    "    f=open(gold, \"rb\")\n",
    "    gold_lines=f.readlines()\n",
    "    f.close()    \n",
    "\n",
    "\n",
    "    if(len(pred_lines)==len(gold_lines)):       \n",
    "        # align tweets ids with gold scores and predictions\n",
    "        data_dic={}\n",
    "\n",
    "    \n",
    "        header=True        \n",
    "        for line in gold_lines:\n",
    "            \n",
    "            if header:\n",
    "                header=False\n",
    "                continue\n",
    "            \n",
    "            parts=line.split('\\t')\n",
    "            \n",
    "            label=int(parts[3].split(\":\")[0])\n",
    "            \n",
    "            if len(parts)==4:   \n",
    "                data_dic[parts[0]]=[label]\n",
    "                \n",
    "            else:\n",
    "                sys.exit('Format problem in '+os.path.basename(gold)+'. Please report this problem to the task organizers.')\n",
    "        \n",
    "        \n",
    "        header=True         \n",
    "        for line in pred_lines:\n",
    "            if header:\n",
    "                header=False\n",
    "                continue            \n",
    "            parts=line.split('\\t')   \n",
    "            label=int(parts[3].split(\":\")[0])\n",
    "            if len(parts)==4:  \n",
    "                if parts[0] in data_dic:\n",
    "                    try:\n",
    "                        data_dic[parts[0]].append(label)\n",
    "                    except ValueError:\n",
    "                        # Invalid predictions are replaced by a default value\n",
    "                        data_dic[parts[0]].append(int(0))\n",
    "                else:\n",
    "                    sys.exit('Invalid tweet id ('+parts[0]+') in '+os.path.basename(pred)+'.')\n",
    "            else:\n",
    "                sys.exit('Format problem in '+os.path.basename(pred)+'.') \n",
    "            \n",
    "        \n",
    "        # lists storing gold and prediction scores\n",
    "        gold_scores=[]  \n",
    "        pred_scores=[]\n",
    "         \n",
    "        \n",
    "        # lists storing gold and prediction scores where gold score >= 0.5\n",
    "        gold_scores_some=[]\n",
    "        pred_scores_some=[]\n",
    "            \n",
    "        for id in data_dic:\n",
    "            if(len(data_dic[id])==2):\n",
    "                gold_scores.append(data_dic[id][0])\n",
    "                pred_scores.append(data_dic[id][1])\n",
    "                \n",
    "                if(data_dic[id][0]!=0):\n",
    "                    gold_scores_some.append(data_dic[id][0])\n",
    "                    pred_scores_some.append(data_dic[id][1])\n",
    "                \n",
    "            else:\n",
    "                sys.exit('Repeated id ('+id+') in '+os.path.basename(pred)+' .')\n",
    "\n",
    "                \n",
    "                \n",
    "        # return null scores if predictions are constant\n",
    "        if np.std(pred_scores)==0 or np.std(gold_scores)==0:\n",
    "            return (0,0,0,0)\n",
    "\n",
    "        pears_corr=scipy.stats.pearsonr(pred_scores,gold_scores)[0]              \n",
    "        pears_corr_some=scipy.stats.pearsonr(pred_scores_some,gold_scores_some)[0]  \n",
    "        \n",
    "        # fix labels to values observed in gold data        \n",
    "        gold_labels=list(sorted(set(gold_scores)))\n",
    "       \n",
    "        kappa=cohen_kappa_score(pred_scores,gold_scores,labels=gold_labels, weights='quadratic')        \n",
    "        kappa_some=cohen_kappa_score(pred_scores_some,gold_scores_some, labels=gold_labels, weights='quadratic')\n",
    " \n",
    "        return (pears_corr,pears_corr_some,kappa,kappa_some)\n",
    "\n",
    "    else:\n",
    "        sys.exit('Predictions ('+os.path.basename(pred)+') and gold data ('+os.path.basename(gold)+') have different number of lines.')          \n",
    "  \n",
    "        \n",
    "def evaluate_multilabel(pred,gold):  \n",
    "    \"\"\"Calculates performance metrics for multi-label classification.\n",
    "    \n",
    "    :param pred: the file path of the predictions\n",
    "    :param gold: the filte path withe gold data\n",
    "    :return: a list with performace metrics.    \n",
    "    \"\"\"     \n",
    "    \n",
    "    f=open(pred, \"rb\")\n",
    "    pred_lines=f.readlines()\n",
    "    f.close()\n",
    "    \n",
    "    f=open(gold, \"rb\")\n",
    "    gold_lines=f.readlines()\n",
    "    f.close()    \n",
    "\n",
    "    if(len(pred_lines)==len(gold_lines)):       \n",
    "        # align tweets ids with gold scores and predictions\n",
    "        data_dic={}\n",
    "\n",
    "    \n",
    "        header=True        \n",
    "        for line in gold_lines:\n",
    "           \n",
    "            if header:\n",
    "                header=False\n",
    "                continue\n",
    "            \n",
    "            parts=line.split('\\t')\n",
    "            \n",
    "            if len(parts)==13:  \n",
    "                labels=[]\n",
    "                for m_label in parts[2:13]:\n",
    "                    labels.append(int(m_label))\n",
    " \n",
    "                data_dic[parts[0]]=[tuple(labels)]\n",
    "                \n",
    "            else:\n",
    "                sys.exit('Format problem in '+os.path.basename(gold)+'. Please report this problem to the task organizers.')\n",
    "        \n",
    "        header=True         \n",
    "        for line in pred_lines:\n",
    "            if header:\n",
    "                header=False\n",
    "                continue            \n",
    "            parts=line.split('\\t')   \n",
    "            if len(parts)==13:  \n",
    "                if parts[0] in data_dic:\n",
    "                    try:\n",
    "                        labels=[]\n",
    "                        for m_label in parts[2:13]:\n",
    "                            labels.append(int(m_label))\n",
    "                        data_dic[parts[0]].append(tuple(labels))\n",
    "                    except ValueError:\n",
    "                        # Invalid predictions are replaced by a default value\n",
    "                        data_dic[parts[0]].append((0,0,0,0,0,0,0,0,0,0,0))\n",
    "                else:\n",
    "                    sys.exit('Invalid tweet id in '+os.path.basename(pred)+'.')\n",
    "            else:\n",
    "                sys.exit('Format problem in '+os.path.basename(pred)+'.')   \n",
    "            \n",
    "\n",
    "       # lists storing gold and prediction scores\n",
    "        gold_scores=[]  \n",
    "        pred_scores=[]\n",
    "         \n",
    "            \n",
    "        for id in data_dic:\n",
    "            if(len(data_dic[id])==2):\n",
    "                gold_scores.append(data_dic[id][0])\n",
    "                pred_scores.append(data_dic[id][1])\n",
    "                \n",
    "            else:\n",
    "                sys.exit('Repeated id ('+id+') in '+os.path.basename(pred)+' .')\n",
    "\n",
    "\n",
    "        y_true = np.array(gold_scores)\n",
    "        y_pred = np.array(pred_scores)       \n",
    "    \n",
    "        acc=jaccard_similarity_score(y_true,y_pred)       \n",
    "        f1_micro=f1_score(y_true, y_pred, average='micro')  \n",
    "        f1_macro=f1_score(y_true, y_pred, average='macro')  \n",
    "            \n",
    "        return (acc,f1_micro,f1_macro)\n",
    "    \n",
    "    else:\n",
    "        sys.exit('Predictions ('+os.path.basename(pred)+') and gold data ('+os.path.basename(gold)+') have different number of lines.')          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zxXHdW0IODWx"
   },
   "outputs": [],
   "source": [
    "def model1(x_train_3, y_train_3,x_val_3, y_val_3, embedding_layer):\n",
    "\n",
    "\tmodel1 = Sequential()\n",
    "\tmodel1.add(embedding_layer)\n",
    "\tmodel1.add(LSTM(32))\n",
    "\tmodel1.add(Dropout(0.2))\n",
    "\tmodel1.add(Dense(32, activation='relu'))\n",
    "\tmodel1.add(Dropout(0.2))\n",
    "\tmodel1.add(Dense(3, activation='softmax'))\n",
    "\tmodel1.compile(loss='categorical_crossentropy',\n",
    "\t\t\t      optimizer='Adam',\n",
    "\t\t\t      metrics=['acc'])\n",
    "\tmodel1.summary()\n",
    "\thistory=model1.fit(x_train_3, y_train_3, validation_data=(x_val_3, y_val_3),epochs=6, batch_size=50)\n",
    "\tmodel1.save(\"./model1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sdLO3C6WODMT"
   },
   "outputs": [],
   "source": [
    "def model2(x_train_3, y_train_3,x_val_3, y_val_3, embedding_layer,epochs, batch_size):\n",
    "\tmodel2 = Sequential()\n",
    "\tmodel2.add(embedding_layer)\n",
    "\tmodel2.add(GRU(32))\n",
    "\tmodel2.add(Dropout(0.2))\n",
    "\tmodel2.add(Dense(3, activation='softmax'))\n",
    "\tmodel2.compile(loss='categorical_crossentropy',\n",
    "\t\t\t      optimizer='rmsprop',\n",
    "\t\t\t      metrics=['acc'])\n",
    "\tmodel2.summary()\n",
    "\thistory=model2.fit(x_train_3, y_train_3, validation_data=(x_val_3, y_val_3),epochs=epochs, batch_size=batch_size)\n",
    "\tmodel2.save(\"./model2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OQiBIkDmKoEU"
   },
   "outputs": [],
   "source": [
    "#Path of datas\n",
    "data_train_3 =  \"train_1_1.csv\"\n",
    "data_train_7 =  \"train_3_3.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FezwSR_SLp4p"
   },
   "outputs": [],
   "source": [
    "#Load Data\n",
    "data_train_3= pd.read_csv(data_train_3,sep='\\t',names=['id','sentiment','data'])\n",
    "data_train_7= pd.read_csv(data_train_7,sep='\\t',names=['id','sentiment','data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CTmWa7BuKoBy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "# Apply the preprocessiog on the data\n",
    "# Normalize the tweets\n",
    "tweets_train_3, sentiments_train_3,data_train_3 = data_preprocessing(data_train_3)\n",
    "tweets_train_7, sentiments_train_7,data_train_7 = data_preprocessing(data_train_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rlA-OcLQLKwi"
   },
   "outputs": [],
   "source": [
    "all_tweet = tweets_train_3.append(tweets_train_7)\n",
    "tokenizer = Tokenizer(filters=' ')\n",
    "tokenizer.fit_on_texts(all_tweet)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3KWoHuF5MvAZ"
   },
   "outputs": [],
   "source": [
    "sequences_train_3 = tokenizer.texts_to_sequences(tweets_train_3)\n",
    "sequences_train_7 = tokenizer.texts_to_sequences(tweets_train_7)\n",
    "sequences = sequences_train_3 + sequences_train_7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0rER1ZxaLgd1"
   },
   "outputs": [],
   "source": [
    "#Seek the longuest Tweet\n",
    "#To Apply a Padding\n",
    "MAX_SEQUENCE_LENGTH = 0\n",
    "for elt in sequences:\n",
    "\tif len(elt) > MAX_SEQUENCE_LENGTH:\n",
    "\t\tMAX_SEQUENCE_LENGTH = len(elt)\n",
    "  \n",
    "data_train_3 = pad_sequences(sequences_train_3, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_train_7 = pad_sequences(sequences_train_7, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GL0k6uf1Lga3"
   },
   "outputs": [],
   "source": [
    "indices_train_3 = np.arange(data_train_3.shape[0])\n",
    "data_train_3 = data_train_3[indices_train_3]\n",
    "\n",
    "indices_train_7 = np.arange(data_train_7.shape[0])\n",
    "data_train_7 = data_train_7[indices_train_7]\n",
    "\n",
    "labels_train_3 = to_categorical(np.asarray(sentiments_train_3), 3)\n",
    "labels_train_3 = labels_train_3[indices_train_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dEFq7QZbLgSk"
   },
   "outputs": [],
   "source": [
    "nb_words=len(word_index)+1\n",
    "EMBEDDING_DIM=300\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "90yb26nXNvnt"
   },
   "outputs": [],
   "source": [
    "oov=[]\n",
    "oov.append((np.random.rand(EMBEDDING_DIM)) - 1.0)\n",
    "oov = oov / np.linalg.norm(oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8X5fM9nSNvj5"
   },
   "outputs": [],
   "source": [
    "for word, i in word_index.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)\n",
    "    else:\n",
    "        embedding_matrix[i] = oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ofA7BZ1UNvgD"
   },
   "outputs": [],
   "source": [
    "split_idx = int(len(data_train_3)*0.70)\n",
    "x_train_3, x_val_3 = data_train_3[:split_idx], data_train_3[split_idx:]\n",
    "y_train_3, y_val_3 = labels_train_3 [:split_idx], labels_train_3[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RGqBPaa-Nvb8"
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False, name='embedding_layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wc_MJrSoNvYo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 31, 300)           18438300  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                42624     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 18,482,079\n",
      "Trainable params: 43,779\n",
      "Non-trainable params: 18,438,300\n",
      "_________________________________________________________________\n",
      "Train on 35233 samples, validate on 15100 samples\n",
      "Epoch 1/6\n",
      "35233/35233 [==============================] - 10s 295us/step - loss: 0.8374 - acc: 0.6122 - val_loss: 0.8151 - val_acc: 0.6087\n",
      "Epoch 2/6\n",
      "35233/35233 [==============================] - 10s 274us/step - loss: 0.7647 - acc: 0.6572 - val_loss: 0.8270 - val_acc: 0.6025\n",
      "Epoch 3/6\n",
      "35233/35233 [==============================] - 10s 282us/step - loss: 0.7380 - acc: 0.6725 - val_loss: 0.8025 - val_acc: 0.6145\n",
      "Epoch 4/6\n",
      "35233/35233 [==============================] - 10s 279us/step - loss: 0.7214 - acc: 0.6794 - val_loss: 0.8316 - val_acc: 0.6087\n",
      "Epoch 5/6\n",
      "35233/35233 [==============================] - 9s 258us/step - loss: 0.7034 - acc: 0.6904 - val_loss: 0.8505 - val_acc: 0.6106\n",
      "Epoch 6/6\n",
      "35233/35233 [==============================] - 9s 267us/step - loss: 0.6877 - acc: 0.6964 - val_loss: 0.8178 - val_acc: 0.6230\n"
     ]
    }
   ],
   "source": [
    "model1(x_train_3, y_train_3,x_val_3, y_val_3, embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dtZU5RcnNvS2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 31, 300)           18438300  \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 32)                31968     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 18,470,367\n",
      "Trainable params: 32,067\n",
      "Non-trainable params: 18,438,300\n",
      "_________________________________________________________________\n",
      "Train on 35233 samples, validate on 15100 samples\n",
      "Epoch 1/6\n",
      "35233/35233 [==============================] - 10s 273us/step - loss: 0.8351 - acc: 0.6103 - val_loss: 0.8939 - val_acc: 0.5842\n",
      "Epoch 2/6\n",
      "35233/35233 [==============================] - 9s 256us/step - loss: 0.7561 - acc: 0.6616 - val_loss: 0.8297 - val_acc: 0.6103\n",
      "Epoch 3/6\n",
      "35233/35233 [==============================] - 9s 261us/step - loss: 0.7346 - acc: 0.6741 - val_loss: 0.8391 - val_acc: 0.6046\n",
      "Epoch 4/6\n",
      "35233/35233 [==============================] - 9s 270us/step - loss: 0.7182 - acc: 0.6828 - val_loss: 0.8091 - val_acc: 0.6223\n",
      "Epoch 5/6\n",
      "35233/35233 [==============================] - 10s 282us/step - loss: 0.7037 - acc: 0.6896 - val_loss: 0.8326 - val_acc: 0.6179\n",
      "Epoch 6/6\n",
      "35233/35233 [==============================] - 10s 292us/step - loss: 0.6899 - acc: 0.6948 - val_loss: 0.8040 - val_acc: 0.6259\n"
     ]
    }
   ],
   "source": [
    "model2(x_train_3, y_train_3,x_val_3, y_val_3, embedding_layer, 6, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Af6C8eMXNvOF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4      -2\n",
      "       ..\n",
      "1625    3\n",
      "1626   -2\n",
      "1627    1\n",
      "1628    0\n",
      "1629   -2\n",
      "Name: sentiment, Length: 1630, dtype: int64 0                             [yeah, â˜º, ï¸, playing, well]\n",
      "1       [least, guy, trying, discourage, anymore, want...\n",
      "2       [uplift, still, discouraged, means, listening,...\n",
      "3                              [age, heyday, blood, tame]\n",
      "4       [embarrassed, saw, us, like, knvfkkjg, thinks,...\n",
      "                              ...                        \n",
      "1625       [idk, help, someone, smile, comes, lips, n, n]\n",
      "1626    [think, leep, favorite, get, dark, maybe, inso...\n",
      "1627                   [amelia, want, sarah, v, grateful]\n",
      "1628                                  [lack, makes, n, n]\n",
      "1629                   [james, clapper, cary, disturbing]\n",
      "Name: data, Length: 1630, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(sentiments_train_7, tweets_train_7)\n",
    "labels_train_7 = to_categorical(np.asarray(sentiments_train_7), 7)\n",
    "labels_train_7 = labels_train_7[indices_train_7]\n",
    "\n",
    "split_idx = int(len(data_train_7)*0.85)\n",
    "x_train_7, x_val_7 = data_train_7[:split_idx], data_train_7[split_idx:]\n",
    "y_train_7, y_val_7 = labels_train_7 [:split_idx], labels_train_7[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zJAVpUubpzrY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 31, 300)           18438300  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                42624     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 18,482,079\n",
      "Trainable params: 43,779\n",
      "Non-trainable params: 18,438,300\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 31, 300)           18438300  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                42624     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 99        \n",
      "_________________________________________________________________\n",
      "dense1 (Dense)               (None, 150)               600       \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               (None, 64)                9664      \n",
      "_________________________________________________________________\n",
      "dense3 (Dense)               (None, 7)                 455       \n",
      "=================================================================\n",
      "Total params: 18,482,079\n",
      "Trainable params: 43,779\n",
      "Non-trainable params: 18,438,300\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beuvry_j/.local/lib/python3.7/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1385 samples, validate on 245 samples\n",
      "Epoch 1/11\n",
      "1385/1385 [==============================] - 1s 559us/step - loss: 1.8448 - accuracy: 0.2845 - val_loss: 1.7641 - val_accuracy: 0.2898\n",
      "Epoch 2/11\n",
      "1385/1385 [==============================] - 0s 302us/step - loss: 1.6811 - accuracy: 0.3307 - val_loss: 1.7109 - val_accuracy: 0.2898\n",
      "Epoch 3/11\n",
      "1385/1385 [==============================] - 0s 300us/step - loss: 1.6149 - accuracy: 0.3552 - val_loss: 1.6569 - val_accuracy: 0.2898\n",
      "Epoch 4/11\n",
      "1385/1385 [==============================] - 0s 288us/step - loss: 1.5719 - accuracy: 0.3509 - val_loss: 1.7284 - val_accuracy: 0.2857\n",
      "Epoch 5/11\n",
      "1385/1385 [==============================] - 0s 308us/step - loss: 1.5446 - accuracy: 0.3632 - val_loss: 1.6649 - val_accuracy: 0.2939\n",
      "Epoch 6/11\n",
      "1385/1385 [==============================] - 0s 294us/step - loss: 1.5207 - accuracy: 0.3841 - val_loss: 1.6690 - val_accuracy: 0.3265\n",
      "Epoch 7/11\n",
      "1385/1385 [==============================] - 0s 295us/step - loss: 1.4876 - accuracy: 0.3964 - val_loss: 1.6613 - val_accuracy: 0.3592\n",
      "Epoch 8/11\n",
      "1385/1385 [==============================] - 0s 307us/step - loss: 1.4675 - accuracy: 0.4058 - val_loss: 1.6890 - val_accuracy: 0.3592\n",
      "Epoch 9/11\n",
      "1385/1385 [==============================] - 0s 288us/step - loss: 1.4445 - accuracy: 0.4130 - val_loss: 1.7020 - val_accuracy: 0.3755\n",
      "Epoch 10/11\n",
      "1385/1385 [==============================] - 0s 296us/step - loss: 1.4170 - accuracy: 0.4289 - val_loss: 1.7198 - val_accuracy: 0.3673\n",
      "Epoch 11/11\n",
      "1385/1385 [==============================] - 0s 309us/step - loss: 1.3963 - accuracy: 0.4419 - val_loss: 1.7517 - val_accuracy: 0.3592\n"
     ]
    }
   ],
   "source": [
    "model=keras.models.load_model(\"./model1.h5\")\n",
    "model.summary()\n",
    "model.layers.pop()\n",
    "model.layers.pop()\n",
    "model.add(Dense(150,activation='relu',name='dense1'))\n",
    "model.add(Dense(64,activation='relu',name='dense2'))\n",
    "model.add(Dense(7,activation='softmax',name='dense3'))\n",
    "model.summary()\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "history = model.fit(x_train_7, y_train_7,   validation_data=(x_val_7,y_val_7), epochs=11, batch_size=50)\n",
    "model.save(\"./model3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e02yEAVrNvKm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 0,  2, -3, ..., -2,  2,  2]), array([ 0,  0,  0, ...,  1,  0, -2]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpora_test_7=\"./train_3_3.csv\"\n",
    "corpora_test_7= pd.read_csv(corpora_test_7,sep='\\t',names=['id','sentiment','data'])\n",
    "\n",
    "tweets_test_7,sentiments_test_7,corpora_test_7 = data_preprocessing(corpora_test_7)\n",
    "sequences_test_7 = tokenizer.texts_to_sequences(tweets_test_7)\n",
    "pad_sequence_test_7 = pad_sequences(sequences_test_7, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "y_test_predict_7 = model.predict_classes(pad_sequence_test_7) - 3 # To replace between -3 and 3\n",
    "\n",
    "#cohen_kappa_score(y_test_predict_7, sentiments_test_7.values)\n",
    "y_test_predict_7, sentiments_test_7.values"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
