{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2270,
     "status": "ok",
     "timestamp": 1573606004254,
     "user": {
      "displayName": "Benoit FrÃ©dÃ©ric-Moreau",
      "photoUrl": "",
      "userId": "09939474294080590706"
     },
     "user_tz": -60
    },
    "id": "XfkBZ2f9J_20",
    "outputId": "287a12b2-840a-4b29-c37f-b48b495e2c4e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/beuvry_j/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#from google.colab import drive\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import LSTM, Dropout, Dense, Bidirectional,  Flatten, Input, GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "from unidecode import unidecode\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords=stopwords.words('english')\n",
    "\n",
    "T = TweetTokenizer()\n",
    "EMBEDDING_FILE = '../GoogleNews-vectors-negative300.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tpo2LX6hJ_vg"
   },
   "outputs": [],
   "source": [
    "def value_sentiment(x):\n",
    "  if x == \"neutral\":\n",
    "    return 0\n",
    "  if x == \"positive\":\n",
    "    return 1\n",
    "  if x == \"negative\":\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m6d-_t2NKvQp"
   },
   "outputs": [],
   "source": [
    "def onehot (tweets,length):\n",
    "  onh = []\n",
    "  for tweet in tweets:\n",
    "      onh_seq = np.zeros(length) \n",
    "      for i in tweet:\n",
    "        onh_seq[i -1] = 1\n",
    "      onh.append(onh_seq)\n",
    "  return onh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TlSZY2t6J_gj"
   },
   "outputs": [],
   "source": [
    "#replace emojy\n",
    "emojis = {\n",
    "    u\":â€‘\\)\":\"â˜ºï¸\",\n",
    "    u\":\\)\":\"â˜ºï¸\",\n",
    "    u\":-\\]\":\"â˜ºï¸\",\n",
    "    u\":\\]\":\"â˜ºï¸\",\n",
    "    u\":-3\":\"â˜ºï¸\",\n",
    "    u\":3\":\"â˜ºï¸\",\n",
    "    u\":->\":\"â˜ºï¸\",\n",
    "    u\":>\":\"â˜ºï¸\",\n",
    "    u\"8-\\)\":\"â˜ºï¸\",\n",
    "    u\":o\\)\":\"â˜ºï¸\",\n",
    "    u\":-\\}\":\"â˜ºï¸\",\n",
    "    u\":\\}\":\"â˜ºï¸\",\n",
    "    u\":-\\)\":\"â˜ºï¸\",\n",
    "    u\":c\\)\":\"â˜ºï¸\",\n",
    "    u\":\\^\\)\":\"â˜ºï¸\",\n",
    "    u\"=\\]\":\"â˜ºï¸\",\n",
    "    u\"=\\)\":\"â˜ºï¸\",\n",
    "    u\":â€‘D\":\"ğŸ˜ƒ\",\n",
    "    u\":D\":\"ğŸ˜ƒ\",\n",
    "    u\"8â€‘D\":\"ğŸ˜ƒ\",\n",
    "    u\"8D\":\"ğŸ˜ƒ\",\n",
    "    u\"Xâ€‘D\":\"ğŸ˜ƒ\",\n",
    "    u\"XD\":\"ğŸ˜ƒ\",\n",
    "    u\"=D\":\"ğŸ˜ƒ\",\n",
    "    u\"=3\":\"ğŸ˜ƒ\",\n",
    "    u\"B\\^D\":\"ğŸ˜ƒ\",\n",
    "    u\":-\\)\\)\":\"ğŸ˜ƒ\",\n",
    "    u\":â€‘\\(\":\"â˜¹ï¸\",\n",
    "    u\":-\\(\":\"â˜¹ï¸\",\n",
    "    u\":\\(\":\"â˜¹ï¸\",\n",
    "    u\":â€‘c\":\"â˜¹ï¸\",\n",
    "    u\":c\":\"â˜¹ï¸\",\n",
    "    u\":â€‘<\":\"â˜¹ï¸\",\n",
    "    u\":<\":\"â˜¹ï¸\",\n",
    "    u\":â€‘\\[\":\"â˜¹ï¸\",\n",
    "    u\":\\[\":\"â˜¹ï¸\",\n",
    "    u\":-\\|\\|\":\"â˜¹ï¸\",\n",
    "    u\">:\\[\":\"â˜¹ï¸\",\n",
    "    u\":\\{\":\"â˜¹ï¸\",\n",
    "    u\":@\":\"â˜¹ï¸\",\n",
    "    u\">:\\(\":\"â˜¹ï¸\",\n",
    "    u\":'â€‘\\(\":\"ğŸ˜­\",\n",
    "    u\":'\\(\":\"ğŸ˜­\",\n",
    "    u\":'â€‘\\)\":\"ğŸ˜ƒ\",\n",
    "    u\":'\\)\":\"ğŸ˜ƒ\",\n",
    "    u\"Dâ€‘':\":\"ğŸ˜¨\",\n",
    "    u\"D:<\":\"ğŸ˜¨\",\n",
    "    u\"D:\":\"ğŸ˜§\",\n",
    "    u\"D8\":\"ğŸ˜§\",\n",
    "    u\"D;\":\"ğŸ˜§\",\n",
    "    u\"D=\":\"ğŸ˜§\",\n",
    "    u\"DX\":\"ğŸ˜§\",\n",
    "    u\":â€‘O\":\"ğŸ˜®\",\n",
    "    u\":O\":\"ğŸ˜®\",\n",
    "    u\":â€‘o\":\"ğŸ˜®\",\n",
    "    u\":o\":\"ğŸ˜®\",\n",
    "    u\":-0\":\"ğŸ˜®\",\n",
    "    u\"8â€‘0\":\"ğŸ˜®\",\n",
    "    u\">:O\":\"ğŸ˜®\",\n",
    "    u\":-\\*\":\"ğŸ˜—\",\n",
    "    u\":\\*\":\"ğŸ˜—\",\n",
    "    u\":X\":\"ğŸ˜—\",\n",
    "    u\";â€‘\\)\":\"ğŸ˜‰\",\n",
    "    u\";\\)\":\"ğŸ˜‰\",\n",
    "    u\"\\*-\\)\":\"ğŸ˜‰\",\n",
    "    u\"\\*\\)\":\"ğŸ˜‰\",\n",
    "    u\";â€‘\\]\":\"ğŸ˜‰\",\n",
    "    u\";\\]\":\"ğŸ˜‰\",\n",
    "    u\";\\^\\)\":\"ğŸ˜‰\",\n",
    "    u\":â€‘,\":\"ğŸ˜‰\",\n",
    "    u\";D\":\"ğŸ˜‰\",\n",
    "    u\":â€‘P\":\"ğŸ˜›\",\n",
    "    u\":P\":\"ğŸ˜›\",\n",
    "    u\"Xâ€‘P\":\"ğŸ˜›\",\n",
    "    u\"XP\":\"ğŸ˜›\",\n",
    "    u\":â€‘Ã\":\"ğŸ˜›\",\n",
    "    u\":Ã\":\"ğŸ˜›\",\n",
    "    u\":b\":\"ğŸ˜›\",\n",
    "    u\"d:\":\"ğŸ˜›\",\n",
    "    u\"=p\":\"ğŸ˜›\",\n",
    "    u\">:P\":\"ğŸ˜›\",\n",
    "    u\":â€‘/\":\"ğŸ˜•\",\n",
    "    u\":/\":\"ğŸ˜•\",\n",
    "    u\":-[.]\":\"ğŸ˜•\",\n",
    "    u\">:[(\\\\\\)]\":\"ğŸ˜•\",\n",
    "    u\">:/\":\"ğŸ˜•\",\n",
    "    u\":[(\\\\\\)]\":\"ğŸ˜•\",\n",
    "    u\"=/\":\"ğŸ˜•\",\n",
    "    u\"=[(\\\\\\)]\":\"ğŸ˜•\",\n",
    "    u\":L\":\"ğŸ˜•\",\n",
    "    u\"=L\":\"ğŸ˜•\",\n",
    "    u\":S\":\"ğŸ˜•\",\n",
    "    u\":â€‘\\|\":\"ğŸ˜\",\n",
    "    u\":\\|\":\"ğŸ˜\",\n",
    "    u\":$\":\"ğŸ˜³\",\n",
    "    u\":â€‘x\":\"ğŸ¤\",\n",
    "    u\":x\":\"ğŸ¤\",\n",
    "    u\":â€‘#\":\"ğŸ¤\",\n",
    "    u\":#\":\"ğŸ¤\",\n",
    "    u\":â€‘&\":\"ğŸ¤\",\n",
    "    u\":&\":\"ğŸ¤\",\n",
    "    u\"O:â€‘\\)\":\"ğŸ˜‡\",\n",
    "    u\"O:\\)\":\"ğŸ˜‡\",\n",
    "    u\"0:â€‘3\":\"ğŸ˜‡\",\n",
    "    u\"0:3\":\"ğŸ˜‡\",\n",
    "    u\"0:â€‘\\)\":\"ğŸ˜‡\",\n",
    "    u\"0:\\)\":\"ğŸ˜‡\",\n",
    "    u\":â€‘b\":\"ğŸ˜›\",\n",
    "    u\"0;\\^\\)\":\"ğŸ˜‡\",\n",
    "    u\">:â€‘\\)\":\"ğŸ˜ˆ\",\n",
    "    u\">:\\)\":\"ğŸ˜ˆ\",\n",
    "    u\"\\}:â€‘\\)\":\"ğŸ˜ˆ\",\n",
    "    u\"\\}:\\)\":\"ğŸ˜ˆ\",\n",
    "    u\"3:â€‘\\)\":\"ğŸ˜ˆ\",\n",
    "    u\"3:\\)\":\"ğŸ˜ˆ\",\n",
    "    u\">;\\)\":\"ğŸ˜ˆ\",\n",
    "    u\"\\|;â€‘\\)\":\"ğŸ˜\",\n",
    "    u\"\\|â€‘O\":\"ğŸ˜\",\n",
    "    u\":â€‘J\":\"ğŸ˜\",\n",
    "    u\"%â€‘\\)\":\"ğŸ˜µ\",\n",
    "    u\"%\\)\":\"ğŸ˜µ\",\n",
    "    u\":-###..\":\"ğŸ¤’\",\n",
    "    u\":###..\":\"ğŸ¤’\",\n",
    "    u\"\\(>_<\\)\":\"ğŸ˜£\",\n",
    "    u\"\\(>_<\\)>\":\"ğŸ˜£\",\n",
    "    u\"\\(';'\\)\":\"ğŸ‘¶\",\n",
    "    u\"\\(\\^\\^>``\":\"ğŸ˜“\",\n",
    "    u\"\\(\\^_\\^;\\)\":\"ğŸ˜“\",\n",
    "    u\"\\(-_-;\\)\":\"ğŸ˜“\",\n",
    "    u\"\\(~_~;\\) \\(ãƒ»\\.ãƒ»;\\)\":\"ğŸ˜“\",\n",
    "    u\"\\(-_-\\)zzz\":\"ğŸ˜´\",\n",
    "    u\"\\(\\^_-\\)\":\"ğŸ˜‰\",\n",
    "    u\"\\(\\(\\+_\\+\\)\\)\":\"ğŸ˜•\",\n",
    "    u\"\\(\\+o\\+\\)\":\"ğŸ˜•\",\n",
    "    u\"\\^_\\^\":\"ğŸ˜ƒ\",\n",
    "    u\"\\(\\^_\\^\\)/\":\"ğŸ˜ƒ\",\n",
    "    u\"\\(\\^O\\^\\)ï¼\":\"ğŸ˜ƒ\",\n",
    "    u\"\\(\\^o\\^\\)ï¼\":\"ğŸ˜ƒ\",\n",
    "    u\"\\(__\\)\":\"ğŸ™‡\",\n",
    "    u\"_\\(\\._\\.\\)_\":\"ğŸ™‡\",\n",
    "    u\"<\\(_ _\\)>\":\"ğŸ™‡\",\n",
    "    u\"<m\\(__\\)m>\":\"ğŸ™‡\",\n",
    "    u\"m\\(__\\)m\":\"ğŸ™‡\",\n",
    "    u\"m\\(_ _\\)m\":\"ğŸ™‡\",\n",
    "    u\"\\('_'\\)\":\"ğŸ˜­\",\n",
    "    u\"\\(/_;\\)\":\"ğŸ˜­\",\n",
    "    u\"\\(T_T\\) \\(;_;\\)\":\"ğŸ˜­\",\n",
    "    u\"\\(;_;\":\"ğŸ˜­\",\n",
    "    u\"\\(;_:\\)\":\"ğŸ˜­\",\n",
    "    u\"\\(;O;\\)\":\"ğŸ˜­\",\n",
    "    u\"\\(:_;\\)\":\"ğŸ˜­\",\n",
    "    u\"\\(ToT\\)\":\"ğŸ˜­\",\n",
    "    u\";_;\":\"ğŸ˜­\",\n",
    "    u\";-;\":\"ğŸ˜­\",\n",
    "    u\";n;\":\"ğŸ˜­\",\n",
    "    u\";;\":\"ğŸ˜­\",\n",
    "    u\"Q\\.Q\":\"ğŸ˜­\",\n",
    "    u\"T\\.T\":\"ğŸ˜­\",\n",
    "    u\"QQ\":\"ğŸ˜­\",\n",
    "    u\"Q_Q\":\"ğŸ˜­\",\n",
    "    u\"\\(-\\.-\\)\":\"ğŸ˜\",\n",
    "    u\"\\(-_-\\)\":\"ğŸ˜\",\n",
    "    u\"\\(ä¸€ä¸€\\)\":\"ğŸ˜\",\n",
    "    u\"\\(ï¼›ä¸€_ä¸€\\)\":\"ğŸ˜\",\n",
    "    u\"\\(=_=\\)\":\"ğŸ˜©\",\n",
    "    u\"\\(=\\^\\Â·\\^=\\)\":\"ğŸ˜º\",\n",
    "    u\"\\(=\\^\\Â·\\Â·\\^=\\)\":\"ğŸ˜º\",\n",
    "    u\"=_\\^=\t\":\"ğŸ˜º\",\n",
    "    u\"\\(\\.\\.\\)\":\"ğŸ˜”\",\n",
    "    u\"\\(\\._\\.\\)\":\"ğŸ˜”\",\n",
    "    u\"\\(\\ãƒ»\\ãƒ»?\":\"ğŸ˜•\",\n",
    "    u\"\\(?_?\\)\":\"ğŸ˜•\",\n",
    "    u\">\\^_\\^<\":\"ğŸ˜ƒ\",\n",
    "    u\"<\\^!\\^>\":\"ğŸ˜ƒ\",\n",
    "    u\"\\^/\\^\":\"ğŸ˜ƒ\",\n",
    "    u\"\\ï¼ˆ\\*\\^_\\^\\*ï¼‰\" :\"ğŸ˜ƒ\",\n",
    "    u\"\\(\\^<\\^\\) \\(\\^\\.\\^\\)\":\"ğŸ˜ƒ\",\n",
    "    u\"\\(^\\^\\)\":\"ğŸ˜ƒ\",\n",
    "    u\"\\(\\^\\.\\^\\)\":\"ğŸ˜ƒ\",\n",
    "    u\"\\(\\^_\\^\\.\\)\":\"ğŸ˜ƒ\",\n",
    "    u\"\\(\\^_\\^\\)\":\"ğŸ˜ƒ\",\n",
    "    u\"\\(\\^\\^\\)\":\"ğŸ˜ƒ\",\n",
    "    u\"\\(\\^J\\^\\)\":\"ğŸ˜ƒ\",\n",
    "    u\"\\(\\*\\^\\.\\^\\*\\)\":\"ğŸ˜ƒ\",\n",
    "    u\"\\(\\^â€”\\^\\ï¼‰\":\"ğŸ˜ƒ\",\n",
    "    u\"\\(#\\^\\.\\^#\\)\":\"ğŸ˜ƒ\",\n",
    "    u\"\\ï¼ˆ\\^â€”\\^\\ï¼‰\":\"ğŸ‘‹\",\n",
    "    u\"\\(;_;\\)/~~~\":\"ğŸ‘‹\",\n",
    "    u\"\\(\\^\\.\\^\\)/~~~\":\"ğŸ‘‹\",\n",
    "    u\"\\(T_T\\)/~~~\":\"ğŸ‘‹\",\n",
    "    u\"\\(ToT\\)/~~~\":\"ğŸ‘‹\",\n",
    "    u\"\\(\\*\\^0\\^\\*\\)\":\"ğŸ˜\",\n",
    "    u\"\\(\\*_\\*\\)\":\"ğŸ˜\",\n",
    "    u\"\\(\\*_\\*;\":\"ğŸ˜\",\n",
    "    u\"\\(\\+_\\+\\) \\(@_@\\)\":\"ğŸ˜\",\n",
    "    u\"\\(\\*\\^\\^\\)v\":\"ğŸ˜‚\",\n",
    "    u\"\\(\\^_\\^\\)v\":\"ğŸ˜‚\",\n",
    "    u'\\(-\"-\\)':\"ğŸ˜“\",\n",
    "    u\"\\(ãƒ¼ãƒ¼;\\)\":\"ğŸ˜“\",\n",
    "    u\"\\(\\^0_0\\^\\)\":\"ğŸ˜\",\n",
    "    u\"\\(\\ï¼¾ï½–\\ï¼¾\\)\":\"ğŸ˜€\",\n",
    "    u\"\\(\\ï¼¾ï½•\\ï¼¾\\)\":\"ğŸ˜€\",\n",
    "    u\"\\(\\^\\)o\\(\\^\\)\":\"ğŸ˜€\",\n",
    "    u\"\\(\\^O\\^\\)\":\"ğŸ˜€\",\n",
    "    u\"\\(\\^o\\^\\)\":\"ğŸ˜€\",\n",
    "    u\"\\)\\^o\\^\\(\":\"ğŸ˜€\",\n",
    "    u\":O o_O\":\"ğŸ˜®\",\n",
    "    u\"o_0\":\"ğŸ˜®\",\n",
    "    u\"o\\.O\":\"ğŸ˜®\",\n",
    "    u\"\\(o\\.o\\)\":\"ğŸ˜®\",\n",
    "    u\"oO\":\"ğŸ˜®\",\n",
    "    u\"\\(\\*ï¿£mï¿£\\)\":\"ğŸ˜ \",\n",
    "    u\":â€‘)\":\"â˜ºï¸\",\n",
    "    u\":)\":\"â˜ºï¸\",\n",
    "    u\":-]\":\"â˜ºï¸\",\n",
    "    u\":]\":\"â˜ºï¸\",\n",
    "    u\":-3\":\"â˜ºï¸\",\n",
    "    u\":3\":\"â˜ºï¸\",\n",
    "    u\":->\":\"â˜ºï¸\",\n",
    "    u\":>\":\"â˜ºï¸\",\n",
    "    u\"8-)\":\"â˜ºï¸\",\n",
    "    u\":o)\":\"â˜ºï¸\",\n",
    "    u\":-}\":\"â˜ºï¸\",\n",
    "    u\":}\":\"â˜ºï¸\",\n",
    "    u\":-)\":\"â˜ºï¸\",\n",
    "    u\":c)\":\"â˜ºï¸\",\n",
    "    u\":^)\":\"â˜ºï¸\",\n",
    "    u\"=]\":\"â˜ºï¸\",\n",
    "    u\"=)\":\"â˜ºï¸\",\n",
    "    u\":â€‘D\":\"ğŸ˜ƒ\",\n",
    "    u\":D\":\"ğŸ˜ƒ\",\n",
    "    u\"8â€‘D\":\"ğŸ˜ƒ\",\n",
    "    u\"8D\":\"ğŸ˜ƒ\",\n",
    "    u\"Xâ€‘D\":\"ğŸ˜ƒ\",\n",
    "    u\"XD\":\"ğŸ˜ƒ\",\n",
    "    u\"=D\":\"ğŸ˜ƒ\",\n",
    "    u\"=3\":\"ğŸ˜ƒ\",\n",
    "    u\"B^D\":\"ğŸ˜ƒ\",\n",
    "    u\":-))\":\"ğŸ˜ƒ\",\n",
    "    u\":-(\":\"â˜¹ï¸\",\n",
    "    u\":â€‘(\":\"â˜¹ï¸\",\n",
    "    u\":(\":\"â˜¹ï¸\",\n",
    "    u\":â€‘c\":\"â˜¹ï¸\",\n",
    "    u\":c\":\"â˜¹ï¸\",\n",
    "    u\":â€‘<\":\"â˜¹ï¸\",\n",
    "    u\":<\":\"â˜¹ï¸\",\n",
    "    u\":â€‘[\":\"â˜¹ï¸\",\n",
    "    u\":[\":\"â˜¹ï¸\",\n",
    "    u\":-||\":\"â˜¹ï¸\",\n",
    "    u\">:[\":\"â˜¹ï¸\",\n",
    "    u\":{\":\"â˜¹ï¸\",\n",
    "    u\":@\":\"â˜¹ï¸\",\n",
    "    u\">:(\":\"â˜¹ï¸\",\n",
    "    u\":'â€‘(\":\"ğŸ˜­\",\n",
    "    u\":'(\":\"ğŸ˜­\",\n",
    "    u\":'â€‘)\":\"ğŸ˜ƒ\",\n",
    "    u\":')\":\"ğŸ˜ƒ\",\n",
    "    u\"Dâ€‘':\":\"ğŸ˜§\",\n",
    "    u\"D:<\":\"ğŸ˜¨\",\n",
    "    u\"D:\":\"ğŸ˜§\",\n",
    "    u\"D8\":\"ğŸ˜§\",\n",
    "    u\"D;\":\"ğŸ˜§\",\n",
    "    u\"D=\":\"ğŸ˜§\",\n",
    "    u\"DX\":\"ğŸ˜§\",\n",
    "    u\":â€‘O\":\"ğŸ˜®\",\n",
    "    u\":O\":\"ğŸ˜®\",\n",
    "    u\":â€‘o\":\"ğŸ˜®\",\n",
    "    u\":o\":\"ğŸ˜®\",\n",
    "    u\":-0\":\"ğŸ˜®\",\n",
    "    u\"8â€‘0\":\"ğŸ˜®\",\n",
    "    u\">:O\":\"ğŸ˜®\",\n",
    "    u\":-*\":\"ğŸ˜—\",\n",
    "    u\":*\":\"ğŸ˜—\",\n",
    "    u\":X\":\"ğŸ˜—\",\n",
    "    u\";â€‘)\":\"ğŸ˜‰\",\n",
    "    u\";)\":\"ğŸ˜‰\",\n",
    "    u\"*-)\":\"ğŸ˜‰\",\n",
    "    u\"*)\":\"ğŸ˜‰\",\n",
    "    u\";â€‘]\":\"ğŸ˜‰\",\n",
    "    u\";]\":\"ğŸ˜‰\",\n",
    "    u\";^)\":\"ğŸ˜‰\",\n",
    "    u\":â€‘,\":\"ğŸ˜‰\",\n",
    "    u\";D\":\"ğŸ˜‰\",\n",
    "    u\":â€‘P\":\"ğŸ˜›\",\n",
    "    u\":P\":\"ğŸ˜›\",\n",
    "    u\"Xâ€‘P\":\"ğŸ˜›\",\n",
    "    u\"XP\":\"ğŸ˜›\",\n",
    "    u\":â€‘Ã\":\"ğŸ˜›\",\n",
    "    u\":Ã\":\"ğŸ˜›\",\n",
    "    u\":b\":\"ğŸ˜›\",\n",
    "    u\"d:\":\"ğŸ˜›\",\n",
    "    u\"=p\":\"ğŸ˜›\",\n",
    "    u\">:P\":\"ğŸ˜›\",\n",
    "    u\":â€‘/\":\"ğŸ˜•\",\n",
    "    u\":/\":\"ğŸ˜•\",\n",
    "    u\":-[.]\":\"ğŸ˜•\",\n",
    "    u\">:[(\\)]\":\"ğŸ˜•\",\n",
    "    u\">:/\":\"ğŸ˜•\",\n",
    "    u\":[(\\)]\":\"ğŸ˜•\",\n",
    "    u\"=/\":\"ğŸ˜•\",\n",
    "    u\"=[(\\)]\":\"ğŸ˜•\",\n",
    "    u\":L\":\"ğŸ˜•\",\n",
    "    u\"=L\":\"ğŸ˜•\",\n",
    "    u\":S\":\"ğŸ˜•\",\n",
    "    u\":â€‘|\":\"ğŸ˜\",\n",
    "    u\":|\":\"ğŸ˜\",\n",
    "    u\":$\":\"ğŸ˜³\",\n",
    "    u\":â€‘x\":\"ğŸ¤\",\n",
    "    u\":x\":\"ğŸ¤\",\n",
    "    u\":â€‘#\":\"ğŸ¤\",\n",
    "    u\":#\":\"ğŸ¤\",\n",
    "    u\":â€‘&\":\"ğŸ¤\",\n",
    "    u\":&\":\"ğŸ¤\",\n",
    "    u\"O:â€‘)\":\"ğŸ˜‡\",\n",
    "    u\"O:)\":\"ğŸ˜‡\",\n",
    "    u\"0:â€‘3\":\"ğŸ˜‡\",\n",
    "    u\"0:3\":\"ğŸ˜‡\",\n",
    "    u\"0:â€‘)\":\"ğŸ˜‡\",\n",
    "    u\"0:)\":\"ğŸ˜‡\",\n",
    "    u\":â€‘b\":\"ğŸ˜›\",\n",
    "    u\"0;^)\":\"ğŸ˜‡\",\n",
    "    u\">:â€‘)\":\"ğŸ˜ˆ\",\n",
    "    u\">:)\":\"ğŸ˜ˆ\",\n",
    "    u\"}:â€‘)\":\"ğŸ˜ˆ\",\n",
    "    u\"}:)\":\"ğŸ˜ˆ\",\n",
    "    u\"3:â€‘)\":\"ğŸ˜ˆ\",\n",
    "    u\"3:)\":\"ğŸ˜ˆ\",\n",
    "    u\">;)\":\"ğŸ˜ˆ\",\n",
    "    u\"|;â€‘)\":\"ğŸ˜\",\n",
    "    u\"|â€‘O\":\"ğŸ˜\",\n",
    "    u\":â€‘J\":\"ğŸ˜\",\n",
    "    u\"%â€‘)\":\"ğŸ˜µ\",\n",
    "    u\"%)\":\"ğŸ˜µ\",\n",
    "    u\":-###..\":\"ğŸ¤’\",\n",
    "    u\":###..\":\"ğŸ¤’\",\n",
    "    u\"(>_<)\":\"ğŸ˜£\",\n",
    "    u\"(>_<)>\":\"ğŸ˜£\",\n",
    "    u\"(';')\":\"Baby\",\n",
    "    u\"(^^>``\":\"ğŸ˜“\",\n",
    "    u\"(^_^;)\":\"ğŸ˜“\",\n",
    "    u\"(-_-;)\":\"ğŸ˜“\",\n",
    "    u\"(~_~;) (ãƒ».ãƒ»;)\":\"ğŸ˜“\",\n",
    "    u\"(-_-)zzz\":\"ğŸ˜´\",\n",
    "    u\"(^_-)\":\"ğŸ˜‰\",\n",
    "    u\"((+_+))\":\"ğŸ˜•\",\n",
    "    u\"(+o+)\":\"ğŸ˜•\",\n",
    "    u\"^_^\":\"ğŸ˜ƒ\",\n",
    "    u\"(^_^)/\":\"ğŸ˜ƒ\",\n",
    "    u\"(^O^)ï¼\":\"ğŸ˜ƒ\",\n",
    "    u\"(^o^)ï¼\":\"ğŸ˜ƒ\",\n",
    "    u\"(__)\":\"ğŸ™‡\",\n",
    "    u\"_(._.)_\":\"ğŸ™‡\",\n",
    "    u\"<(_ _)>\":\"ğŸ™‡\",\n",
    "    u\"<m(__)m>\":\"ğŸ™‡\",\n",
    "    u\"m(__)m\":\"ğŸ™‡\",\n",
    "    u\"m(_ _)m\":\"ğŸ™‡\",\n",
    "    u\"('_')\":\"ğŸ˜­\",\n",
    "    u\"(/_;)\":\"ğŸ˜­\",\n",
    "    u\"(T_T) (;_;)\":\"ğŸ˜­\",\n",
    "    u\"(;_;\":\"ğŸ˜­\",\n",
    "    u\"(;_:)\":\"ğŸ˜­\",\n",
    "    u\"(;O;)\":\"ğŸ˜­\",\n",
    "    u\"(:_;)\":\"ğŸ˜­\",\n",
    "    u\"(ToT)\":\"ğŸ˜­\",\n",
    "    u\";_;\":\"ğŸ˜­\",\n",
    "    u\";-;\":\"ğŸ˜­\",\n",
    "    u\";n;\":\"ğŸ˜­\",\n",
    "    u\";;\":\"ğŸ˜­\",\n",
    "    u\"Q.Q\":\"ğŸ˜­\",\n",
    "    u\"T.T\":\"ğŸ˜­\",\n",
    "    u\"QQ\":\"ğŸ˜­\",\n",
    "    u\"Q_Q\":\"ğŸ˜­\",\n",
    "    u\"(-.-)\":\"ğŸ˜\",\n",
    "    u\"(-_-)\":\"ğŸ˜\",\n",
    "    u\"(ä¸€ä¸€)\":\"ğŸ˜\",\n",
    "    u\"(ï¼›ä¸€_ä¸€)\":\"ğŸ˜\",\n",
    "    u\"(=_=)\":\"ğŸ˜©\",\n",
    "    u\"(=^Â·^=)\":\"ğŸ˜º\",\n",
    "    u\"(=^Â·Â·^=)\":\"ğŸ˜º\",\n",
    "    u\"=_^= \":\"ğŸ˜º\",\n",
    "    u\"(..)\":\"ğŸ˜”\",\n",
    "    u\"(._.)\":\"ğŸ˜”\",\n",
    "    u\"(ãƒ»ãƒ»?\":\"ğŸ˜•\",\n",
    "    u\"(?_?)\":\"ğŸ˜•\",\n",
    "    u\">^_^<\":\"ğŸ˜ƒ\",\n",
    "    u\"<^!^>\":\"ğŸ˜ƒ\",\n",
    "    u\"^/^\":\"ğŸ˜ƒ\",\n",
    "    u\"ï¼ˆ*^_^*ï¼‰\" :\"ğŸ˜ƒ\",\n",
    "    u\"(^<^) (^.^)\":\"ğŸ˜ƒ\",\n",
    "    u\"(^^)\":\"ğŸ˜ƒ\",\n",
    "    u\"(^.^)\":\"ğŸ˜ƒ\",\n",
    "    u\"(^_^.)\":\"ğŸ˜ƒ\",\n",
    "    u\"(^_^)\":\"ğŸ˜ƒ\",\n",
    "    u\"(^^)\":\"ğŸ˜ƒ\",\n",
    "    u\"(^J^)\":\"ğŸ˜ƒ\",\n",
    "    u\"(*^.^*)\":\"ğŸ˜ƒ\",\n",
    "    u\"(^â€”^ï¼‰\":\"ğŸ˜ƒ\",\n",
    "    u\"(#^.^#)\":\"ğŸ˜ƒ\",\n",
    "    u\"ï¼ˆ^â€”^ï¼‰\":\"ğŸ‘‹\",\n",
    "    u\"(;_;)/~~~\":\"ğŸ‘‹\",\n",
    "    u\"(^.^)/~~~\":\"ğŸ‘‹\",\n",
    "    u\"(-_-)/~~~ ($Â·Â·)/~~~\":\"ğŸ‘‹\",\n",
    "    u\"(T_T)/~~~\":\"ğŸ‘‹\",\n",
    "    u\"(ToT)/~~~\":\"ğŸ‘‹\",\n",
    "    u\"(*^0^*)\":\"ğŸ˜\",\n",
    "    u\"(*_*)\":\"ğŸ˜\",\n",
    "    u\"(*_*;\":\"ğŸ˜\",\n",
    "    u\"(+_+) (@_@)\":\"ğŸ˜\",\n",
    "    u\"(*^^)v\":\"ğŸ˜‚\",\n",
    "    u\"(^_^)v\":\"ğŸ˜‚\",\n",
    "    u'(-\"-)':\"ğŸ˜“\",\n",
    "    u\"(ãƒ¼ãƒ¼;)\":\"ğŸ˜“\",\n",
    "    u\"(^0_0^)\":\"ğŸ˜\",\n",
    "    u\"(ï¼¾ï½–ï¼¾)\":\"ğŸ˜€\",\n",
    "    u\"(ï¼¾ï½•ï¼¾)\":\"ğŸ˜€\",\n",
    "    u\"(^)o(^)\":\"ğŸ˜€\",\n",
    "    u\"(^O^)\":\"ğŸ˜€\",\n",
    "    u\"(^o^)\":\"ğŸ˜€\",\n",
    "    u\")^o^(\":\"ğŸ˜€\",\n",
    "    u\":O o_O\":\"ğŸ˜®\",\n",
    "    u\"o_0\":\"ğŸ˜®\",\n",
    "    u\"o.O\":\"ğŸ˜®\",\n",
    "    u\"(o.o)\":\"ğŸ˜®\",\n",
    "    u\"oO\":\"ğŸ˜®\",\n",
    "}\n",
    "\n",
    "\n",
    "def str2emoji(tweet):\n",
    "\t\n",
    "\tfor pos,ej in enumerate(tweet):\n",
    "\t\tif ej in emojis:\n",
    "\t\t\ttweet[pos]=emojis[ej]\n",
    "\treturn tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UjpS6y1RKeeZ"
   },
   "outputs": [],
   "source": [
    "def norm_tweet(tweet):\n",
    "  tweet = re.sub(r\"\\\\u2019\", \"'\", tweet)\n",
    "  tweet = re.sub(r\"\\\\u002c\", \",\", tweet)\n",
    "  tweet=' '.join(str2emoji(unidecode(tweet).lower().split()))\n",
    "  tweet = re.sub(r\"\\'ve\", \" have\", tweet)\n",
    "  tweet = re.sub(r\" can\\'t\", \" cannot\", tweet)\n",
    "  tweet = re.sub(r\"n\\'t\", \" not\", tweet)\n",
    "  tweet = re.sub(r\"\\'re\", \" are\", tweet)\n",
    "  tweet = re.sub(r\"\\'d\", \" would\", tweet)\n",
    "  tweet = re.sub(r\"\\'ll\", \" will\", tweet)\n",
    "  tweet = re.sub(r\"\\'s\", \"\", tweet)\n",
    "  tweet = re.sub(r\"\\'n\", \"\", tweet)\n",
    "  tweet = re.sub(r\"\\'m\", \" am\", tweet)\n",
    "  tweet = re.sub(r\"@\\w+\", r' ',tweet)\n",
    "  tweet = re.sub(r\"#\\w+\", r' ',tweet)\n",
    "  tweet = re.sub(r\"[.]+\",\" \",tweet)\n",
    "  #T = tokenizer.TweetTokenizer()\n",
    "  tweet = T.tokenize(tweet)\n",
    "  #tweet = [lemmatizer.lemmatize(i,j[0].lower()) if j[0].lower() in ['a','n','v']  else lemmatizer.lemmatize(i) for i,j in pos_tag(tknzr.tokenize(tweet))]\n",
    "  tweet = [ i for i in tweet if (i not in stopwords) and (i not in string.punctuation ) ]\n",
    "  #tweet = ' '.join(tweet)\n",
    "  return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Rv6ih5wKoHV"
   },
   "outputs": [],
   "source": [
    "def data_preprocessing(data):\n",
    "  data['data'] = data['data'].apply(norm_tweet)\n",
    "  if data['sentiment'].dtypes not in [np.int64]:\n",
    "    data['sentiment']=data['sentiment'].apply(value_sentiment)\n",
    "  return data['data'],data['sentiment'],data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import scipy.stats\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score, jaccard_similarity_score\n",
    "\n",
    "def cohen_kappa_score(y1, y2, labels=None, weights=None):\n",
    "    \"\"\"Cohen's kappa: a statistic that measures inter-annotator agreement.\n",
    "\n",
    "    This function computes Cohen's kappa [1]_, a score that expresses the level\n",
    "    of agreement between two annotators on a classification problem. It is\n",
    "    defined as\n",
    "\n",
    "    .. math::\n",
    "        \\kappa = (p_o - p_e) / (1 - p_e)\n",
    "\n",
    "    where :math:`p_o` is the empirical probability of agreement on the label\n",
    "    assigned to any sample (the observed agreement ratio), and :math:`p_e` is\n",
    "    the expected agreement when both annotators assign labels randomly.\n",
    "    :math:`p_e` is estimated using a per-annotator empirical prior over the\n",
    "    class labels [2]_.\n",
    "\n",
    "    Read more in the :ref:`User Guide <cohen_kappa>`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y1 : array, shape = [n_samples]\n",
    "        Labels assigned by the first annotator.\n",
    "\n",
    "    y2 : array, shape = [n_samples]\n",
    "        Labels assigned by the second annotator. The kappa statistic is\n",
    "        symmetric, so swapping ``y1`` and ``y2`` doesn't change the value.\n",
    "\n",
    "    labels : array, shape = [n_classes], optional\n",
    "        List of labels to index the matrix. This may be used to select a\n",
    "        subset of labels. If None, all labels that appear at least once in\n",
    "        ``y1`` or ``y2`` are used.\n",
    "\n",
    "    weights : str, optional\n",
    "        List of weighting type to calculate the score. None means no weighted;\n",
    "        \"linear\" means linear weighted; \"quadratic\" means quadratic weighted.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    kappa : float\n",
    "        The kappa statistic, which is a number between -1 and 1. The maximum\n",
    "        value means complete agreement; zero or lower means chance agreement.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] J. Cohen (1960). \"A coefficient of agreement for nominal scales\".\n",
    "           Educational and Psychological Measurement 20(1):37-46.\n",
    "           doi:10.1177/001316446002000104.\n",
    "    .. [2] `R. Artstein and M. Poesio (2008). \"Inter-coder agreement for\n",
    "           computational linguistics\". Computational Linguistics 34(4):555-596.\n",
    "           <http://www.mitpressjournals.org/doi/abs/10.1162/coli.07-034-R2#.V0J1MJMrIWo>`_\n",
    "    .. [3] `Wikipedia entry for the Cohen's kappa.\n",
    "            <https://en.wikipedia.org/wiki/Cohen%27s_kappa>`_\n",
    "    \"\"\"\n",
    "    confusion = metrics.confusion_matrix(y1, y2, labels=labels)\n",
    "    n_classes = confusion.shape[0]\n",
    "    sum0 = np.sum(confusion, axis=0)\n",
    "    sum1 = np.sum(confusion, axis=1)\n",
    "    expected = np.outer(sum0, sum1)*1.0 / np.sum(sum0)\n",
    "\n",
    "    if weights is None:\n",
    "        w_mat = np.ones([n_classes, n_classes], dtype=np.int)\n",
    "        w_mat.flat[:: n_classes + 1] = 0\n",
    "    elif weights == \"linear\" or weights == \"quadratic\":\n",
    "        w_mat = np.zeros([n_classes, n_classes], dtype=np.int)\n",
    "        w_mat += np.arange(n_classes)\n",
    "        if weights == \"linear\":\n",
    "            w_mat = np.abs(w_mat - w_mat.T)\n",
    "        else:\n",
    "            w_mat = (w_mat - w_mat.T) ** 2\n",
    "    else:\n",
    "        raise ValueError(\"Unknown kappa weighting type.\")\n",
    "\n",
    "    k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
    "    return 1 - k\n",
    "\n",
    "\n",
    "def evaluate_ei(pred,gold):  \n",
    "    \"\"\"Calculates performance metrics for regression.\n",
    "    \n",
    "    :param pred: the file path of the predictions\n",
    "    :param gold: the filte path withe gold data\n",
    "    :return: a list with performace metrics.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    f=open(pred, \"rb\")\n",
    "    pred_lines=f.readlines()\n",
    "    f.close()\n",
    "    \n",
    "    f=open(gold, \"rb\")\n",
    "    gold_lines=f.readlines()\n",
    "    f.close()    \n",
    "\n",
    "    if(len(pred_lines)==len(gold_lines)):       \n",
    "        # align tweets ids with gold scores and predictions\n",
    "        data_dic={}\n",
    "\n",
    "\n",
    "        header=True        \n",
    "        for line in gold_lines:\n",
    "            #line=line.decode('utf-8')\n",
    "            \n",
    "            if header:\n",
    "                header=False\n",
    "                continue\n",
    "\n",
    "            parts=line.split('\\t')\n",
    "            if len(parts)==4:\n",
    "                # tweet ids containing the word mystery are discarded\n",
    "                if(not 'mystery' in parts[0]):\n",
    "                    data_dic[parts[0]]=[float(line.split('\\t')[3])]\n",
    "            else:\n",
    "                sys.exit('Format problem in '+os.path.basename(gold)+'. Please report this problem to the task organizers.')\n",
    "\n",
    "        header=True        \n",
    "        for line in pred_lines:\n",
    "            \n",
    "            if header:\n",
    "                header=False\n",
    "                continue\n",
    "\n",
    "            parts=line.split('\\t')      \n",
    "            if len(parts)==4:\n",
    "                # tweet ids containing the word mystery are discarded\n",
    "                if(not 'mystery' in parts[0]):\n",
    "                    if parts[0] in data_dic:\n",
    "                        try:\n",
    "                            data_dic[parts[0]].append(float(line.split('\\t')[3]))\n",
    "                        except ValueError:\n",
    "                            # Invalid predictions are replaced by a default value\n",
    "                            data_dic[parts[0]].append(0.5)\n",
    "                    else:\n",
    "                        sys.exit('Invalid tweet id ('+parts[0]+') in '+os.path.basename(pred)+'.')\n",
    "            else:\n",
    "                sys.exit('Format problem in '+os.path.basename(pred)+'.') \n",
    "            \n",
    "        \n",
    "        # lists storing gold and prediction scores\n",
    "        gold_scores=[]  \n",
    "        pred_scores=[]\n",
    "         \n",
    "        \n",
    "        # lists storing gold and prediction scores where gold score >= 0.5\n",
    "        gold_scores_range_05_1=[]\n",
    "        pred_scores_range_05_1=[]\n",
    "            \n",
    "        for id in data_dic:\n",
    "            if(len(data_dic[id])==2):\n",
    "                gold_scores.append(data_dic[id][0])\n",
    "                pred_scores.append(data_dic[id][1])\n",
    "                \n",
    "                if(data_dic[id][0]>=0.5):\n",
    "                    gold_scores_range_05_1.append(data_dic[id][0])\n",
    "                    pred_scores_range_05_1.append(data_dic[id][1])\n",
    "                \n",
    "            else:\n",
    "                sys.exit('Repeated id ('+id+') in '+os.path.basename(pred)+' .')\n",
    "\n",
    "                \n",
    "                    \n",
    "      \n",
    "        # return zero correlation if predictions are constant\n",
    "        if np.std(pred_scores)==0 or np.std(gold_scores)==0:\n",
    "            return (0,0)\n",
    "        \n",
    "\n",
    "        pears_corr=scipy.stats.pearsonr(pred_scores,gold_scores)[0]                                                 \n",
    "        pears_corr_range_05_1=scipy.stats.pearsonr(pred_scores_range_05_1,gold_scores_range_05_1)[0]                         \n",
    "        return (pears_corr,pears_corr_range_05_1)                        \n",
    "                          \n",
    "    else:\n",
    "        sys.exit('Predictions ('+os.path.basename(pred)+') and gold data ('+os.path.basename(gold)+') have different number of lines.')          \n",
    "        \n",
    "        \n",
    "def evaluate_oc(pred,gold):  \n",
    "    \"\"\"Calculates performance metrics for ordinal classification.\n",
    "    \n",
    "    :param pred: the file path of the predictions\n",
    "    :param gold: the filte path withe gold data\n",
    "    :return: a list with performace metrics.\n",
    "    \n",
    "    \"\"\"\n",
    "  \n",
    "   \n",
    "    f=open(pred, \"rb\")\n",
    "    pred_lines=f.readlines()\n",
    "    f.close()\n",
    "    \n",
    "    f=open(gold, \"rb\")\n",
    "    gold_lines=f.readlines()\n",
    "    f.close()    \n",
    "\n",
    "\n",
    "    if(len(pred_lines)==len(gold_lines)):       \n",
    "        # align tweets ids with gold scores and predictions\n",
    "        data_dic={}\n",
    "\n",
    "    \n",
    "        header=True        \n",
    "        for line in gold_lines:\n",
    "            \n",
    "            if header:\n",
    "                header=False\n",
    "                continue\n",
    "            \n",
    "            parts=line.split('\\t')\n",
    "            \n",
    "            label=int(parts[3].split(\":\")[0])\n",
    "            \n",
    "            if len(parts)==4:   \n",
    "                data_dic[parts[0]]=[label]\n",
    "                \n",
    "            else:\n",
    "                sys.exit('Format problem in '+os.path.basename(gold)+'. Please report this problem to the task organizers.')\n",
    "        \n",
    "        \n",
    "        header=True         \n",
    "        for line in pred_lines:\n",
    "            if header:\n",
    "                header=False\n",
    "                continue            \n",
    "            parts=line.split('\\t')   \n",
    "            label=int(parts[3].split(\":\")[0])\n",
    "            if len(parts)==4:  \n",
    "                if parts[0] in data_dic:\n",
    "                    try:\n",
    "                        data_dic[parts[0]].append(label)\n",
    "                    except ValueError:\n",
    "                        # Invalid predictions are replaced by a default value\n",
    "                        data_dic[parts[0]].append(int(0))\n",
    "                else:\n",
    "                    sys.exit('Invalid tweet id ('+parts[0]+') in '+os.path.basename(pred)+'.')\n",
    "            else:\n",
    "                sys.exit('Format problem in '+os.path.basename(pred)+'.') \n",
    "            \n",
    "        \n",
    "        # lists storing gold and prediction scores\n",
    "        gold_scores=[]  \n",
    "        pred_scores=[]\n",
    "         \n",
    "        \n",
    "        # lists storing gold and prediction scores where gold score >= 0.5\n",
    "        gold_scores_some=[]\n",
    "        pred_scores_some=[]\n",
    "            \n",
    "        for id in data_dic:\n",
    "            if(len(data_dic[id])==2):\n",
    "                gold_scores.append(data_dic[id][0])\n",
    "                pred_scores.append(data_dic[id][1])\n",
    "                \n",
    "                if(data_dic[id][0]!=0):\n",
    "                    gold_scores_some.append(data_dic[id][0])\n",
    "                    pred_scores_some.append(data_dic[id][1])\n",
    "                \n",
    "            else:\n",
    "                sys.exit('Repeated id ('+id+') in '+os.path.basename(pred)+' .')\n",
    "\n",
    "                \n",
    "                \n",
    "        # return null scores if predictions are constant\n",
    "        if np.std(pred_scores)==0 or np.std(gold_scores)==0:\n",
    "            return (0,0,0,0)\n",
    "\n",
    "        pears_corr=scipy.stats.pearsonr(pred_scores,gold_scores)[0]              \n",
    "        pears_corr_some=scipy.stats.pearsonr(pred_scores_some,gold_scores_some)[0]  \n",
    "        \n",
    "        # fix labels to values observed in gold data        \n",
    "        gold_labels=list(sorted(set(gold_scores)))\n",
    "       \n",
    "        kappa=cohen_kappa_score(pred_scores,gold_scores,labels=gold_labels, weights='quadratic')        \n",
    "        kappa_some=cohen_kappa_score(pred_scores_some,gold_scores_some, labels=gold_labels, weights='quadratic')\n",
    " \n",
    "        return (pears_corr,pears_corr_some,kappa,kappa_some)\n",
    "\n",
    "    else:\n",
    "        sys.exit('Predictions ('+os.path.basename(pred)+') and gold data ('+os.path.basename(gold)+') have different number of lines.')          \n",
    "  \n",
    "        \n",
    "def evaluate_multilabel(pred,gold):  \n",
    "    \"\"\"Calculates performance metrics for multi-label classification.\n",
    "    \n",
    "    :param pred: the file path of the predictions\n",
    "    :param gold: the filte path withe gold data\n",
    "    :return: a list with performace metrics.    \n",
    "    \"\"\"     \n",
    "    \n",
    "    f=open(pred, \"rb\")\n",
    "    pred_lines=f.readlines()\n",
    "    f.close()\n",
    "    \n",
    "    f=open(gold, \"rb\")\n",
    "    gold_lines=f.readlines()\n",
    "    f.close()    \n",
    "\n",
    "    if(len(pred_lines)==len(gold_lines)):       \n",
    "        # align tweets ids with gold scores and predictions\n",
    "        data_dic={}\n",
    "\n",
    "    \n",
    "        header=True        \n",
    "        for line in gold_lines:\n",
    "           \n",
    "            if header:\n",
    "                header=False\n",
    "                continue\n",
    "            \n",
    "            parts=line.split('\\t')\n",
    "            \n",
    "            if len(parts)==13:  \n",
    "                labels=[]\n",
    "                for m_label in parts[2:13]:\n",
    "                    labels.append(int(m_label))\n",
    " \n",
    "                data_dic[parts[0]]=[tuple(labels)]\n",
    "                \n",
    "            else:\n",
    "                sys.exit('Format problem in '+os.path.basename(gold)+'. Please report this problem to the task organizers.')\n",
    "        \n",
    "        header=True         \n",
    "        for line in pred_lines:\n",
    "            if header:\n",
    "                header=False\n",
    "                continue            \n",
    "            parts=line.split('\\t')   \n",
    "            if len(parts)==13:  \n",
    "                if parts[0] in data_dic:\n",
    "                    try:\n",
    "                        labels=[]\n",
    "                        for m_label in parts[2:13]:\n",
    "                            labels.append(int(m_label))\n",
    "                        data_dic[parts[0]].append(tuple(labels))\n",
    "                    except ValueError:\n",
    "                        # Invalid predictions are replaced by a default value\n",
    "                        data_dic[parts[0]].append((0,0,0,0,0,0,0,0,0,0,0))\n",
    "                else:\n",
    "                    sys.exit('Invalid tweet id in '+os.path.basename(pred)+'.')\n",
    "            else:\n",
    "                sys.exit('Format problem in '+os.path.basename(pred)+'.')   \n",
    "            \n",
    "\n",
    "       # lists storing gold and prediction scores\n",
    "        gold_scores=[]  \n",
    "        pred_scores=[]\n",
    "         \n",
    "            \n",
    "        for id in data_dic:\n",
    "            if(len(data_dic[id])==2):\n",
    "                gold_scores.append(data_dic[id][0])\n",
    "                pred_scores.append(data_dic[id][1])\n",
    "                \n",
    "            else:\n",
    "                sys.exit('Repeated id ('+id+') in '+os.path.basename(pred)+' .')\n",
    "\n",
    "\n",
    "        y_true = np.array(gold_scores)\n",
    "        y_pred = np.array(pred_scores)       \n",
    "    \n",
    "        acc=jaccard_similarity_score(y_true,y_pred)       \n",
    "        f1_micro=f1_score(y_true, y_pred, average='micro')  \n",
    "        f1_macro=f1_score(y_true, y_pred, average='macro')  \n",
    "            \n",
    "        return (acc,f1_micro,f1_macro)\n",
    "    \n",
    "    else:\n",
    "        sys.exit('Predictions ('+os.path.basename(pred)+') and gold data ('+os.path.basename(gold)+') have different number of lines.')          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zxXHdW0IODWx"
   },
   "outputs": [],
   "source": [
    "def model1(x_train_3, y_train_3,x_val_3, y_val_3, embedding_layer):\n",
    "\n",
    "\tmodel1 = Sequential()\n",
    "\tmodel1.add(embedding_layer)\n",
    "\tmodel1.add(LSTM(32))\n",
    "\tmodel1.add(Dropout(0.2))\n",
    "\tmodel1.add(Dense(32, activation='relu'))\n",
    "\tmodel1.add(Dropout(0.2))\n",
    "\tmodel1.add(Dense(3, activation='softmax'))\n",
    "\tmodel1.compile(loss='categorical_crossentropy',\n",
    "\t\t\t      optimizer='Adam',\n",
    "\t\t\t      metrics=['acc'])\n",
    "\tmodel1.summary()\n",
    "\thistory=model1.fit(x_train_3, y_train_3, validation_data=(x_val_3, y_val_3),epochs=6, batch_size=50)\n",
    "\tmodel1.save(\"./model1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sdLO3C6WODMT"
   },
   "outputs": [],
   "source": [
    "def model2(x_train_3, y_train_3,x_val_3, y_val_3, embedding_layer,epochs, batch_size):\n",
    "\tmodel2 = Sequential()\n",
    "\tmodel2.add(embedding_layer)\n",
    "\tmodel2.add(GRU(32))\n",
    "\tmodel2.add(Dropout(0.2))\n",
    "\tmodel2.add(Dense(3, activation='softmax'))\n",
    "\tmodel2.compile(loss='categorical_crossentropy',\n",
    "\t\t\t      optimizer='rmsprop',\n",
    "\t\t\t      metrics=['acc'])\n",
    "\tmodel2.summary()\n",
    "\thistory=model2.fit(x_train_3, y_train_3, validation_data=(x_val_3, y_val_3),epochs=epochs, batch_size=batch_size)\n",
    "\tmodel2.save(\"./model2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OQiBIkDmKoEU"
   },
   "outputs": [],
   "source": [
    "#Path of datas\n",
    "data_train_3 =  \"train_1_1.csv\"\n",
    "data_train_7 =  \"train_3_3.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FezwSR_SLp4p"
   },
   "outputs": [],
   "source": [
    "#Load Data\n",
    "data_train_3= pd.read_csv(data_train_3,sep='\\t',names=['id','sentiment','data'])\n",
    "data_train_7= pd.read_csv(data_train_7,sep='\\t',names=['id','sentiment','data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CTmWa7BuKoBy"
   },
   "outputs": [],
   "source": [
    "# Apply the preprocessiog on the data\n",
    "# Normalize the tweets\n",
    "tweets_train_3, sentiments_train_3,data_train_3 = data_preprocessing(data_train_3)\n",
    "tweets_train_7, sentiments_train_7,data_train_7 = data_preprocessing(data_train_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rlA-OcLQLKwi"
   },
   "outputs": [],
   "source": [
    "all_tweet = tweets_train_3.append(tweets_train_7)\n",
    "tokenizer = Tokenizer(filters=' ')\n",
    "tokenizer.fit_on_texts(all_tweet)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3KWoHuF5MvAZ"
   },
   "outputs": [],
   "source": [
    "sequences_train_3 = tokenizer.texts_to_sequences(tweets_train_3)\n",
    "sequences_train_7 = tokenizer.texts_to_sequences(tweets_train_7)\n",
    "sequences = sequences_train_3 + sequences_train_7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0rER1ZxaLgd1"
   },
   "outputs": [],
   "source": [
    "#Seek the longuest Tweet\n",
    "#To Apply a Padding\n",
    "MAX_SEQUENCE_LENGTH = 0\n",
    "for elt in sequences:\n",
    "\tif len(elt) > MAX_SEQUENCE_LENGTH:\n",
    "\t\tMAX_SEQUENCE_LENGTH = len(elt)\n",
    "  \n",
    "data_train_3 = pad_sequences(sequences_train_3, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_train_7 = pad_sequences(sequences_train_7, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GL0k6uf1Lga3"
   },
   "outputs": [],
   "source": [
    "indices_train_3 = np.arange(data_train_3.shape[0])\n",
    "data_train_3 = data_train_3[indices_train_3]\n",
    "\n",
    "indices_train_7 = np.arange(data_train_7.shape[0])\n",
    "data_train_7 = data_train_7[indices_train_7]\n",
    "\n",
    "labels_train_3 = to_categorical(np.asarray(sentiments_train_3), 3)\n",
    "labels_train_3 = labels_train_3[indices_train_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dEFq7QZbLgSk"
   },
   "outputs": [],
   "source": [
    "nb_words=len(word_index)+1\n",
    "EMBEDDING_DIM=300\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "90yb26nXNvnt"
   },
   "outputs": [],
   "source": [
    "oov=[]\n",
    "oov.append((np.random.rand(EMBEDDING_DIM)) - 1.0)\n",
    "oov = oov / np.linalg.norm(oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8X5fM9nSNvj5"
   },
   "outputs": [],
   "source": [
    "for word, i in word_index.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)\n",
    "    else:\n",
    "        embedding_matrix[i] = oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ofA7BZ1UNvgD"
   },
   "outputs": [],
   "source": [
    "split_idx = int(len(data_train_3)*0.70)\n",
    "x_train_3, x_val_3 = data_train_3[:split_idx], data_train_3[split_idx:]\n",
    "y_train_3, y_val_3 = labels_train_3 [:split_idx], labels_train_3[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RGqBPaa-Nvb8"
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False, name='embedding_layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wc_MJrSoNvYo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 31, 300)           18438300  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                42624     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 18,482,079\n",
      "Trainable params: 43,779\n",
      "Non-trainable params: 18,438,300\n",
      "_________________________________________________________________\n",
      "Train on 35233 samples, validate on 15100 samples\n",
      "Epoch 1/6\n",
      "35233/35233 [==============================] - 9s 255us/step - loss: 0.8455 - acc: 0.6032 - val_loss: 0.8269 - val_acc: 0.6069\n",
      "Epoch 2/6\n",
      "35233/35233 [==============================] - 9s 247us/step - loss: 0.7649 - acc: 0.6588 - val_loss: 0.8023 - val_acc: 0.6179\n",
      "Epoch 3/6\n",
      "35233/35233 [==============================] - 9s 249us/step - loss: 0.7386 - acc: 0.6733 - val_loss: 0.8009 - val_acc: 0.6168\n",
      "Epoch 4/6\n",
      "35233/35233 [==============================] - 9s 248us/step - loss: 0.7219 - acc: 0.6808 - val_loss: 0.8104 - val_acc: 0.6144\n",
      "Epoch 5/6\n",
      "35233/35233 [==============================] - 10s 272us/step - loss: 0.7068 - acc: 0.6883 - val_loss: 0.8193 - val_acc: 0.6170\n",
      "Epoch 6/6\n",
      "35233/35233 [==============================] - 9s 253us/step - loss: 0.6894 - acc: 0.6957 - val_loss: 0.8359 - val_acc: 0.6128\n"
     ]
    }
   ],
   "source": [
    "model1(x_train_3, y_train_3,x_val_3, y_val_3, embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dtZU5RcnNvS2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 31, 300)           18438300  \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 32)                31968     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 18,470,367\n",
      "Trainable params: 32,067\n",
      "Non-trainable params: 18,438,300\n",
      "_________________________________________________________________\n",
      "Train on 35233 samples, validate on 15100 samples\n",
      "Epoch 1/6\n",
      "35233/35233 [==============================] - 9s 259us/step - loss: 0.8336 - acc: 0.6113 - val_loss: 0.8331 - val_acc: 0.5971\n",
      "Epoch 2/6\n",
      "35233/35233 [==============================] - 9s 243us/step - loss: 0.7537 - acc: 0.6634 - val_loss: 0.7968 - val_acc: 0.6262\n",
      "Epoch 3/6\n",
      "35233/35233 [==============================] - 9s 246us/step - loss: 0.7330 - acc: 0.6752 - val_loss: 0.8162 - val_acc: 0.6197\n",
      "Epoch 4/6\n",
      "35233/35233 [==============================] - 9s 252us/step - loss: 0.7169 - acc: 0.6838 - val_loss: 0.7930 - val_acc: 0.6239\n",
      "Epoch 5/6\n",
      "35233/35233 [==============================] - 9s 264us/step - loss: 0.7030 - acc: 0.6914 - val_loss: 0.8110 - val_acc: 0.6233\n",
      "Epoch 6/6\n",
      "35233/35233 [==============================] - 9s 263us/step - loss: 0.6897 - acc: 0.6976 - val_loss: 0.8226 - val_acc: 0.6181\n"
     ]
    }
   ],
   "source": [
    "model2(x_train_3, y_train_3,x_val_3, y_val_3, embedding_layer, 6, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Af6C8eMXNvOF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4      -2\n",
      "       ..\n",
      "1625    3\n",
      "1626   -2\n",
      "1627    1\n",
      "1628    0\n",
      "1629   -2\n",
      "Name: sentiment, Length: 1630, dtype: int64 0                             [yeah, â˜º, ï¸, playing, well]\n",
      "1       [least, guy, trying, discourage, anymore, want...\n",
      "2       [uplift, still, discouraged, means, listening,...\n",
      "3                              [age, heyday, blood, tame]\n",
      "4       [embarrassed, saw, us, like, knvfkkjg, thinks,...\n",
      "                              ...                        \n",
      "1625       [idk, help, someone, smile, comes, lips, n, n]\n",
      "1626    [think, leep, favorite, get, dark, maybe, inso...\n",
      "1627                   [amelia, want, sarah, v, grateful]\n",
      "1628                                  [lack, makes, n, n]\n",
      "1629                   [james, clapper, cary, disturbing]\n",
      "Name: data, Length: 1630, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(sentiments_train_7, tweets_train_7)\n",
    "labels_train_7 = to_categorical(np.asarray(sentiments_train_7), 7)\n",
    "labels_train_7 = labels_train_7[indices_train_7]\n",
    "\n",
    "split_idx = int(len(data_train_7)*0.85)\n",
    "x_train_7, x_val_7 = data_train_7[:split_idx], data_train_7[split_idx:]\n",
    "y_train_7, y_val_7 = labels_train_7 [:split_idx], labels_train_7[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zJAVpUubpzrY",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 31, 300)           18438300  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                42624     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 18,482,079\n",
      "Trainable params: 43,779\n",
      "Non-trainable params: 18,438,300\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 31, 300)           18438300  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                42624     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 99        \n",
      "_________________________________________________________________\n",
      "dense1 (Dense)               (None, 150)               600       \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               (None, 64)                9664      \n",
      "_________________________________________________________________\n",
      "dense3 (Dense)               (None, 7)                 455       \n",
      "=================================================================\n",
      "Total params: 18,482,079\n",
      "Trainable params: 43,779\n",
      "Non-trainable params: 18,438,300\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beuvry_j/.local/lib/python3.7/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1385 samples, validate on 245 samples\n",
      "Epoch 1/11\n",
      "1385/1385 [==============================] - 1s 556us/step - loss: 1.8274 - accuracy: 0.2939 - val_loss: 1.7912 - val_accuracy: 0.2857\n",
      "Epoch 2/11\n",
      "1385/1385 [==============================] - 0s 275us/step - loss: 1.6932 - accuracy: 0.3242 - val_loss: 1.7084 - val_accuracy: 0.2694\n",
      "Epoch 3/11\n",
      "1385/1385 [==============================] - 0s 280us/step - loss: 1.6172 - accuracy: 0.3487 - val_loss: 1.6664 - val_accuracy: 0.3184\n",
      "Epoch 4/11\n",
      "1385/1385 [==============================] - 0s 283us/step - loss: 1.5726 - accuracy: 0.3502 - val_loss: 1.6690 - val_accuracy: 0.3347\n",
      "Epoch 5/11\n",
      "1385/1385 [==============================] - 0s 286us/step - loss: 1.5450 - accuracy: 0.3545 - val_loss: 1.6587 - val_accuracy: 0.2980\n",
      "Epoch 6/11\n",
      "1385/1385 [==============================] - 0s 280us/step - loss: 1.5249 - accuracy: 0.3863 - val_loss: 1.6626 - val_accuracy: 0.3061\n",
      "Epoch 7/11\n",
      "1385/1385 [==============================] - 0s 280us/step - loss: 1.4835 - accuracy: 0.3942 - val_loss: 1.6478 - val_accuracy: 0.3633\n",
      "Epoch 8/11\n",
      "1385/1385 [==============================] - 0s 283us/step - loss: 1.4647 - accuracy: 0.4065 - val_loss: 1.6636 - val_accuracy: 0.3878\n",
      "Epoch 9/11\n",
      "1385/1385 [==============================] - 0s 283us/step - loss: 1.4449 - accuracy: 0.4058 - val_loss: 1.6753 - val_accuracy: 0.3714\n",
      "Epoch 10/11\n",
      "1385/1385 [==============================] - 0s 280us/step - loss: 1.4196 - accuracy: 0.4267 - val_loss: 1.6936 - val_accuracy: 0.3510\n",
      "Epoch 11/11\n",
      "1385/1385 [==============================] - 0s 286us/step - loss: 1.3861 - accuracy: 0.4339 - val_loss: 1.7154 - val_accuracy: 0.3592\n"
     ]
    }
   ],
   "source": [
    "model=keras.models.load_model(\"./model1.h5\")\n",
    "model.summary()\n",
    "model.layers.pop()\n",
    "model.layers.pop()\n",
    "model.add(Dense(150,activation='relu',name='dense1'))\n",
    "model.add(Dense(64,activation='relu',name='dense2'))\n",
    "model.add(Dense(7,activation='softmax',name='dense3'))\n",
    "model.summary()\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "history = model.fit(x_train_7, y_train_7,   validation_data=(x_val_7,y_val_7), epochs=11, batch_size=50)\n",
    "model.save(\"./model3.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e02yEAVrNvKm"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "corpora_test_7=\"./train_3_3.csv\"\n",
    "corpora_test_7= pd.read_csv(corpora_test_7,sep='\\t',names=['id','sentiment','data'])\n",
    "\n",
    "tweets_test_7,sentiments_test_7,corpora_test_processed_7 = data_preprocessing(corpora_test_7.copy())\n",
    "sequences_test_7 = tokenizer.texts_to_sequences(tweets_test_7)\n",
    "pad_sequence_test_7 = pad_sequences(sequences_test_7, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0, -3, -3, ..., -3,  2,  2]),\n",
       " array(['0: neutral or mixed emotional state can be inferred',\n",
       "        '-3: very negative emotional state can be inferred',\n",
       "        '-3: very negative emotional state can be inferred', ...,\n",
       "        '-3: very negative emotional state can be inferred',\n",
       "        '2: moderately positive emotional state can be inferred',\n",
       "        '2: moderately positive emotional state can be inferred'],\n",
       "       dtype='<U55'))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict the classes\n",
    "y_test_predict_7 = model.predict_classes(pad_sequence_test_7) - 3 # To replace between -3 and 3\n",
    "y_test_predict_7_classes = np.vectorize(lambda index: \"{}: \".format(index) + {\n",
    "    -3:\"very negative\",\n",
    "    -2:\"moderately negative\",\n",
    "    -1:\"slightly negative\",\n",
    "    0:\"neutral or mixed\",\n",
    "    1:\"slightly positive\",\n",
    "    2:\"moderately positive\",\n",
    "    3:\"very positive\" }[index] + \" emotional state can be inferred\")(y_test_predict_7)\n",
    "\n",
    "#cohen_kappa_score(y_test_predict_7, sentiments_test_7.values)\n",
    "y_test_predict_7, y_test_predict_7_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Affect Dimension</th>\n",
       "      <th>Intensity Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@liamch88 yeah! :) playing well</td>\n",
       "      <td>valence</td>\n",
       "      <td>0: neutral or mixed emotional state can be inf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>At least I don't have a guy trying to discoura...</td>\n",
       "      <td>valence</td>\n",
       "      <td>-3: very negative emotional state can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>UPLIFT: If you're still discouraged it means y...</td>\n",
       "      <td>valence</td>\n",
       "      <td>-3: very negative emotional state can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>...at your age, the heyday in the blood is tam...</td>\n",
       "      <td>valence</td>\n",
       "      <td>-3: very negative emotional state can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>i was so embarrassed when she saw us i was lik...</td>\n",
       "      <td>valence</td>\n",
       "      <td>2: moderately positive emotional state can be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1625</th>\n",
       "      <td>1625</td>\n",
       "      <td>Idk why but when I help someone,  a smile come...</td>\n",
       "      <td>valence</td>\n",
       "      <td>-3: very negative emotional state can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1626</th>\n",
       "      <td>1626</td>\n",
       "      <td>I think 'Sleep' is my favorite from How did we...</td>\n",
       "      <td>valence</td>\n",
       "      <td>-3: very negative emotional state can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1627</th>\n",
       "      <td>1627</td>\n",
       "      <td>What does Amelia want?! Sarah was v grateful  ...</td>\n",
       "      <td>valence</td>\n",
       "      <td>-3: very negative emotional state can be inferred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1628</th>\n",
       "      <td>1628</td>\n",
       "      <td>Itâ€™s lack of #faith that makes #people #afraid...</td>\n",
       "      <td>valence</td>\n",
       "      <td>2: moderately positive emotional state can be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1629</th>\n",
       "      <td>1629</td>\n",
       "      <td>James Clapper 'scary and disturbing'.  #25thAm...</td>\n",
       "      <td>valence</td>\n",
       "      <td>2: moderately positive emotional state can be ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1630 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID                                              Tweet  \\\n",
       "0        0                   @liamch88 yeah! :) playing well    \n",
       "1        1  At least I don't have a guy trying to discoura...   \n",
       "2        2  UPLIFT: If you're still discouraged it means y...   \n",
       "3        3  ...at your age, the heyday in the blood is tam...   \n",
       "4        4  i was so embarrassed when she saw us i was lik...   \n",
       "...    ...                                                ...   \n",
       "1625  1625  Idk why but when I help someone,  a smile come...   \n",
       "1626  1626  I think 'Sleep' is my favorite from How did we...   \n",
       "1627  1627  What does Amelia want?! Sarah was v grateful  ...   \n",
       "1628  1628  Itâ€™s lack of #faith that makes #people #afraid...   \n",
       "1629  1629  James Clapper 'scary and disturbing'.  #25thAm...   \n",
       "\n",
       "     Affect Dimension                                    Intensity Class  \n",
       "0             valence  0: neutral or mixed emotional state can be inf...  \n",
       "1             valence  -3: very negative emotional state can be inferred  \n",
       "2             valence  -3: very negative emotional state can be inferred  \n",
       "3             valence  -3: very negative emotional state can be inferred  \n",
       "4             valence  2: moderately positive emotional state can be ...  \n",
       "...               ...                                                ...  \n",
       "1625          valence  -3: very negative emotional state can be inferred  \n",
       "1626          valence  -3: very negative emotional state can be inferred  \n",
       "1627          valence  -3: very negative emotional state can be inferred  \n",
       "1628          valence  2: moderately positive emotional state can be ...  \n",
       "1629          valence  2: moderately positive emotional state can be ...  \n",
       "\n",
       "[1630 rows x 4 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output to file\n",
    "\n",
    "output_df = corpora_test_7.drop(columns=[\"sentiment\"]).rename(columns = { \"id\": \"ID\", \"data\": \"Tweet\" })\n",
    "output_df[\"Affect Dimension\"] = \"valence\"\n",
    "output_df[\"Intensity Class\"] = y_test_predict_7_classes\n",
    "\n",
    "output_df.to_csv(\"result_TP1.csv\", sep=\"\\t\", index=False)\n",
    "\n",
    "output_df"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
