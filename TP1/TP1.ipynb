{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2270,
     "status": "ok",
     "timestamp": 1573606004254,
     "user": {
      "displayName": "Benoit Frédéric-Moreau",
      "photoUrl": "",
      "userId": "09939474294080590706"
     },
     "user_tz": -60
    },
    "id": "XfkBZ2f9J_20",
    "outputId": "287a12b2-840a-4b29-c37f-b48b495e2c4e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/beuvry_j/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "#from google.colab import drive\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import LSTM, Dropout, Dense, Bidirectional,  Flatten, Input, GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "from unidecode import unidecode\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords=stopwords.words('english')\n",
    "\n",
    "T = TweetTokenizer()\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "#!wget -P /root/input/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
    "EMBEDDING_FILE = '../GoogleNews-vectors-negative300.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tpo2LX6hJ_vg"
   },
   "outputs": [],
   "source": [
    "def value_sentiment(x):\n",
    "  if x == \"neutral\":\n",
    "    return 0\n",
    "  if x == \"positive\":\n",
    "    return 1\n",
    "  if x == \"negative\":\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m6d-_t2NKvQp"
   },
   "outputs": [],
   "source": [
    "def onehot (tweets,length):\n",
    "  onh = []\n",
    "  for tweet in tweets:\n",
    "      onh_seq = np.zeros(length) \n",
    "      for i in tweet:\n",
    "        onh_seq[i -1] = 1\n",
    "      onh.append(onh_seq)\n",
    "  return onh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TlSZY2t6J_gj"
   },
   "outputs": [],
   "source": [
    "#replace emojy\n",
    "emojis = {\n",
    "    u\":‑\\)\":\"☺️\",\n",
    "    u\":\\)\":\"☺️\",\n",
    "    u\":-\\]\":\"☺️\",\n",
    "    u\":\\]\":\"☺️\",\n",
    "    u\":-3\":\"☺️\",\n",
    "    u\":3\":\"☺️\",\n",
    "    u\":->\":\"☺️\",\n",
    "    u\":>\":\"☺️\",\n",
    "    u\"8-\\)\":\"☺️\",\n",
    "    u\":o\\)\":\"☺️\",\n",
    "    u\":-\\}\":\"☺️\",\n",
    "    u\":\\}\":\"☺️\",\n",
    "    u\":-\\)\":\"☺️\",\n",
    "    u\":c\\)\":\"☺️\",\n",
    "    u\":\\^\\)\":\"☺️\",\n",
    "    u\"=\\]\":\"☺️\",\n",
    "    u\"=\\)\":\"☺️\",\n",
    "    u\":‑D\":\"😃\",\n",
    "    u\":D\":\"😃\",\n",
    "    u\"8‑D\":\"😃\",\n",
    "    u\"8D\":\"😃\",\n",
    "    u\"X‑D\":\"😃\",\n",
    "    u\"XD\":\"😃\",\n",
    "    u\"=D\":\"😃\",\n",
    "    u\"=3\":\"😃\",\n",
    "    u\"B\\^D\":\"😃\",\n",
    "    u\":-\\)\\)\":\"😃\",\n",
    "    u\":‑\\(\":\"☹️\",\n",
    "    u\":-\\(\":\"☹️\",\n",
    "    u\":\\(\":\"☹️\",\n",
    "    u\":‑c\":\"☹️\",\n",
    "    u\":c\":\"☹️\",\n",
    "    u\":‑<\":\"☹️\",\n",
    "    u\":<\":\"☹️\",\n",
    "    u\":‑\\[\":\"☹️\",\n",
    "    u\":\\[\":\"☹️\",\n",
    "    u\":-\\|\\|\":\"☹️\",\n",
    "    u\">:\\[\":\"☹️\",\n",
    "    u\":\\{\":\"☹️\",\n",
    "    u\":@\":\"☹️\",\n",
    "    u\">:\\(\":\"☹️\",\n",
    "    u\":'‑\\(\":\"😭\",\n",
    "    u\":'\\(\":\"😭\",\n",
    "    u\":'‑\\)\":\"😃\",\n",
    "    u\":'\\)\":\"😃\",\n",
    "    u\"D‑':\":\"😨\",\n",
    "    u\"D:<\":\"😨\",\n",
    "    u\"D:\":\"😧\",\n",
    "    u\"D8\":\"😧\",\n",
    "    u\"D;\":\"😧\",\n",
    "    u\"D=\":\"😧\",\n",
    "    u\"DX\":\"😧\",\n",
    "    u\":‑O\":\"😮\",\n",
    "    u\":O\":\"😮\",\n",
    "    u\":‑o\":\"😮\",\n",
    "    u\":o\":\"😮\",\n",
    "    u\":-0\":\"😮\",\n",
    "    u\"8‑0\":\"😮\",\n",
    "    u\">:O\":\"😮\",\n",
    "    u\":-\\*\":\"😗\",\n",
    "    u\":\\*\":\"😗\",\n",
    "    u\":X\":\"😗\",\n",
    "    u\";‑\\)\":\"😉\",\n",
    "    u\";\\)\":\"😉\",\n",
    "    u\"\\*-\\)\":\"😉\",\n",
    "    u\"\\*\\)\":\"😉\",\n",
    "    u\";‑\\]\":\"😉\",\n",
    "    u\";\\]\":\"😉\",\n",
    "    u\";\\^\\)\":\"😉\",\n",
    "    u\":‑,\":\"😉\",\n",
    "    u\";D\":\"😉\",\n",
    "    u\":‑P\":\"😛\",\n",
    "    u\":P\":\"😛\",\n",
    "    u\"X‑P\":\"😛\",\n",
    "    u\"XP\":\"😛\",\n",
    "    u\":‑Þ\":\"😛\",\n",
    "    u\":Þ\":\"😛\",\n",
    "    u\":b\":\"😛\",\n",
    "    u\"d:\":\"😛\",\n",
    "    u\"=p\":\"😛\",\n",
    "    u\">:P\":\"😛\",\n",
    "    u\":‑/\":\"😕\",\n",
    "    u\":/\":\"😕\",\n",
    "    u\":-[.]\":\"😕\",\n",
    "    u\">:[(\\\\\\)]\":\"😕\",\n",
    "    u\">:/\":\"😕\",\n",
    "    u\":[(\\\\\\)]\":\"😕\",\n",
    "    u\"=/\":\"😕\",\n",
    "    u\"=[(\\\\\\)]\":\"😕\",\n",
    "    u\":L\":\"😕\",\n",
    "    u\"=L\":\"😕\",\n",
    "    u\":S\":\"😕\",\n",
    "    u\":‑\\|\":\"😐\",\n",
    "    u\":\\|\":\"😐\",\n",
    "    u\":$\":\"😳\",\n",
    "    u\":‑x\":\"🤐\",\n",
    "    u\":x\":\"🤐\",\n",
    "    u\":‑#\":\"🤐\",\n",
    "    u\":#\":\"🤐\",\n",
    "    u\":‑&\":\"🤐\",\n",
    "    u\":&\":\"🤐\",\n",
    "    u\"O:‑\\)\":\"😇\",\n",
    "    u\"O:\\)\":\"😇\",\n",
    "    u\"0:‑3\":\"😇\",\n",
    "    u\"0:3\":\"😇\",\n",
    "    u\"0:‑\\)\":\"😇\",\n",
    "    u\"0:\\)\":\"😇\",\n",
    "    u\":‑b\":\"😛\",\n",
    "    u\"0;\\^\\)\":\"😇\",\n",
    "    u\">:‑\\)\":\"😈\",\n",
    "    u\">:\\)\":\"😈\",\n",
    "    u\"\\}:‑\\)\":\"😈\",\n",
    "    u\"\\}:\\)\":\"😈\",\n",
    "    u\"3:‑\\)\":\"😈\",\n",
    "    u\"3:\\)\":\"😈\",\n",
    "    u\">;\\)\":\"😈\",\n",
    "    u\"\\|;‑\\)\":\"😎\",\n",
    "    u\"\\|‑O\":\"😏\",\n",
    "    u\":‑J\":\"😏\",\n",
    "    u\"%‑\\)\":\"😵\",\n",
    "    u\"%\\)\":\"😵\",\n",
    "    u\":-###..\":\"🤒\",\n",
    "    u\":###..\":\"🤒\",\n",
    "    u\"\\(>_<\\)\":\"😣\",\n",
    "    u\"\\(>_<\\)>\":\"😣\",\n",
    "    u\"\\(';'\\)\":\"👶\",\n",
    "    u\"\\(\\^\\^>``\":\"😓\",\n",
    "    u\"\\(\\^_\\^;\\)\":\"😓\",\n",
    "    u\"\\(-_-;\\)\":\"😓\",\n",
    "    u\"\\(~_~;\\) \\(・\\.・;\\)\":\"😓\",\n",
    "    u\"\\(-_-\\)zzz\":\"😴\",\n",
    "    u\"\\(\\^_-\\)\":\"😉\",\n",
    "    u\"\\(\\(\\+_\\+\\)\\)\":\"😕\",\n",
    "    u\"\\(\\+o\\+\\)\":\"😕\",\n",
    "    u\"\\^_\\^\":\"😃\",\n",
    "    u\"\\(\\^_\\^\\)/\":\"😃\",\n",
    "    u\"\\(\\^O\\^\\)／\":\"😃\",\n",
    "    u\"\\(\\^o\\^\\)／\":\"😃\",\n",
    "    u\"\\(__\\)\":\"🙇\",\n",
    "    u\"_\\(\\._\\.\\)_\":\"🙇\",\n",
    "    u\"<\\(_ _\\)>\":\"🙇\",\n",
    "    u\"<m\\(__\\)m>\":\"🙇\",\n",
    "    u\"m\\(__\\)m\":\"🙇\",\n",
    "    u\"m\\(_ _\\)m\":\"🙇\",\n",
    "    u\"\\('_'\\)\":\"😭\",\n",
    "    u\"\\(/_;\\)\":\"😭\",\n",
    "    u\"\\(T_T\\) \\(;_;\\)\":\"😭\",\n",
    "    u\"\\(;_;\":\"😭\",\n",
    "    u\"\\(;_:\\)\":\"😭\",\n",
    "    u\"\\(;O;\\)\":\"😭\",\n",
    "    u\"\\(:_;\\)\":\"😭\",\n",
    "    u\"\\(ToT\\)\":\"😭\",\n",
    "    u\";_;\":\"😭\",\n",
    "    u\";-;\":\"😭\",\n",
    "    u\";n;\":\"😭\",\n",
    "    u\";;\":\"😭\",\n",
    "    u\"Q\\.Q\":\"😭\",\n",
    "    u\"T\\.T\":\"😭\",\n",
    "    u\"QQ\":\"😭\",\n",
    "    u\"Q_Q\":\"😭\",\n",
    "    u\"\\(-\\.-\\)\":\"😞\",\n",
    "    u\"\\(-_-\\)\":\"😞\",\n",
    "    u\"\\(一一\\)\":\"😞\",\n",
    "    u\"\\(；一_一\\)\":\"😞\",\n",
    "    u\"\\(=_=\\)\":\"😩\",\n",
    "    u\"\\(=\\^\\·\\^=\\)\":\"😺\",\n",
    "    u\"\\(=\\^\\·\\·\\^=\\)\":\"😺\",\n",
    "    u\"=_\\^=\t\":\"😺\",\n",
    "    u\"\\(\\.\\.\\)\":\"😔\",\n",
    "    u\"\\(\\._\\.\\)\":\"😔\",\n",
    "    u\"\\(\\・\\・?\":\"😕\",\n",
    "    u\"\\(?_?\\)\":\"😕\",\n",
    "    u\">\\^_\\^<\":\"😃\",\n",
    "    u\"<\\^!\\^>\":\"😃\",\n",
    "    u\"\\^/\\^\":\"😃\",\n",
    "    u\"\\（\\*\\^_\\^\\*）\" :\"😃\",\n",
    "    u\"\\(\\^<\\^\\) \\(\\^\\.\\^\\)\":\"😃\",\n",
    "    u\"\\(^\\^\\)\":\"😃\",\n",
    "    u\"\\(\\^\\.\\^\\)\":\"😃\",\n",
    "    u\"\\(\\^_\\^\\.\\)\":\"😃\",\n",
    "    u\"\\(\\^_\\^\\)\":\"😃\",\n",
    "    u\"\\(\\^\\^\\)\":\"😃\",\n",
    "    u\"\\(\\^J\\^\\)\":\"😃\",\n",
    "    u\"\\(\\*\\^\\.\\^\\*\\)\":\"😃\",\n",
    "    u\"\\(\\^—\\^\\）\":\"😃\",\n",
    "    u\"\\(#\\^\\.\\^#\\)\":\"😃\",\n",
    "    u\"\\（\\^—\\^\\）\":\"👋\",\n",
    "    u\"\\(;_;\\)/~~~\":\"👋\",\n",
    "    u\"\\(\\^\\.\\^\\)/~~~\":\"👋\",\n",
    "    u\"\\(T_T\\)/~~~\":\"👋\",\n",
    "    u\"\\(ToT\\)/~~~\":\"👋\",\n",
    "    u\"\\(\\*\\^0\\^\\*\\)\":\"😍\",\n",
    "    u\"\\(\\*_\\*\\)\":\"😍\",\n",
    "    u\"\\(\\*_\\*;\":\"😍\",\n",
    "    u\"\\(\\+_\\+\\) \\(@_@\\)\":\"😍\",\n",
    "    u\"\\(\\*\\^\\^\\)v\":\"😂\",\n",
    "    u\"\\(\\^_\\^\\)v\":\"😂\",\n",
    "    u'\\(-\"-\\)':\"😓\",\n",
    "    u\"\\(ーー;\\)\":\"😓\",\n",
    "    u\"\\(\\^0_0\\^\\)\":\"😎\",\n",
    "    u\"\\(\\＾ｖ\\＾\\)\":\"😀\",\n",
    "    u\"\\(\\＾ｕ\\＾\\)\":\"😀\",\n",
    "    u\"\\(\\^\\)o\\(\\^\\)\":\"😀\",\n",
    "    u\"\\(\\^O\\^\\)\":\"😀\",\n",
    "    u\"\\(\\^o\\^\\)\":\"😀\",\n",
    "    u\"\\)\\^o\\^\\(\":\"😀\",\n",
    "    u\":O o_O\":\"😮\",\n",
    "    u\"o_0\":\"😮\",\n",
    "    u\"o\\.O\":\"😮\",\n",
    "    u\"\\(o\\.o\\)\":\"😮\",\n",
    "    u\"oO\":\"😮\",\n",
    "    u\"\\(\\*￣m￣\\)\":\"😠\",\n",
    "    u\":‑)\":\"☺️\",\n",
    "    u\":)\":\"☺️\",\n",
    "    u\":-]\":\"☺️\",\n",
    "    u\":]\":\"☺️\",\n",
    "    u\":-3\":\"☺️\",\n",
    "    u\":3\":\"☺️\",\n",
    "    u\":->\":\"☺️\",\n",
    "    u\":>\":\"☺️\",\n",
    "    u\"8-)\":\"☺️\",\n",
    "    u\":o)\":\"☺️\",\n",
    "    u\":-}\":\"☺️\",\n",
    "    u\":}\":\"☺️\",\n",
    "    u\":-)\":\"☺️\",\n",
    "    u\":c)\":\"☺️\",\n",
    "    u\":^)\":\"☺️\",\n",
    "    u\"=]\":\"☺️\",\n",
    "    u\"=)\":\"☺️\",\n",
    "    u\":‑D\":\"😃\",\n",
    "    u\":D\":\"😃\",\n",
    "    u\"8‑D\":\"😃\",\n",
    "    u\"8D\":\"😃\",\n",
    "    u\"X‑D\":\"😃\",\n",
    "    u\"XD\":\"😃\",\n",
    "    u\"=D\":\"😃\",\n",
    "    u\"=3\":\"😃\",\n",
    "    u\"B^D\":\"😃\",\n",
    "    u\":-))\":\"😃\",\n",
    "    u\":-(\":\"☹️\",\n",
    "    u\":‑(\":\"☹️\",\n",
    "    u\":(\":\"☹️\",\n",
    "    u\":‑c\":\"☹️\",\n",
    "    u\":c\":\"☹️\",\n",
    "    u\":‑<\":\"☹️\",\n",
    "    u\":<\":\"☹️\",\n",
    "    u\":‑[\":\"☹️\",\n",
    "    u\":[\":\"☹️\",\n",
    "    u\":-||\":\"☹️\",\n",
    "    u\">:[\":\"☹️\",\n",
    "    u\":{\":\"☹️\",\n",
    "    u\":@\":\"☹️\",\n",
    "    u\">:(\":\"☹️\",\n",
    "    u\":'‑(\":\"😭\",\n",
    "    u\":'(\":\"😭\",\n",
    "    u\":'‑)\":\"😃\",\n",
    "    u\":')\":\"😃\",\n",
    "    u\"D‑':\":\"😧\",\n",
    "    u\"D:<\":\"😨\",\n",
    "    u\"D:\":\"😧\",\n",
    "    u\"D8\":\"😧\",\n",
    "    u\"D;\":\"😧\",\n",
    "    u\"D=\":\"😧\",\n",
    "    u\"DX\":\"😧\",\n",
    "    u\":‑O\":\"😮\",\n",
    "    u\":O\":\"😮\",\n",
    "    u\":‑o\":\"😮\",\n",
    "    u\":o\":\"😮\",\n",
    "    u\":-0\":\"😮\",\n",
    "    u\"8‑0\":\"😮\",\n",
    "    u\">:O\":\"😮\",\n",
    "    u\":-*\":\"😗\",\n",
    "    u\":*\":\"😗\",\n",
    "    u\":X\":\"😗\",\n",
    "    u\";‑)\":\"😉\",\n",
    "    u\";)\":\"😉\",\n",
    "    u\"*-)\":\"😉\",\n",
    "    u\"*)\":\"😉\",\n",
    "    u\";‑]\":\"😉\",\n",
    "    u\";]\":\"😉\",\n",
    "    u\";^)\":\"😉\",\n",
    "    u\":‑,\":\"😉\",\n",
    "    u\";D\":\"😉\",\n",
    "    u\":‑P\":\"😛\",\n",
    "    u\":P\":\"😛\",\n",
    "    u\"X‑P\":\"😛\",\n",
    "    u\"XP\":\"😛\",\n",
    "    u\":‑Þ\":\"😛\",\n",
    "    u\":Þ\":\"😛\",\n",
    "    u\":b\":\"😛\",\n",
    "    u\"d:\":\"😛\",\n",
    "    u\"=p\":\"😛\",\n",
    "    u\">:P\":\"😛\",\n",
    "    u\":‑/\":\"😕\",\n",
    "    u\":/\":\"😕\",\n",
    "    u\":-[.]\":\"😕\",\n",
    "    u\">:[(\\)]\":\"😕\",\n",
    "    u\">:/\":\"😕\",\n",
    "    u\":[(\\)]\":\"😕\",\n",
    "    u\"=/\":\"😕\",\n",
    "    u\"=[(\\)]\":\"😕\",\n",
    "    u\":L\":\"😕\",\n",
    "    u\"=L\":\"😕\",\n",
    "    u\":S\":\"😕\",\n",
    "    u\":‑|\":\"😐\",\n",
    "    u\":|\":\"😐\",\n",
    "    u\":$\":\"😳\",\n",
    "    u\":‑x\":\"🤐\",\n",
    "    u\":x\":\"🤐\",\n",
    "    u\":‑#\":\"🤐\",\n",
    "    u\":#\":\"🤐\",\n",
    "    u\":‑&\":\"🤐\",\n",
    "    u\":&\":\"🤐\",\n",
    "    u\"O:‑)\":\"😇\",\n",
    "    u\"O:)\":\"😇\",\n",
    "    u\"0:‑3\":\"😇\",\n",
    "    u\"0:3\":\"😇\",\n",
    "    u\"0:‑)\":\"😇\",\n",
    "    u\"0:)\":\"😇\",\n",
    "    u\":‑b\":\"😛\",\n",
    "    u\"0;^)\":\"😇\",\n",
    "    u\">:‑)\":\"😈\",\n",
    "    u\">:)\":\"😈\",\n",
    "    u\"}:‑)\":\"😈\",\n",
    "    u\"}:)\":\"😈\",\n",
    "    u\"3:‑)\":\"😈\",\n",
    "    u\"3:)\":\"😈\",\n",
    "    u\">;)\":\"😈\",\n",
    "    u\"|;‑)\":\"😎\",\n",
    "    u\"|‑O\":\"😏\",\n",
    "    u\":‑J\":\"😏\",\n",
    "    u\"%‑)\":\"😵\",\n",
    "    u\"%)\":\"😵\",\n",
    "    u\":-###..\":\"🤒\",\n",
    "    u\":###..\":\"🤒\",\n",
    "    u\"(>_<)\":\"😣\",\n",
    "    u\"(>_<)>\":\"😣\",\n",
    "    u\"(';')\":\"Baby\",\n",
    "    u\"(^^>``\":\"😓\",\n",
    "    u\"(^_^;)\":\"😓\",\n",
    "    u\"(-_-;)\":\"😓\",\n",
    "    u\"(~_~;) (・.・;)\":\"😓\",\n",
    "    u\"(-_-)zzz\":\"😴\",\n",
    "    u\"(^_-)\":\"😉\",\n",
    "    u\"((+_+))\":\"😕\",\n",
    "    u\"(+o+)\":\"😕\",\n",
    "    u\"^_^\":\"😃\",\n",
    "    u\"(^_^)/\":\"😃\",\n",
    "    u\"(^O^)／\":\"😃\",\n",
    "    u\"(^o^)／\":\"😃\",\n",
    "    u\"(__)\":\"🙇\",\n",
    "    u\"_(._.)_\":\"🙇\",\n",
    "    u\"<(_ _)>\":\"🙇\",\n",
    "    u\"<m(__)m>\":\"🙇\",\n",
    "    u\"m(__)m\":\"🙇\",\n",
    "    u\"m(_ _)m\":\"🙇\",\n",
    "    u\"('_')\":\"😭\",\n",
    "    u\"(/_;)\":\"😭\",\n",
    "    u\"(T_T) (;_;)\":\"😭\",\n",
    "    u\"(;_;\":\"😭\",\n",
    "    u\"(;_:)\":\"😭\",\n",
    "    u\"(;O;)\":\"😭\",\n",
    "    u\"(:_;)\":\"😭\",\n",
    "    u\"(ToT)\":\"😭\",\n",
    "    u\";_;\":\"😭\",\n",
    "    u\";-;\":\"😭\",\n",
    "    u\";n;\":\"😭\",\n",
    "    u\";;\":\"😭\",\n",
    "    u\"Q.Q\":\"😭\",\n",
    "    u\"T.T\":\"😭\",\n",
    "    u\"QQ\":\"😭\",\n",
    "    u\"Q_Q\":\"😭\",\n",
    "    u\"(-.-)\":\"😞\",\n",
    "    u\"(-_-)\":\"😞\",\n",
    "    u\"(一一)\":\"😞\",\n",
    "    u\"(；一_一)\":\"😞\",\n",
    "    u\"(=_=)\":\"😩\",\n",
    "    u\"(=^·^=)\":\"😺\",\n",
    "    u\"(=^··^=)\":\"😺\",\n",
    "    u\"=_^= \":\"😺\",\n",
    "    u\"(..)\":\"😔\",\n",
    "    u\"(._.)\":\"😔\",\n",
    "    u\"(・・?\":\"😕\",\n",
    "    u\"(?_?)\":\"😕\",\n",
    "    u\">^_^<\":\"😃\",\n",
    "    u\"<^!^>\":\"😃\",\n",
    "    u\"^/^\":\"😃\",\n",
    "    u\"（*^_^*）\" :\"😃\",\n",
    "    u\"(^<^) (^.^)\":\"😃\",\n",
    "    u\"(^^)\":\"😃\",\n",
    "    u\"(^.^)\":\"😃\",\n",
    "    u\"(^_^.)\":\"😃\",\n",
    "    u\"(^_^)\":\"😃\",\n",
    "    u\"(^^)\":\"😃\",\n",
    "    u\"(^J^)\":\"😃\",\n",
    "    u\"(*^.^*)\":\"😃\",\n",
    "    u\"(^—^）\":\"😃\",\n",
    "    u\"(#^.^#)\":\"😃\",\n",
    "    u\"（^—^）\":\"👋\",\n",
    "    u\"(;_;)/~~~\":\"👋\",\n",
    "    u\"(^.^)/~~~\":\"👋\",\n",
    "    u\"(-_-)/~~~ ($··)/~~~\":\"👋\",\n",
    "    u\"(T_T)/~~~\":\"👋\",\n",
    "    u\"(ToT)/~~~\":\"👋\",\n",
    "    u\"(*^0^*)\":\"😍\",\n",
    "    u\"(*_*)\":\"😍\",\n",
    "    u\"(*_*;\":\"😍\",\n",
    "    u\"(+_+) (@_@)\":\"😍\",\n",
    "    u\"(*^^)v\":\"😂\",\n",
    "    u\"(^_^)v\":\"😂\",\n",
    "    u'(-\"-)':\"😓\",\n",
    "    u\"(ーー;)\":\"😓\",\n",
    "    u\"(^0_0^)\":\"😎\",\n",
    "    u\"(＾ｖ＾)\":\"😀\",\n",
    "    u\"(＾ｕ＾)\":\"😀\",\n",
    "    u\"(^)o(^)\":\"😀\",\n",
    "    u\"(^O^)\":\"😀\",\n",
    "    u\"(^o^)\":\"😀\",\n",
    "    u\")^o^(\":\"😀\",\n",
    "    u\":O o_O\":\"😮\",\n",
    "    u\"o_0\":\"😮\",\n",
    "    u\"o.O\":\"😮\",\n",
    "    u\"(o.o)\":\"😮\",\n",
    "    u\"oO\":\"😮\",\n",
    "}\n",
    "\n",
    "\n",
    "def str2emoji(tweet):\n",
    "\t\n",
    "\tfor pos,ej in enumerate(tweet):\n",
    "\t\tif ej in emojis:\n",
    "\t\t\ttweet[pos]=emojis[ej]\n",
    "\treturn tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UjpS6y1RKeeZ"
   },
   "outputs": [],
   "source": [
    "def norm_tweet(tweet):\n",
    "  tweet = re.sub(r\"\\\\u2019\", \"'\", tweet)\n",
    "  tweet = re.sub(r\"\\\\u002c\", \",\", tweet)\n",
    "  tweet=' '.join(str2emoji(unidecode(tweet).lower().split()))\n",
    "  tweet = re.sub(r\"\\'ve\", \" have\", tweet)\n",
    "  tweet = re.sub(r\" can\\'t\", \" cannot\", tweet)\n",
    "  tweet = re.sub(r\"n\\'t\", \" not\", tweet)\n",
    "  tweet = re.sub(r\"\\'re\", \" are\", tweet)\n",
    "  tweet = re.sub(r\"\\'d\", \" would\", tweet)\n",
    "  tweet = re.sub(r\"\\'ll\", \" will\", tweet)\n",
    "  tweet = re.sub(r\"\\'s\", \"\", tweet)\n",
    "  tweet = re.sub(r\"\\'n\", \"\", tweet)\n",
    "  tweet = re.sub(r\"\\'m\", \" am\", tweet)\n",
    "  tweet = re.sub(r\"@\\w+\", r' ',tweet)\n",
    "  tweet = re.sub(r\"#\\w+\", r' ',tweet)\n",
    "  tweet = re.sub(r\"[.]+\",\" \",tweet)\n",
    "  #T = tokenizer.TweetTokenizer()\n",
    "  tweet = T.tokenize(tweet)\n",
    "  #tweet = [lemmatizer.lemmatize(i,j[0].lower()) if j[0].lower() in ['a','n','v']  else lemmatizer.lemmatize(i) for i,j in pos_tag(tknzr.tokenize(tweet))]\n",
    "  tweet = [ i for i in tweet if (i not in stopwords) and (i not in string.punctuation ) ]\n",
    "  #tweet = ' '.join(tweet)\n",
    "  return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Rv6ih5wKoHV"
   },
   "outputs": [],
   "source": [
    "def data_preprocessing(data):\n",
    "  data['data'] = data['data'].apply(norm_tweet)\n",
    "  if data['sentiment'].dtypes not in [np.int64]:\n",
    "    data['sentiment']=data['sentiment'].apply(value_sentiment)\n",
    "  return data['data'],data['sentiment'],data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import scipy.stats\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score, jaccard_similarity_score\n",
    "\n",
    "def cohen_kappa_score(y1, y2, labels=None, weights=None):\n",
    "    \"\"\"Cohen's kappa: a statistic that measures inter-annotator agreement.\n",
    "\n",
    "    This function computes Cohen's kappa [1]_, a score that expresses the level\n",
    "    of agreement between two annotators on a classification problem. It is\n",
    "    defined as\n",
    "\n",
    "    .. math::\n",
    "        \\kappa = (p_o - p_e) / (1 - p_e)\n",
    "\n",
    "    where :math:`p_o` is the empirical probability of agreement on the label\n",
    "    assigned to any sample (the observed agreement ratio), and :math:`p_e` is\n",
    "    the expected agreement when both annotators assign labels randomly.\n",
    "    :math:`p_e` is estimated using a per-annotator empirical prior over the\n",
    "    class labels [2]_.\n",
    "\n",
    "    Read more in the :ref:`User Guide <cohen_kappa>`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y1 : array, shape = [n_samples]\n",
    "        Labels assigned by the first annotator.\n",
    "\n",
    "    y2 : array, shape = [n_samples]\n",
    "        Labels assigned by the second annotator. The kappa statistic is\n",
    "        symmetric, so swapping ``y1`` and ``y2`` doesn't change the value.\n",
    "\n",
    "    labels : array, shape = [n_classes], optional\n",
    "        List of labels to index the matrix. This may be used to select a\n",
    "        subset of labels. If None, all labels that appear at least once in\n",
    "        ``y1`` or ``y2`` are used.\n",
    "\n",
    "    weights : str, optional\n",
    "        List of weighting type to calculate the score. None means no weighted;\n",
    "        \"linear\" means linear weighted; \"quadratic\" means quadratic weighted.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    kappa : float\n",
    "        The kappa statistic, which is a number between -1 and 1. The maximum\n",
    "        value means complete agreement; zero or lower means chance agreement.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] J. Cohen (1960). \"A coefficient of agreement for nominal scales\".\n",
    "           Educational and Psychological Measurement 20(1):37-46.\n",
    "           doi:10.1177/001316446002000104.\n",
    "    .. [2] `R. Artstein and M. Poesio (2008). \"Inter-coder agreement for\n",
    "           computational linguistics\". Computational Linguistics 34(4):555-596.\n",
    "           <http://www.mitpressjournals.org/doi/abs/10.1162/coli.07-034-R2#.V0J1MJMrIWo>`_\n",
    "    .. [3] `Wikipedia entry for the Cohen's kappa.\n",
    "            <https://en.wikipedia.org/wiki/Cohen%27s_kappa>`_\n",
    "    \"\"\"\n",
    "    confusion = metrics.confusion_matrix(y1, y2, labels=labels)\n",
    "    n_classes = confusion.shape[0]\n",
    "    sum0 = np.sum(confusion, axis=0)\n",
    "    sum1 = np.sum(confusion, axis=1)\n",
    "    expected = np.outer(sum0, sum1)*1.0 / np.sum(sum0)\n",
    "\n",
    "    if weights is None:\n",
    "        w_mat = np.ones([n_classes, n_classes], dtype=np.int)\n",
    "        w_mat.flat[:: n_classes + 1] = 0\n",
    "    elif weights == \"linear\" or weights == \"quadratic\":\n",
    "        w_mat = np.zeros([n_classes, n_classes], dtype=np.int)\n",
    "        w_mat += np.arange(n_classes)\n",
    "        if weights == \"linear\":\n",
    "            w_mat = np.abs(w_mat - w_mat.T)\n",
    "        else:\n",
    "            w_mat = (w_mat - w_mat.T) ** 2\n",
    "    else:\n",
    "        raise ValueError(\"Unknown kappa weighting type.\")\n",
    "\n",
    "    k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n",
    "    return 1 - k\n",
    "\n",
    "\n",
    "def evaluate_ei(pred,gold):  \n",
    "    \"\"\"Calculates performance metrics for regression.\n",
    "    \n",
    "    :param pred: the file path of the predictions\n",
    "    :param gold: the filte path withe gold data\n",
    "    :return: a list with performace metrics.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    f=open(pred, \"rb\")\n",
    "    pred_lines=f.readlines()\n",
    "    f.close()\n",
    "    \n",
    "    f=open(gold, \"rb\")\n",
    "    gold_lines=f.readlines()\n",
    "    f.close()    \n",
    "\n",
    "    if(len(pred_lines)==len(gold_lines)):       \n",
    "        # align tweets ids with gold scores and predictions\n",
    "        data_dic={}\n",
    "\n",
    "\n",
    "        header=True        \n",
    "        for line in gold_lines:\n",
    "            #line=line.decode('utf-8')\n",
    "            \n",
    "            if header:\n",
    "                header=False\n",
    "                continue\n",
    "\n",
    "            parts=line.split('\\t')\n",
    "            if len(parts)==4:\n",
    "                # tweet ids containing the word mystery are discarded\n",
    "                if(not 'mystery' in parts[0]):\n",
    "                    data_dic[parts[0]]=[float(line.split('\\t')[3])]\n",
    "            else:\n",
    "                sys.exit('Format problem in '+os.path.basename(gold)+'. Please report this problem to the task organizers.')\n",
    "\n",
    "        header=True        \n",
    "        for line in pred_lines:\n",
    "            \n",
    "            if header:\n",
    "                header=False\n",
    "                continue\n",
    "\n",
    "            parts=line.split('\\t')      \n",
    "            if len(parts)==4:\n",
    "                # tweet ids containing the word mystery are discarded\n",
    "                if(not 'mystery' in parts[0]):\n",
    "                    if parts[0] in data_dic:\n",
    "                        try:\n",
    "                            data_dic[parts[0]].append(float(line.split('\\t')[3]))\n",
    "                        except ValueError:\n",
    "                            # Invalid predictions are replaced by a default value\n",
    "                            data_dic[parts[0]].append(0.5)\n",
    "                    else:\n",
    "                        sys.exit('Invalid tweet id ('+parts[0]+') in '+os.path.basename(pred)+'.')\n",
    "            else:\n",
    "                sys.exit('Format problem in '+os.path.basename(pred)+'.') \n",
    "            \n",
    "        \n",
    "        # lists storing gold and prediction scores\n",
    "        gold_scores=[]  \n",
    "        pred_scores=[]\n",
    "         \n",
    "        \n",
    "        # lists storing gold and prediction scores where gold score >= 0.5\n",
    "        gold_scores_range_05_1=[]\n",
    "        pred_scores_range_05_1=[]\n",
    "            \n",
    "        for id in data_dic:\n",
    "            if(len(data_dic[id])==2):\n",
    "                gold_scores.append(data_dic[id][0])\n",
    "                pred_scores.append(data_dic[id][1])\n",
    "                \n",
    "                if(data_dic[id][0]>=0.5):\n",
    "                    gold_scores_range_05_1.append(data_dic[id][0])\n",
    "                    pred_scores_range_05_1.append(data_dic[id][1])\n",
    "                \n",
    "            else:\n",
    "                sys.exit('Repeated id ('+id+') in '+os.path.basename(pred)+' .')\n",
    "\n",
    "                \n",
    "                    \n",
    "      \n",
    "        # return zero correlation if predictions are constant\n",
    "        if np.std(pred_scores)==0 or np.std(gold_scores)==0:\n",
    "            return (0,0)\n",
    "        \n",
    "\n",
    "        pears_corr=scipy.stats.pearsonr(pred_scores,gold_scores)[0]                                                 \n",
    "        pears_corr_range_05_1=scipy.stats.pearsonr(pred_scores_range_05_1,gold_scores_range_05_1)[0]                         \n",
    "        return (pears_corr,pears_corr_range_05_1)                        \n",
    "                          \n",
    "    else:\n",
    "        sys.exit('Predictions ('+os.path.basename(pred)+') and gold data ('+os.path.basename(gold)+') have different number of lines.')          \n",
    "        \n",
    "        \n",
    "def evaluate_oc(pred,gold):  \n",
    "    \"\"\"Calculates performance metrics for ordinal classification.\n",
    "    \n",
    "    :param pred: the file path of the predictions\n",
    "    :param gold: the filte path withe gold data\n",
    "    :return: a list with performace metrics.\n",
    "    \n",
    "    \"\"\"\n",
    "  \n",
    "   \n",
    "    f=open(pred, \"rb\")\n",
    "    pred_lines=f.readlines()\n",
    "    f.close()\n",
    "    \n",
    "    f=open(gold, \"rb\")\n",
    "    gold_lines=f.readlines()\n",
    "    f.close()    \n",
    "\n",
    "\n",
    "    if(len(pred_lines)==len(gold_lines)):       \n",
    "        # align tweets ids with gold scores and predictions\n",
    "        data_dic={}\n",
    "\n",
    "    \n",
    "        header=True        \n",
    "        for line in gold_lines:\n",
    "            \n",
    "            if header:\n",
    "                header=False\n",
    "                continue\n",
    "            \n",
    "            parts=line.split('\\t')\n",
    "            \n",
    "            label=int(parts[3].split(\":\")[0])\n",
    "            \n",
    "            if len(parts)==4:   \n",
    "                data_dic[parts[0]]=[label]\n",
    "                \n",
    "            else:\n",
    "                sys.exit('Format problem in '+os.path.basename(gold)+'. Please report this problem to the task organizers.')\n",
    "        \n",
    "        \n",
    "        header=True         \n",
    "        for line in pred_lines:\n",
    "            if header:\n",
    "                header=False\n",
    "                continue            \n",
    "            parts=line.split('\\t')   \n",
    "            label=int(parts[3].split(\":\")[0])\n",
    "            if len(parts)==4:  \n",
    "                if parts[0] in data_dic:\n",
    "                    try:\n",
    "                        data_dic[parts[0]].append(label)\n",
    "                    except ValueError:\n",
    "                        # Invalid predictions are replaced by a default value\n",
    "                        data_dic[parts[0]].append(int(0))\n",
    "                else:\n",
    "                    sys.exit('Invalid tweet id ('+parts[0]+') in '+os.path.basename(pred)+'.')\n",
    "            else:\n",
    "                sys.exit('Format problem in '+os.path.basename(pred)+'.') \n",
    "            \n",
    "        \n",
    "        # lists storing gold and prediction scores\n",
    "        gold_scores=[]  \n",
    "        pred_scores=[]\n",
    "         \n",
    "        \n",
    "        # lists storing gold and prediction scores where gold score >= 0.5\n",
    "        gold_scores_some=[]\n",
    "        pred_scores_some=[]\n",
    "            \n",
    "        for id in data_dic:\n",
    "            if(len(data_dic[id])==2):\n",
    "                gold_scores.append(data_dic[id][0])\n",
    "                pred_scores.append(data_dic[id][1])\n",
    "                \n",
    "                if(data_dic[id][0]!=0):\n",
    "                    gold_scores_some.append(data_dic[id][0])\n",
    "                    pred_scores_some.append(data_dic[id][1])\n",
    "                \n",
    "            else:\n",
    "                sys.exit('Repeated id ('+id+') in '+os.path.basename(pred)+' .')\n",
    "\n",
    "                \n",
    "                \n",
    "        # return null scores if predictions are constant\n",
    "        if np.std(pred_scores)==0 or np.std(gold_scores)==0:\n",
    "            return (0,0,0,0)\n",
    "\n",
    "        pears_corr=scipy.stats.pearsonr(pred_scores,gold_scores)[0]              \n",
    "        pears_corr_some=scipy.stats.pearsonr(pred_scores_some,gold_scores_some)[0]  \n",
    "        \n",
    "        # fix labels to values observed in gold data        \n",
    "        gold_labels=list(sorted(set(gold_scores)))\n",
    "       \n",
    "        kappa=cohen_kappa_score(pred_scores,gold_scores,labels=gold_labels, weights='quadratic')        \n",
    "        kappa_some=cohen_kappa_score(pred_scores_some,gold_scores_some, labels=gold_labels, weights='quadratic')\n",
    " \n",
    "        return (pears_corr,pears_corr_some,kappa,kappa_some)\n",
    "\n",
    "    else:\n",
    "        sys.exit('Predictions ('+os.path.basename(pred)+') and gold data ('+os.path.basename(gold)+') have different number of lines.')          \n",
    "  \n",
    "        \n",
    "def evaluate_multilabel(pred,gold):  \n",
    "    \"\"\"Calculates performance metrics for multi-label classification.\n",
    "    \n",
    "    :param pred: the file path of the predictions\n",
    "    :param gold: the filte path withe gold data\n",
    "    :return: a list with performace metrics.    \n",
    "    \"\"\"     \n",
    "    \n",
    "    f=open(pred, \"rb\")\n",
    "    pred_lines=f.readlines()\n",
    "    f.close()\n",
    "    \n",
    "    f=open(gold, \"rb\")\n",
    "    gold_lines=f.readlines()\n",
    "    f.close()    \n",
    "\n",
    "    if(len(pred_lines)==len(gold_lines)):       \n",
    "        # align tweets ids with gold scores and predictions\n",
    "        data_dic={}\n",
    "\n",
    "    \n",
    "        header=True        \n",
    "        for line in gold_lines:\n",
    "           \n",
    "            if header:\n",
    "                header=False\n",
    "                continue\n",
    "            \n",
    "            parts=line.split('\\t')\n",
    "            \n",
    "            if len(parts)==13:  \n",
    "                labels=[]\n",
    "                for m_label in parts[2:13]:\n",
    "                    labels.append(int(m_label))\n",
    " \n",
    "                data_dic[parts[0]]=[tuple(labels)]\n",
    "                \n",
    "            else:\n",
    "                sys.exit('Format problem in '+os.path.basename(gold)+'. Please report this problem to the task organizers.')\n",
    "        \n",
    "        header=True         \n",
    "        for line in pred_lines:\n",
    "            if header:\n",
    "                header=False\n",
    "                continue            \n",
    "            parts=line.split('\\t')   \n",
    "            if len(parts)==13:  \n",
    "                if parts[0] in data_dic:\n",
    "                    try:\n",
    "                        labels=[]\n",
    "                        for m_label in parts[2:13]:\n",
    "                            labels.append(int(m_label))\n",
    "                        data_dic[parts[0]].append(tuple(labels))\n",
    "                    except ValueError:\n",
    "                        # Invalid predictions are replaced by a default value\n",
    "                        data_dic[parts[0]].append((0,0,0,0,0,0,0,0,0,0,0))\n",
    "                else:\n",
    "                    sys.exit('Invalid tweet id in '+os.path.basename(pred)+'.')\n",
    "            else:\n",
    "                sys.exit('Format problem in '+os.path.basename(pred)+'.')   \n",
    "            \n",
    "\n",
    "       # lists storing gold and prediction scores\n",
    "        gold_scores=[]  \n",
    "        pred_scores=[]\n",
    "         \n",
    "            \n",
    "        for id in data_dic:\n",
    "            if(len(data_dic[id])==2):\n",
    "                gold_scores.append(data_dic[id][0])\n",
    "                pred_scores.append(data_dic[id][1])\n",
    "                \n",
    "            else:\n",
    "                sys.exit('Repeated id ('+id+') in '+os.path.basename(pred)+' .')\n",
    "\n",
    "\n",
    "        y_true = np.array(gold_scores)\n",
    "        y_pred = np.array(pred_scores)       \n",
    "    \n",
    "        acc=jaccard_similarity_score(y_true,y_pred)       \n",
    "        f1_micro=f1_score(y_true, y_pred, average='micro')  \n",
    "        f1_macro=f1_score(y_true, y_pred, average='macro')  \n",
    "            \n",
    "        return (acc,f1_micro,f1_macro)\n",
    "    \n",
    "    else:\n",
    "        sys.exit('Predictions ('+os.path.basename(pred)+') and gold data ('+os.path.basename(gold)+') have different number of lines.')          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zxXHdW0IODWx"
   },
   "outputs": [],
   "source": [
    "def model1(x_train_3, y_train_3,x_val_3, y_val_3, embedding_layer):\n",
    "\n",
    "\tmodel1 = Sequential()\n",
    "\tmodel1.add(embedding_layer)\n",
    "\tmodel1.add(LSTM(32))\n",
    "\tmodel1.add(Dropout(0.2))\n",
    "\tmodel1.add(Dense(32, activation='relu'))\n",
    "\tmodel1.add(Dropout(0.2))\n",
    "\tmodel1.add(Dense(3, activation='softmax'))\n",
    "\tmodel1.compile(loss='categorical_crossentropy',\n",
    "\t\t\t      optimizer='Adam',\n",
    "\t\t\t      metrics=['acc'])\n",
    "\tmodel1.summary()\n",
    "\thistory=model1.fit(x_train_3, y_train_3, validation_data=(x_val_3, y_val_3),epochs=6, batch_size=50)\n",
    "\tmodel1.save(\"./model1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sdLO3C6WODMT"
   },
   "outputs": [],
   "source": [
    "def model2(x_train_3, y_train_3,x_val_3, y_val_3, embedding_layer,epochs, batch_size):\n",
    "\tmodel2 = Sequential()\n",
    "\tmodel2.add(embedding_layer)\n",
    "\tmodel2.add(GRU(32))\n",
    "\tmodel2.add(Dropout(0.2))\n",
    "\tmodel2.add(Dense(3, activation='softmax'))\n",
    "\tmodel2.compile(loss='categorical_crossentropy',\n",
    "\t\t\t      optimizer='rmsprop',\n",
    "\t\t\t      metrics=['acc'])\n",
    "\tmodel2.summary()\n",
    "\thistory=model2.fit(x_train_3, y_train_3, validation_data=(x_val_3, y_val_3),epochs=epochs, batch_size=batch_size)\n",
    "\tmodel2.save(\"./model2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OQiBIkDmKoEU"
   },
   "outputs": [],
   "source": [
    "#Path of datas\n",
    "data_train_3 =  \"train_1_1.csv\"\n",
    "data_train_7 =  \"train_3_3.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FezwSR_SLp4p"
   },
   "outputs": [],
   "source": [
    "#Load Data\n",
    "data_train_3= pd.read_csv(data_train_3,sep='\\t',names=['id','sentiment','data'])\n",
    "data_train_7= pd.read_csv(data_train_7,sep='\\t',names=['id','sentiment','data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CTmWa7BuKoBy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "# Apply the preprocessiog on the data\n",
    "# Normalize the tweets\n",
    "tweets_train_3, sentiments_train_3,data_train_3 = data_preprocessing(data_train_3)\n",
    "tweets_train_7, sentiments_train_7,data_train_7 = data_preprocessing(data_train_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rlA-OcLQLKwi"
   },
   "outputs": [],
   "source": [
    "all_tweet = tweets_train_3.append(tweets_train_7)\n",
    "tokenizer = Tokenizer(filters=' ')\n",
    "tokenizer.fit_on_texts(all_tweet)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3KWoHuF5MvAZ"
   },
   "outputs": [],
   "source": [
    "sequences_train_3 = tokenizer.texts_to_sequences(tweets_train_3)\n",
    "sequences_train_7 = tokenizer.texts_to_sequences(tweets_train_7)\n",
    "sequences = sequences_train_3 + sequences_train_7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0rER1ZxaLgd1"
   },
   "outputs": [],
   "source": [
    "#Seek the longuest Tweet\n",
    "#To Apply a Padding\n",
    "MAX_SEQUENCE_LENGTH = 0\n",
    "for elt in sequences:\n",
    "\tif len(elt) > MAX_SEQUENCE_LENGTH:\n",
    "\t\tMAX_SEQUENCE_LENGTH = len(elt)\n",
    "  \n",
    "data_train_3 = pad_sequences(sequences_train_3, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_train_7 = pad_sequences(sequences_train_7, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GL0k6uf1Lga3"
   },
   "outputs": [],
   "source": [
    "indices_train_3 = np.arange(data_train_3.shape[0])\n",
    "data_train_3 = data_train_3[indices_train_3]\n",
    "\n",
    "indices_train_7 = np.arange(data_train_7.shape[0])\n",
    "data_train_7 = data_train_7[indices_train_7]\n",
    "\n",
    "labels_train_3 = to_categorical(np.asarray(sentiments_train_3), 3)\n",
    "labels_train_3 = labels_train_3[indices_train_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dEFq7QZbLgSk"
   },
   "outputs": [],
   "source": [
    "nb_words=len(word_index)+1\n",
    "EMBEDDING_DIM=300\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "90yb26nXNvnt"
   },
   "outputs": [],
   "source": [
    "oov=[]\n",
    "oov.append((np.random.rand(EMBEDDING_DIM)) - 1.0)\n",
    "oov = oov / np.linalg.norm(oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8X5fM9nSNvj5"
   },
   "outputs": [],
   "source": [
    "for word, i in word_index.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)\n",
    "    else:\n",
    "        embedding_matrix[i] = oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ofA7BZ1UNvgD"
   },
   "outputs": [],
   "source": [
    "split_idx = int(len(data_train_3)*0.70)\n",
    "x_train_3, x_val_3 = data_train_3[:split_idx], data_train_3[split_idx:]\n",
    "y_train_3, y_val_3 = labels_train_3 [:split_idx], labels_train_3[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RGqBPaa-Nvb8"
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False, name='embedding_layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wc_MJrSoNvYo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 31, 300)           18438300  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                42624     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 18,482,079\n",
      "Trainable params: 43,779\n",
      "Non-trainable params: 18,438,300\n",
      "_________________________________________________________________\n",
      "Train on 35233 samples, validate on 15100 samples\n",
      "Epoch 1/6\n",
      "35233/35233 [==============================] - 10s 295us/step - loss: 0.8374 - acc: 0.6122 - val_loss: 0.8151 - val_acc: 0.6087\n",
      "Epoch 2/6\n",
      "35233/35233 [==============================] - 10s 274us/step - loss: 0.7647 - acc: 0.6572 - val_loss: 0.8270 - val_acc: 0.6025\n",
      "Epoch 3/6\n",
      "35233/35233 [==============================] - 10s 282us/step - loss: 0.7380 - acc: 0.6725 - val_loss: 0.8025 - val_acc: 0.6145\n",
      "Epoch 4/6\n",
      "35233/35233 [==============================] - 10s 279us/step - loss: 0.7214 - acc: 0.6794 - val_loss: 0.8316 - val_acc: 0.6087\n",
      "Epoch 5/6\n",
      "35233/35233 [==============================] - 9s 258us/step - loss: 0.7034 - acc: 0.6904 - val_loss: 0.8505 - val_acc: 0.6106\n",
      "Epoch 6/6\n",
      "35233/35233 [==============================] - 9s 267us/step - loss: 0.6877 - acc: 0.6964 - val_loss: 0.8178 - val_acc: 0.6230\n"
     ]
    }
   ],
   "source": [
    "model1(x_train_3, y_train_3,x_val_3, y_val_3, embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dtZU5RcnNvS2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 31, 300)           18438300  \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 32)                31968     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 18,470,367\n",
      "Trainable params: 32,067\n",
      "Non-trainable params: 18,438,300\n",
      "_________________________________________________________________\n",
      "Train on 35233 samples, validate on 15100 samples\n",
      "Epoch 1/6\n",
      "35233/35233 [==============================] - 10s 273us/step - loss: 0.8351 - acc: 0.6103 - val_loss: 0.8939 - val_acc: 0.5842\n",
      "Epoch 2/6\n",
      "35233/35233 [==============================] - 9s 256us/step - loss: 0.7561 - acc: 0.6616 - val_loss: 0.8297 - val_acc: 0.6103\n",
      "Epoch 3/6\n",
      "35233/35233 [==============================] - 9s 261us/step - loss: 0.7346 - acc: 0.6741 - val_loss: 0.8391 - val_acc: 0.6046\n",
      "Epoch 4/6\n",
      "35233/35233 [==============================] - 9s 270us/step - loss: 0.7182 - acc: 0.6828 - val_loss: 0.8091 - val_acc: 0.6223\n",
      "Epoch 5/6\n",
      "35233/35233 [==============================] - 10s 282us/step - loss: 0.7037 - acc: 0.6896 - val_loss: 0.8326 - val_acc: 0.6179\n",
      "Epoch 6/6\n",
      "35233/35233 [==============================] - 10s 292us/step - loss: 0.6899 - acc: 0.6948 - val_loss: 0.8040 - val_acc: 0.6259\n"
     ]
    }
   ],
   "source": [
    "model2(x_train_3, y_train_3,x_val_3, y_val_3, embedding_layer, 6, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Af6C8eMXNvOF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4      -2\n",
      "       ..\n",
      "1625    3\n",
      "1626   -2\n",
      "1627    1\n",
      "1628    0\n",
      "1629   -2\n",
      "Name: sentiment, Length: 1630, dtype: int64 0                             [yeah, ☺, ️, playing, well]\n",
      "1       [least, guy, trying, discourage, anymore, want...\n",
      "2       [uplift, still, discouraged, means, listening,...\n",
      "3                              [age, heyday, blood, tame]\n",
      "4       [embarrassed, saw, us, like, knvfkkjg, thinks,...\n",
      "                              ...                        \n",
      "1625       [idk, help, someone, smile, comes, lips, n, n]\n",
      "1626    [think, leep, favorite, get, dark, maybe, inso...\n",
      "1627                   [amelia, want, sarah, v, grateful]\n",
      "1628                                  [lack, makes, n, n]\n",
      "1629                   [james, clapper, cary, disturbing]\n",
      "Name: data, Length: 1630, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(sentiments_train_7, tweets_train_7)\n",
    "labels_train_7 = to_categorical(np.asarray(sentiments_train_7), 7)\n",
    "labels_train_7 = labels_train_7[indices_train_7]\n",
    "\n",
    "split_idx = int(len(data_train_7)*0.85)\n",
    "x_train_7, x_val_7 = data_train_7[:split_idx], data_train_7[split_idx:]\n",
    "y_train_7, y_val_7 = labels_train_7 [:split_idx], labels_train_7[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zJAVpUubpzrY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 31, 300)           18438300  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                42624     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 18,482,079\n",
      "Trainable params: 43,779\n",
      "Non-trainable params: 18,438,300\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 31, 300)           18438300  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                42624     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 99        \n",
      "_________________________________________________________________\n",
      "dense1 (Dense)               (None, 150)               600       \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               (None, 64)                9664      \n",
      "_________________________________________________________________\n",
      "dense3 (Dense)               (None, 7)                 455       \n",
      "=================================================================\n",
      "Total params: 18,482,079\n",
      "Trainable params: 43,779\n",
      "Non-trainable params: 18,438,300\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beuvry_j/.local/lib/python3.7/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1385 samples, validate on 245 samples\n",
      "Epoch 1/11\n",
      "1385/1385 [==============================] - 1s 559us/step - loss: 1.8448 - accuracy: 0.2845 - val_loss: 1.7641 - val_accuracy: 0.2898\n",
      "Epoch 2/11\n",
      "1385/1385 [==============================] - 0s 302us/step - loss: 1.6811 - accuracy: 0.3307 - val_loss: 1.7109 - val_accuracy: 0.2898\n",
      "Epoch 3/11\n",
      "1385/1385 [==============================] - 0s 300us/step - loss: 1.6149 - accuracy: 0.3552 - val_loss: 1.6569 - val_accuracy: 0.2898\n",
      "Epoch 4/11\n",
      "1385/1385 [==============================] - 0s 288us/step - loss: 1.5719 - accuracy: 0.3509 - val_loss: 1.7284 - val_accuracy: 0.2857\n",
      "Epoch 5/11\n",
      "1385/1385 [==============================] - 0s 308us/step - loss: 1.5446 - accuracy: 0.3632 - val_loss: 1.6649 - val_accuracy: 0.2939\n",
      "Epoch 6/11\n",
      "1385/1385 [==============================] - 0s 294us/step - loss: 1.5207 - accuracy: 0.3841 - val_loss: 1.6690 - val_accuracy: 0.3265\n",
      "Epoch 7/11\n",
      "1385/1385 [==============================] - 0s 295us/step - loss: 1.4876 - accuracy: 0.3964 - val_loss: 1.6613 - val_accuracy: 0.3592\n",
      "Epoch 8/11\n",
      "1385/1385 [==============================] - 0s 307us/step - loss: 1.4675 - accuracy: 0.4058 - val_loss: 1.6890 - val_accuracy: 0.3592\n",
      "Epoch 9/11\n",
      "1385/1385 [==============================] - 0s 288us/step - loss: 1.4445 - accuracy: 0.4130 - val_loss: 1.7020 - val_accuracy: 0.3755\n",
      "Epoch 10/11\n",
      "1385/1385 [==============================] - 0s 296us/step - loss: 1.4170 - accuracy: 0.4289 - val_loss: 1.7198 - val_accuracy: 0.3673\n",
      "Epoch 11/11\n",
      "1385/1385 [==============================] - 0s 309us/step - loss: 1.3963 - accuracy: 0.4419 - val_loss: 1.7517 - val_accuracy: 0.3592\n"
     ]
    }
   ],
   "source": [
    "model=keras.models.load_model(\"./model1.h5\")\n",
    "model.summary()\n",
    "model.layers.pop()\n",
    "model.layers.pop()\n",
    "model.add(Dense(150,activation='relu',name='dense1'))\n",
    "model.add(Dense(64,activation='relu',name='dense2'))\n",
    "model.add(Dense(7,activation='softmax',name='dense3'))\n",
    "model.summary()\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "history = model.fit(x_train_7, y_train_7,   validation_data=(x_val_7,y_val_7), epochs=11, batch_size=50)\n",
    "model.save(\"./model3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e02yEAVrNvKm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 0,  2, -3, ..., -2,  2,  2]), array([ 0,  0,  0, ...,  1,  0, -2]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpora_test_7=\"./train_3_3.csv\"\n",
    "corpora_test_7= pd.read_csv(corpora_test_7,sep='\\t',names=['id','sentiment','data'])\n",
    "\n",
    "tweets_test_7,sentiments_test_7,corpora_test_7 = data_preprocessing(corpora_test_7)\n",
    "sequences_test_7 = tokenizer.texts_to_sequences(tweets_test_7)\n",
    "pad_sequence_test_7 = pad_sequences(sequences_test_7, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "y_test_predict_7 = model.predict_classes(pad_sequence_test_7) - 3 # To replace between -3 and 3\n",
    "\n",
    "#cohen_kappa_score(y_test_predict_7, sentiments_test_7.values)\n",
    "y_test_predict_7, sentiments_test_7.values"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
