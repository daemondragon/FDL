{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2270,
     "status": "ok",
     "timestamp": 1573606004254,
     "user": {
      "displayName": "Benoit Frédéric-Moreau",
      "photoUrl": "",
      "userId": "09939474294080590706"
     },
     "user_tz": -60
    },
    "id": "XfkBZ2f9J_20",
    "outputId": "287a12b2-840a-4b29-c37f-b48b495e2c4e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/beuvry_j/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#from google.colab import drive\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import LSTM, Dropout, Dense, Bidirectional,  Flatten, Input, GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "from unidecode import unidecode\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords=stopwords.words('english')\n",
    "\n",
    "T = TweetTokenizer()\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "#!wget -P /root/input/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
    "EMBEDDING_FILE = '../GoogleNews-vectors-negative300.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tpo2LX6hJ_vg"
   },
   "outputs": [],
   "source": [
    "def value_sentiment(x):\n",
    "  if x == \"neutral\":\n",
    "    return 0\n",
    "  if x == \"positive\":\n",
    "    return 1\n",
    "  if x == \"negative\":\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m6d-_t2NKvQp"
   },
   "outputs": [],
   "source": [
    "def onehot (tweets,length):\n",
    "  onh = []\n",
    "  for tweet in tweets:\n",
    "      onh_seq = np.zeros(length) \n",
    "      for i in tweet:\n",
    "        onh_seq[i -1] = 1\n",
    "      onh.append(onh_seq)\n",
    "  return onh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TlSZY2t6J_gj"
   },
   "outputs": [],
   "source": [
    "#replace emojy\n",
    "emojis = {\n",
    "    u\":‑\\)\":\"☺️\",\n",
    "    u\":\\)\":\"☺️\",\n",
    "    u\":-\\]\":\"☺️\",\n",
    "    u\":\\]\":\"☺️\",\n",
    "    u\":-3\":\"☺️\",\n",
    "    u\":3\":\"☺️\",\n",
    "    u\":->\":\"☺️\",\n",
    "    u\":>\":\"☺️\",\n",
    "    u\"8-\\)\":\"☺️\",\n",
    "    u\":o\\)\":\"☺️\",\n",
    "    u\":-\\}\":\"☺️\",\n",
    "    u\":\\}\":\"☺️\",\n",
    "    u\":-\\)\":\"☺️\",\n",
    "    u\":c\\)\":\"☺️\",\n",
    "    u\":\\^\\)\":\"☺️\",\n",
    "    u\"=\\]\":\"☺️\",\n",
    "    u\"=\\)\":\"☺️\",\n",
    "    u\":‑D\":\"😃\",\n",
    "    u\":D\":\"😃\",\n",
    "    u\"8‑D\":\"😃\",\n",
    "    u\"8D\":\"😃\",\n",
    "    u\"X‑D\":\"😃\",\n",
    "    u\"XD\":\"😃\",\n",
    "    u\"=D\":\"😃\",\n",
    "    u\"=3\":\"😃\",\n",
    "    u\"B\\^D\":\"😃\",\n",
    "    u\":-\\)\\)\":\"😃\",\n",
    "    u\":‑\\(\":\"☹️\",\n",
    "    u\":-\\(\":\"☹️\",\n",
    "    u\":\\(\":\"☹️\",\n",
    "    u\":‑c\":\"☹️\",\n",
    "    u\":c\":\"☹️\",\n",
    "    u\":‑<\":\"☹️\",\n",
    "    u\":<\":\"☹️\",\n",
    "    u\":‑\\[\":\"☹️\",\n",
    "    u\":\\[\":\"☹️\",\n",
    "    u\":-\\|\\|\":\"☹️\",\n",
    "    u\">:\\[\":\"☹️\",\n",
    "    u\":\\{\":\"☹️\",\n",
    "    u\":@\":\"☹️\",\n",
    "    u\">:\\(\":\"☹️\",\n",
    "    u\":'‑\\(\":\"😭\",\n",
    "    u\":'\\(\":\"😭\",\n",
    "    u\":'‑\\)\":\"😃\",\n",
    "    u\":'\\)\":\"😃\",\n",
    "    u\"D‑':\":\"😨\",\n",
    "    u\"D:<\":\"😨\",\n",
    "    u\"D:\":\"😧\",\n",
    "    u\"D8\":\"😧\",\n",
    "    u\"D;\":\"😧\",\n",
    "    u\"D=\":\"😧\",\n",
    "    u\"DX\":\"😧\",\n",
    "    u\":‑O\":\"😮\",\n",
    "    u\":O\":\"😮\",\n",
    "    u\":‑o\":\"😮\",\n",
    "    u\":o\":\"😮\",\n",
    "    u\":-0\":\"😮\",\n",
    "    u\"8‑0\":\"😮\",\n",
    "    u\">:O\":\"😮\",\n",
    "    u\":-\\*\":\"😗\",\n",
    "    u\":\\*\":\"😗\",\n",
    "    u\":X\":\"😗\",\n",
    "    u\";‑\\)\":\"😉\",\n",
    "    u\";\\)\":\"😉\",\n",
    "    u\"\\*-\\)\":\"😉\",\n",
    "    u\"\\*\\)\":\"😉\",\n",
    "    u\";‑\\]\":\"😉\",\n",
    "    u\";\\]\":\"😉\",\n",
    "    u\";\\^\\)\":\"😉\",\n",
    "    u\":‑,\":\"😉\",\n",
    "    u\";D\":\"😉\",\n",
    "    u\":‑P\":\"😛\",\n",
    "    u\":P\":\"😛\",\n",
    "    u\"X‑P\":\"😛\",\n",
    "    u\"XP\":\"😛\",\n",
    "    u\":‑Þ\":\"😛\",\n",
    "    u\":Þ\":\"😛\",\n",
    "    u\":b\":\"😛\",\n",
    "    u\"d:\":\"😛\",\n",
    "    u\"=p\":\"😛\",\n",
    "    u\">:P\":\"😛\",\n",
    "    u\":‑/\":\"😕\",\n",
    "    u\":/\":\"😕\",\n",
    "    u\":-[.]\":\"😕\",\n",
    "    u\">:[(\\\\\\)]\":\"😕\",\n",
    "    u\">:/\":\"😕\",\n",
    "    u\":[(\\\\\\)]\":\"😕\",\n",
    "    u\"=/\":\"😕\",\n",
    "    u\"=[(\\\\\\)]\":\"😕\",\n",
    "    u\":L\":\"😕\",\n",
    "    u\"=L\":\"😕\",\n",
    "    u\":S\":\"😕\",\n",
    "    u\":‑\\|\":\"😐\",\n",
    "    u\":\\|\":\"😐\",\n",
    "    u\":$\":\"😳\",\n",
    "    u\":‑x\":\"🤐\",\n",
    "    u\":x\":\"🤐\",\n",
    "    u\":‑#\":\"🤐\",\n",
    "    u\":#\":\"🤐\",\n",
    "    u\":‑&\":\"🤐\",\n",
    "    u\":&\":\"🤐\",\n",
    "    u\"O:‑\\)\":\"😇\",\n",
    "    u\"O:\\)\":\"😇\",\n",
    "    u\"0:‑3\":\"😇\",\n",
    "    u\"0:3\":\"😇\",\n",
    "    u\"0:‑\\)\":\"😇\",\n",
    "    u\"0:\\)\":\"😇\",\n",
    "    u\":‑b\":\"😛\",\n",
    "    u\"0;\\^\\)\":\"😇\",\n",
    "    u\">:‑\\)\":\"😈\",\n",
    "    u\">:\\)\":\"😈\",\n",
    "    u\"\\}:‑\\)\":\"😈\",\n",
    "    u\"\\}:\\)\":\"😈\",\n",
    "    u\"3:‑\\)\":\"😈\",\n",
    "    u\"3:\\)\":\"😈\",\n",
    "    u\">;\\)\":\"😈\",\n",
    "    u\"\\|;‑\\)\":\"😎\",\n",
    "    u\"\\|‑O\":\"😏\",\n",
    "    u\":‑J\":\"😏\",\n",
    "    u\"%‑\\)\":\"😵\",\n",
    "    u\"%\\)\":\"😵\",\n",
    "    u\":-###..\":\"🤒\",\n",
    "    u\":###..\":\"🤒\",\n",
    "    u\"\\(>_<\\)\":\"😣\",\n",
    "    u\"\\(>_<\\)>\":\"😣\",\n",
    "    u\"\\(';'\\)\":\"👶\",\n",
    "    u\"\\(\\^\\^>``\":\"😓\",\n",
    "    u\"\\(\\^_\\^;\\)\":\"😓\",\n",
    "    u\"\\(-_-;\\)\":\"😓\",\n",
    "    u\"\\(~_~;\\) \\(・\\.・;\\)\":\"😓\",\n",
    "    u\"\\(-_-\\)zzz\":\"😴\",\n",
    "    u\"\\(\\^_-\\)\":\"😉\",\n",
    "    u\"\\(\\(\\+_\\+\\)\\)\":\"😕\",\n",
    "    u\"\\(\\+o\\+\\)\":\"😕\",\n",
    "    u\"\\^_\\^\":\"😃\",\n",
    "    u\"\\(\\^_\\^\\)/\":\"😃\",\n",
    "    u\"\\(\\^O\\^\\)／\":\"😃\",\n",
    "    u\"\\(\\^o\\^\\)／\":\"😃\",\n",
    "    u\"\\(__\\)\":\"🙇\",\n",
    "    u\"_\\(\\._\\.\\)_\":\"🙇\",\n",
    "    u\"<\\(_ _\\)>\":\"🙇\",\n",
    "    u\"<m\\(__\\)m>\":\"🙇\",\n",
    "    u\"m\\(__\\)m\":\"🙇\",\n",
    "    u\"m\\(_ _\\)m\":\"🙇\",\n",
    "    u\"\\('_'\\)\":\"😭\",\n",
    "    u\"\\(/_;\\)\":\"😭\",\n",
    "    u\"\\(T_T\\) \\(;_;\\)\":\"😭\",\n",
    "    u\"\\(;_;\":\"😭\",\n",
    "    u\"\\(;_:\\)\":\"😭\",\n",
    "    u\"\\(;O;\\)\":\"😭\",\n",
    "    u\"\\(:_;\\)\":\"😭\",\n",
    "    u\"\\(ToT\\)\":\"😭\",\n",
    "    u\";_;\":\"😭\",\n",
    "    u\";-;\":\"😭\",\n",
    "    u\";n;\":\"😭\",\n",
    "    u\";;\":\"😭\",\n",
    "    u\"Q\\.Q\":\"😭\",\n",
    "    u\"T\\.T\":\"😭\",\n",
    "    u\"QQ\":\"😭\",\n",
    "    u\"Q_Q\":\"😭\",\n",
    "    u\"\\(-\\.-\\)\":\"😞\",\n",
    "    u\"\\(-_-\\)\":\"😞\",\n",
    "    u\"\\(一一\\)\":\"😞\",\n",
    "    u\"\\(；一_一\\)\":\"😞\",\n",
    "    u\"\\(=_=\\)\":\"😩\",\n",
    "    u\"\\(=\\^\\·\\^=\\)\":\"😺\",\n",
    "    u\"\\(=\\^\\·\\·\\^=\\)\":\"😺\",\n",
    "    u\"=_\\^=\t\":\"😺\",\n",
    "    u\"\\(\\.\\.\\)\":\"😔\",\n",
    "    u\"\\(\\._\\.\\)\":\"😔\",\n",
    "    u\"\\(\\・\\・?\":\"😕\",\n",
    "    u\"\\(?_?\\)\":\"😕\",\n",
    "    u\">\\^_\\^<\":\"😃\",\n",
    "    u\"<\\^!\\^>\":\"😃\",\n",
    "    u\"\\^/\\^\":\"😃\",\n",
    "    u\"\\（\\*\\^_\\^\\*）\" :\"😃\",\n",
    "    u\"\\(\\^<\\^\\) \\(\\^\\.\\^\\)\":\"😃\",\n",
    "    u\"\\(^\\^\\)\":\"😃\",\n",
    "    u\"\\(\\^\\.\\^\\)\":\"😃\",\n",
    "    u\"\\(\\^_\\^\\.\\)\":\"😃\",\n",
    "    u\"\\(\\^_\\^\\)\":\"😃\",\n",
    "    u\"\\(\\^\\^\\)\":\"😃\",\n",
    "    u\"\\(\\^J\\^\\)\":\"😃\",\n",
    "    u\"\\(\\*\\^\\.\\^\\*\\)\":\"😃\",\n",
    "    u\"\\(\\^—\\^\\）\":\"😃\",\n",
    "    u\"\\(#\\^\\.\\^#\\)\":\"😃\",\n",
    "    u\"\\（\\^—\\^\\）\":\"👋\",\n",
    "    u\"\\(;_;\\)/~~~\":\"👋\",\n",
    "    u\"\\(\\^\\.\\^\\)/~~~\":\"👋\",\n",
    "    u\"\\(T_T\\)/~~~\":\"👋\",\n",
    "    u\"\\(ToT\\)/~~~\":\"👋\",\n",
    "    u\"\\(\\*\\^0\\^\\*\\)\":\"😍\",\n",
    "    u\"\\(\\*_\\*\\)\":\"😍\",\n",
    "    u\"\\(\\*_\\*;\":\"😍\",\n",
    "    u\"\\(\\+_\\+\\) \\(@_@\\)\":\"😍\",\n",
    "    u\"\\(\\*\\^\\^\\)v\":\"😂\",\n",
    "    u\"\\(\\^_\\^\\)v\":\"😂\",\n",
    "    u'\\(-\"-\\)':\"😓\",\n",
    "    u\"\\(ーー;\\)\":\"😓\",\n",
    "    u\"\\(\\^0_0\\^\\)\":\"😎\",\n",
    "    u\"\\(\\＾ｖ\\＾\\)\":\"😀\",\n",
    "    u\"\\(\\＾ｕ\\＾\\)\":\"😀\",\n",
    "    u\"\\(\\^\\)o\\(\\^\\)\":\"😀\",\n",
    "    u\"\\(\\^O\\^\\)\":\"😀\",\n",
    "    u\"\\(\\^o\\^\\)\":\"😀\",\n",
    "    u\"\\)\\^o\\^\\(\":\"😀\",\n",
    "    u\":O o_O\":\"😮\",\n",
    "    u\"o_0\":\"😮\",\n",
    "    u\"o\\.O\":\"😮\",\n",
    "    u\"\\(o\\.o\\)\":\"😮\",\n",
    "    u\"oO\":\"😮\",\n",
    "    u\"\\(\\*￣m￣\\)\":\"😠\",\n",
    "    u\":‑)\":\"☺️\",\n",
    "    u\":)\":\"☺️\",\n",
    "    u\":-]\":\"☺️\",\n",
    "    u\":]\":\"☺️\",\n",
    "    u\":-3\":\"☺️\",\n",
    "    u\":3\":\"☺️\",\n",
    "    u\":->\":\"☺️\",\n",
    "    u\":>\":\"☺️\",\n",
    "    u\"8-)\":\"☺️\",\n",
    "    u\":o)\":\"☺️\",\n",
    "    u\":-}\":\"☺️\",\n",
    "    u\":}\":\"☺️\",\n",
    "    u\":-)\":\"☺️\",\n",
    "    u\":c)\":\"☺️\",\n",
    "    u\":^)\":\"☺️\",\n",
    "    u\"=]\":\"☺️\",\n",
    "    u\"=)\":\"☺️\",\n",
    "    u\":‑D\":\"😃\",\n",
    "    u\":D\":\"😃\",\n",
    "    u\"8‑D\":\"😃\",\n",
    "    u\"8D\":\"😃\",\n",
    "    u\"X‑D\":\"😃\",\n",
    "    u\"XD\":\"😃\",\n",
    "    u\"=D\":\"😃\",\n",
    "    u\"=3\":\"😃\",\n",
    "    u\"B^D\":\"😃\",\n",
    "    u\":-))\":\"😃\",\n",
    "    u\":-(\":\"☹️\",\n",
    "    u\":‑(\":\"☹️\",\n",
    "    u\":(\":\"☹️\",\n",
    "    u\":‑c\":\"☹️\",\n",
    "    u\":c\":\"☹️\",\n",
    "    u\":‑<\":\"☹️\",\n",
    "    u\":<\":\"☹️\",\n",
    "    u\":‑[\":\"☹️\",\n",
    "    u\":[\":\"☹️\",\n",
    "    u\":-||\":\"☹️\",\n",
    "    u\">:[\":\"☹️\",\n",
    "    u\":{\":\"☹️\",\n",
    "    u\":@\":\"☹️\",\n",
    "    u\">:(\":\"☹️\",\n",
    "    u\":'‑(\":\"😭\",\n",
    "    u\":'(\":\"😭\",\n",
    "    u\":'‑)\":\"😃\",\n",
    "    u\":')\":\"😃\",\n",
    "    u\"D‑':\":\"😧\",\n",
    "    u\"D:<\":\"😨\",\n",
    "    u\"D:\":\"😧\",\n",
    "    u\"D8\":\"😧\",\n",
    "    u\"D;\":\"😧\",\n",
    "    u\"D=\":\"😧\",\n",
    "    u\"DX\":\"😧\",\n",
    "    u\":‑O\":\"😮\",\n",
    "    u\":O\":\"😮\",\n",
    "    u\":‑o\":\"😮\",\n",
    "    u\":o\":\"😮\",\n",
    "    u\":-0\":\"😮\",\n",
    "    u\"8‑0\":\"😮\",\n",
    "    u\">:O\":\"😮\",\n",
    "    u\":-*\":\"😗\",\n",
    "    u\":*\":\"😗\",\n",
    "    u\":X\":\"😗\",\n",
    "    u\";‑)\":\"😉\",\n",
    "    u\";)\":\"😉\",\n",
    "    u\"*-)\":\"😉\",\n",
    "    u\"*)\":\"😉\",\n",
    "    u\";‑]\":\"😉\",\n",
    "    u\";]\":\"😉\",\n",
    "    u\";^)\":\"😉\",\n",
    "    u\":‑,\":\"😉\",\n",
    "    u\";D\":\"😉\",\n",
    "    u\":‑P\":\"😛\",\n",
    "    u\":P\":\"😛\",\n",
    "    u\"X‑P\":\"😛\",\n",
    "    u\"XP\":\"😛\",\n",
    "    u\":‑Þ\":\"😛\",\n",
    "    u\":Þ\":\"😛\",\n",
    "    u\":b\":\"😛\",\n",
    "    u\"d:\":\"😛\",\n",
    "    u\"=p\":\"😛\",\n",
    "    u\">:P\":\"😛\",\n",
    "    u\":‑/\":\"😕\",\n",
    "    u\":/\":\"😕\",\n",
    "    u\":-[.]\":\"😕\",\n",
    "    u\">:[(\\)]\":\"😕\",\n",
    "    u\">:/\":\"😕\",\n",
    "    u\":[(\\)]\":\"😕\",\n",
    "    u\"=/\":\"😕\",\n",
    "    u\"=[(\\)]\":\"😕\",\n",
    "    u\":L\":\"😕\",\n",
    "    u\"=L\":\"😕\",\n",
    "    u\":S\":\"😕\",\n",
    "    u\":‑|\":\"😐\",\n",
    "    u\":|\":\"😐\",\n",
    "    u\":$\":\"😳\",\n",
    "    u\":‑x\":\"🤐\",\n",
    "    u\":x\":\"🤐\",\n",
    "    u\":‑#\":\"🤐\",\n",
    "    u\":#\":\"🤐\",\n",
    "    u\":‑&\":\"🤐\",\n",
    "    u\":&\":\"🤐\",\n",
    "    u\"O:‑)\":\"😇\",\n",
    "    u\"O:)\":\"😇\",\n",
    "    u\"0:‑3\":\"😇\",\n",
    "    u\"0:3\":\"😇\",\n",
    "    u\"0:‑)\":\"😇\",\n",
    "    u\"0:)\":\"😇\",\n",
    "    u\":‑b\":\"😛\",\n",
    "    u\"0;^)\":\"😇\",\n",
    "    u\">:‑)\":\"😈\",\n",
    "    u\">:)\":\"😈\",\n",
    "    u\"}:‑)\":\"😈\",\n",
    "    u\"}:)\":\"😈\",\n",
    "    u\"3:‑)\":\"😈\",\n",
    "    u\"3:)\":\"😈\",\n",
    "    u\">;)\":\"😈\",\n",
    "    u\"|;‑)\":\"😎\",\n",
    "    u\"|‑O\":\"😏\",\n",
    "    u\":‑J\":\"😏\",\n",
    "    u\"%‑)\":\"😵\",\n",
    "    u\"%)\":\"😵\",\n",
    "    u\":-###..\":\"🤒\",\n",
    "    u\":###..\":\"🤒\",\n",
    "    u\"(>_<)\":\"😣\",\n",
    "    u\"(>_<)>\":\"😣\",\n",
    "    u\"(';')\":\"Baby\",\n",
    "    u\"(^^>``\":\"😓\",\n",
    "    u\"(^_^;)\":\"😓\",\n",
    "    u\"(-_-;)\":\"😓\",\n",
    "    u\"(~_~;) (・.・;)\":\"😓\",\n",
    "    u\"(-_-)zzz\":\"😴\",\n",
    "    u\"(^_-)\":\"😉\",\n",
    "    u\"((+_+))\":\"😕\",\n",
    "    u\"(+o+)\":\"😕\",\n",
    "    u\"^_^\":\"😃\",\n",
    "    u\"(^_^)/\":\"😃\",\n",
    "    u\"(^O^)／\":\"😃\",\n",
    "    u\"(^o^)／\":\"😃\",\n",
    "    u\"(__)\":\"🙇\",\n",
    "    u\"_(._.)_\":\"🙇\",\n",
    "    u\"<(_ _)>\":\"🙇\",\n",
    "    u\"<m(__)m>\":\"🙇\",\n",
    "    u\"m(__)m\":\"🙇\",\n",
    "    u\"m(_ _)m\":\"🙇\",\n",
    "    u\"('_')\":\"😭\",\n",
    "    u\"(/_;)\":\"😭\",\n",
    "    u\"(T_T) (;_;)\":\"😭\",\n",
    "    u\"(;_;\":\"😭\",\n",
    "    u\"(;_:)\":\"😭\",\n",
    "    u\"(;O;)\":\"😭\",\n",
    "    u\"(:_;)\":\"😭\",\n",
    "    u\"(ToT)\":\"😭\",\n",
    "    u\";_;\":\"😭\",\n",
    "    u\";-;\":\"😭\",\n",
    "    u\";n;\":\"😭\",\n",
    "    u\";;\":\"😭\",\n",
    "    u\"Q.Q\":\"😭\",\n",
    "    u\"T.T\":\"😭\",\n",
    "    u\"QQ\":\"😭\",\n",
    "    u\"Q_Q\":\"😭\",\n",
    "    u\"(-.-)\":\"😞\",\n",
    "    u\"(-_-)\":\"😞\",\n",
    "    u\"(一一)\":\"😞\",\n",
    "    u\"(；一_一)\":\"😞\",\n",
    "    u\"(=_=)\":\"😩\",\n",
    "    u\"(=^·^=)\":\"😺\",\n",
    "    u\"(=^··^=)\":\"😺\",\n",
    "    u\"=_^= \":\"😺\",\n",
    "    u\"(..)\":\"😔\",\n",
    "    u\"(._.)\":\"😔\",\n",
    "    u\"(・・?\":\"😕\",\n",
    "    u\"(?_?)\":\"😕\",\n",
    "    u\">^_^<\":\"😃\",\n",
    "    u\"<^!^>\":\"😃\",\n",
    "    u\"^/^\":\"😃\",\n",
    "    u\"（*^_^*）\" :\"😃\",\n",
    "    u\"(^<^) (^.^)\":\"😃\",\n",
    "    u\"(^^)\":\"😃\",\n",
    "    u\"(^.^)\":\"😃\",\n",
    "    u\"(^_^.)\":\"😃\",\n",
    "    u\"(^_^)\":\"😃\",\n",
    "    u\"(^^)\":\"😃\",\n",
    "    u\"(^J^)\":\"😃\",\n",
    "    u\"(*^.^*)\":\"😃\",\n",
    "    u\"(^—^）\":\"😃\",\n",
    "    u\"(#^.^#)\":\"😃\",\n",
    "    u\"（^—^）\":\"👋\",\n",
    "    u\"(;_;)/~~~\":\"👋\",\n",
    "    u\"(^.^)/~~~\":\"👋\",\n",
    "    u\"(-_-)/~~~ ($··)/~~~\":\"👋\",\n",
    "    u\"(T_T)/~~~\":\"👋\",\n",
    "    u\"(ToT)/~~~\":\"👋\",\n",
    "    u\"(*^0^*)\":\"😍\",\n",
    "    u\"(*_*)\":\"😍\",\n",
    "    u\"(*_*;\":\"😍\",\n",
    "    u\"(+_+) (@_@)\":\"😍\",\n",
    "    u\"(*^^)v\":\"😂\",\n",
    "    u\"(^_^)v\":\"😂\",\n",
    "    u'(-\"-)':\"😓\",\n",
    "    u\"(ーー;)\":\"😓\",\n",
    "    u\"(^0_0^)\":\"😎\",\n",
    "    u\"(＾ｖ＾)\":\"😀\",\n",
    "    u\"(＾ｕ＾)\":\"😀\",\n",
    "    u\"(^)o(^)\":\"😀\",\n",
    "    u\"(^O^)\":\"😀\",\n",
    "    u\"(^o^)\":\"😀\",\n",
    "    u\")^o^(\":\"😀\",\n",
    "    u\":O o_O\":\"😮\",\n",
    "    u\"o_0\":\"😮\",\n",
    "    u\"o.O\":\"😮\",\n",
    "    u\"(o.o)\":\"😮\",\n",
    "    u\"oO\":\"😮\",\n",
    "}\n",
    "\n",
    "\n",
    "def str2emoji(tweet):\n",
    "\t\n",
    "\tfor pos,ej in enumerate(tweet):\n",
    "\t\tif ej in emojis:\n",
    "\t\t\ttweet[pos]=emojis[ej]\n",
    "\treturn tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UjpS6y1RKeeZ"
   },
   "outputs": [],
   "source": [
    "def norm_tweet(tweet):\n",
    "  tweet = re.sub(r\"\\\\u2019\", \"'\", tweet)\n",
    "  tweet = re.sub(r\"\\\\u002c\", \",\", tweet)\n",
    "  tweet=' '.join(str2emoji(unidecode(tweet).lower().split()))\n",
    "  tweet = re.sub(r\"\\'ve\", \" have\", tweet)\n",
    "  tweet = re.sub(r\" can\\'t\", \" cannot\", tweet)\n",
    "  tweet = re.sub(r\"n\\'t\", \" not\", tweet)\n",
    "  tweet = re.sub(r\"\\'re\", \" are\", tweet)\n",
    "  tweet = re.sub(r\"\\'d\", \" would\", tweet)\n",
    "  tweet = re.sub(r\"\\'ll\", \" will\", tweet)\n",
    "  tweet = re.sub(r\"\\'s\", \"\", tweet)\n",
    "  tweet = re.sub(r\"\\'n\", \"\", tweet)\n",
    "  tweet = re.sub(r\"\\'m\", \" am\", tweet)\n",
    "  tweet = re.sub(r\"@\\w+\", r' ',tweet)\n",
    "  tweet = re.sub(r\"#\\w+\", r' ',tweet)\n",
    "  tweet = re.sub(r\"[.]+\",\" \",tweet)\n",
    "  #T = tokenizer.TweetTokenizer()\n",
    "  tweet = T.tokenize(tweet)\n",
    "  #tweet = [lemmatizer.lemmatize(i,j[0].lower()) if j[0].lower() in ['a','n','v']  else lemmatizer.lemmatize(i) for i,j in pos_tag(tknzr.tokenize(tweet))]\n",
    "  tweet = [ i for i in tweet if (i not in stopwords) and (i not in string.punctuation ) ]\n",
    "  #tweet = ' '.join(tweet)\n",
    "  return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Rv6ih5wKoHV"
   },
   "outputs": [],
   "source": [
    "def data_preprocessing(data):\n",
    "  data['data'] = data['data'].apply(norm_tweet)\n",
    "  print(data['sentiment'].dtypes)\n",
    "  if data['sentiment'].dtypes not in [np.int64]:\n",
    "    data['sentiment']=data['sentiment'].apply(value_sentiment)\n",
    "  return data['data'],data['sentiment'],data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zxXHdW0IODWx"
   },
   "outputs": [],
   "source": [
    "def model1(x_train_3, y_train_3,x_val_3, y_val_3, embedding_layer):\n",
    "\n",
    "\tmodel1 = Sequential()\n",
    "\tmodel1.add(embedding_layer)\n",
    "\tmodel1.add(LSTM(32))\n",
    "\tmodel1.add(Dropout(0.2))\n",
    "\tmodel1.add(Dense(32, activation='relu'))\n",
    "\tmodel1.add(Dropout(0.2))\n",
    "\tmodel1.add(Dense(3, activation='softmax'))\n",
    "\tmodel1.compile(loss='categorical_crossentropy',\n",
    "\t\t\t      optimizer='Adam',\n",
    "\t\t\t      metrics=['acc'])\n",
    "\tmodel1.summary()\n",
    "\thistory=model1.fit(x_train_3, y_train_3, validation_data=(x_val_3, y_val_3),epochs=6, batch_size=50)\n",
    "\tmodel1.save(\"./model1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sdLO3C6WODMT"
   },
   "outputs": [],
   "source": [
    "def model2(x_train_3, y_train_3,x_val_3, y_val_3, embedding_layer,epochs, batch_size):\n",
    "\tmodel2 = Sequential()\n",
    "\tmodel2.add(embedding_layer)\n",
    "\tmodel2.add(GRU(32))\n",
    "\tmodel2.add(Dropout(0.2))\n",
    "\tmodel2.add(Dense(3, activation='softmax'))\n",
    "\tmodel2.compile(loss='categorical_crossentropy',\n",
    "\t\t\t      optimizer='rmsprop',\n",
    "\t\t\t      metrics=['acc'])\n",
    "\tmodel2.summary()\n",
    "\thistory=model2.fit(x_train_3, y_train_3, validation_data=(x_val_3, y_val_3),epochs=epochs, batch_size=batch_size)\n",
    "\tmodel2.save(\"./model2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OQiBIkDmKoEU"
   },
   "outputs": [],
   "source": [
    "#Path of datas\n",
    "data_train_3 =  \"train_1_1.csv\"\n",
    "data_train_7 =  \"train_3_3.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FezwSR_SLp4p"
   },
   "outputs": [],
   "source": [
    "#Load Data\n",
    "data_train_3= pd.read_csv(data_train_3,sep='\\t',names=['id','sentiment','data'])\n",
    "data_train_7= pd.read_csv(data_train_7,sep='\\t',names=['id','sentiment','data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CTmWa7BuKoBy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "# Apply the preprocessiog on the data\n",
    "# Normalize the tweets\n",
    "tweets_train_3, sentiments_train_3,data_train_3 = data_preprocessing(data_train_3)\n",
    "tweets_train_7, sentiments_train_7,data_train_7 = data_preprocessing(data_train_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rlA-OcLQLKwi"
   },
   "outputs": [],
   "source": [
    "all_tweet = tweets_train_3.append(tweets_train_7)\n",
    "tokenizer = Tokenizer(filters=' ')\n",
    "tokenizer.fit_on_texts(all_tweet)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3KWoHuF5MvAZ"
   },
   "outputs": [],
   "source": [
    "sequences_train_3 = tokenizer.texts_to_sequences(tweets_train_3)\n",
    "sequences_train_7 = tokenizer.texts_to_sequences(tweets_train_7)\n",
    "sequences = sequences_train_3 + sequences_train_7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0rER1ZxaLgd1"
   },
   "outputs": [],
   "source": [
    "#Seek the longuest Tweet\n",
    "#To Apply a Padding\n",
    "MAX_SEQUENCE_LENGTH = 0\n",
    "for elt in sequences:\n",
    "\tif len(elt) > MAX_SEQUENCE_LENGTH:\n",
    "\t\tMAX_SEQUENCE_LENGTH = len(elt)\n",
    "  \n",
    "data_train_3 = pad_sequences(sequences_train_3, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_train_7 = pad_sequences(sequences_train_7, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GL0k6uf1Lga3"
   },
   "outputs": [],
   "source": [
    "indices_train_3 = np.arange(data_train_3.shape[0])\n",
    "data_train_3 = data_train_3[indices_train_3]\n",
    "\n",
    "indices_train_7 = np.arange(data_train_7.shape[0])\n",
    "data_train_7 = data_train_7[indices_train_7]\n",
    "\n",
    "labels_train_3 = to_categorical(np.asarray(sentiments_train_3), 3)\n",
    "labels_train_3 = labels_train_3[indices_train_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dEFq7QZbLgSk"
   },
   "outputs": [],
   "source": [
    "nb_words=len(word_index)+1\n",
    "EMBEDDING_DIM=300\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "90yb26nXNvnt"
   },
   "outputs": [],
   "source": [
    "oov=[]\n",
    "oov.append((np.random.rand(EMBEDDING_DIM)) - 1.0)\n",
    "oov = oov / np.linalg.norm(oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8X5fM9nSNvj5"
   },
   "outputs": [],
   "source": [
    "for word, i in word_index.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)\n",
    "    else:\n",
    "        embedding_matrix[i] = oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ofA7BZ1UNvgD"
   },
   "outputs": [],
   "source": [
    "split_idx = int(len(data_train_3)*0.70)\n",
    "x_train_3, x_val_3 = data_train_3[:split_idx], data_train_3[split_idx:]\n",
    "y_train_3, y_val_3 = labels_train_3 [:split_idx], labels_train_3[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RGqBPaa-Nvb8"
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False, name='embedding_layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wc_MJrSoNvYo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 31, 300)           18438300  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                42624     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 18,482,079\n",
      "Trainable params: 43,779\n",
      "Non-trainable params: 18,438,300\n",
      "_________________________________________________________________\n",
      "Train on 35233 samples, validate on 15100 samples\n",
      "Epoch 1/6\n",
      "35233/35233 [==============================] - 10s 274us/step - loss: 0.8336 - acc: 0.6108 - val_loss: 0.8145 - val_acc: 0.6145\n",
      "Epoch 2/6\n",
      "35233/35233 [==============================] - 9s 267us/step - loss: 0.7590 - acc: 0.6603 - val_loss: 0.8519 - val_acc: 0.5868\n",
      "Epoch 3/6\n",
      "35233/35233 [==============================] - 9s 265us/step - loss: 0.7351 - acc: 0.6724 - val_loss: 0.8126 - val_acc: 0.6144\n",
      "Epoch 4/6\n",
      "35233/35233 [==============================] - 9s 265us/step - loss: 0.7198 - acc: 0.6821 - val_loss: 0.8096 - val_acc: 0.6162\n",
      "Epoch 5/6\n",
      "35233/35233 [==============================] - 9s 266us/step - loss: 0.7015 - acc: 0.6903 - val_loss: 0.7923 - val_acc: 0.6221\n",
      "Epoch 6/6\n",
      "35233/35233 [==============================] - 9s 264us/step - loss: 0.6860 - acc: 0.7002 - val_loss: 0.8309 - val_acc: 0.6109\n"
     ]
    }
   ],
   "source": [
    "model1(x_train_3, y_train_3,x_val_3, y_val_3, embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dtZU5RcnNvS2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 31, 300)           18438300  \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 32)                31968     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 18,470,367\n",
      "Trainable params: 32,067\n",
      "Non-trainable params: 18,438,300\n",
      "_________________________________________________________________\n",
      "Train on 35233 samples, validate on 15100 samples\n",
      "Epoch 1/6\n",
      "35233/35233 [==============================] - 10s 280us/step - loss: 0.8374 - acc: 0.6112 - val_loss: 0.8103 - val_acc: 0.6108\n",
      "Epoch 2/6\n",
      "35233/35233 [==============================] - 9s 267us/step - loss: 0.7539 - acc: 0.6641 - val_loss: 0.7940 - val_acc: 0.6241\n",
      "Epoch 3/6\n",
      "35233/35233 [==============================] - 10s 273us/step - loss: 0.7332 - acc: 0.6753 - val_loss: 0.8172 - val_acc: 0.6145\n",
      "Epoch 4/6\n",
      "35233/35233 [==============================] - 9s 263us/step - loss: 0.7175 - acc: 0.6814 - val_loss: 0.8082 - val_acc: 0.6203\n",
      "Epoch 5/6\n",
      "35233/35233 [==============================] - 9s 264us/step - loss: 0.7037 - acc: 0.6899 - val_loss: 0.8190 - val_acc: 0.6164\n",
      "Epoch 6/6\n",
      "35233/35233 [==============================] - 9s 264us/step - loss: 0.6889 - acc: 0.6955 - val_loss: 0.7952 - val_acc: 0.6269\n"
     ]
    }
   ],
   "source": [
    "model2(x_train_3, y_train_3,x_val_3, y_val_3, embedding_layer, 6, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Af6C8eMXNvOF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4      -2\n",
      "       ..\n",
      "1625    3\n",
      "1626   -2\n",
      "1627    1\n",
      "1628    0\n",
      "1629   -2\n",
      "Name: sentiment, Length: 1630, dtype: int64 0                             [yeah, ☺, ️, playing, well]\n",
      "1       [least, guy, trying, discourage, anymore, want...\n",
      "2       [uplift, still, discouraged, means, listening,...\n",
      "3                              [age, heyday, blood, tame]\n",
      "4       [embarrassed, saw, us, like, knvfkkjg, thinks,...\n",
      "                              ...                        \n",
      "1625       [idk, help, someone, smile, comes, lips, n, n]\n",
      "1626    [think, leep, favorite, get, dark, maybe, inso...\n",
      "1627                   [amelia, want, sarah, v, grateful]\n",
      "1628                                  [lack, makes, n, n]\n",
      "1629                   [james, clapper, cary, disturbing]\n",
      "Name: data, Length: 1630, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(sentiments_train_7, tweets_train_7)\n",
    "labels_train_7 = to_categorical(np.asarray(sentiments_train_7), 7)\n",
    "labels_train_7 = labels_train_7[indices_train_7]\n",
    "\n",
    "split_idx = int(len(data_train_7)*0.85)\n",
    "x_train_7, x_val_7 = data_train_7[:split_idx], data_train_7[split_idx:]\n",
    "y_train_7, y_val_7 = labels_train_7 [:split_idx], labels_train_7[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zJAVpUubpzrY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 31, 300)           18438300  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                42624     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 18,482,079\n",
      "Trainable params: 43,779\n",
      "Non-trainable params: 18,438,300\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 31, 300)           18438300  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                42624     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 99        \n",
      "_________________________________________________________________\n",
      "dense1 (Dense)               (None, 150)               600       \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               (None, 64)                9664      \n",
      "_________________________________________________________________\n",
      "dense3 (Dense)               (None, 7)                 455       \n",
      "=================================================================\n",
      "Total params: 18,482,079\n",
      "Trainable params: 43,779\n",
      "Non-trainable params: 18,438,300\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beuvry_j/.local/lib/python3.7/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1385 samples, validate on 245 samples\n",
      "Epoch 1/11\n",
      "1385/1385 [==============================] - 1s 575us/step - loss: 1.8115 - accuracy: 0.3040 - val_loss: 1.7134 - val_accuracy: 0.2735\n",
      "Epoch 2/11\n",
      "1385/1385 [==============================] - 0s 315us/step - loss: 1.6584 - accuracy: 0.3437 - val_loss: 1.6497 - val_accuracy: 0.2816\n",
      "Epoch 3/11\n",
      "1385/1385 [==============================] - 0s 319us/step - loss: 1.6028 - accuracy: 0.3545 - val_loss: 1.6366 - val_accuracy: 0.2939\n",
      "Epoch 4/11\n",
      "1385/1385 [==============================] - 0s 315us/step - loss: 1.5597 - accuracy: 0.3697 - val_loss: 1.6209 - val_accuracy: 0.3755\n",
      "Epoch 5/11\n",
      "1385/1385 [==============================] - 0s 316us/step - loss: 1.5282 - accuracy: 0.3776 - val_loss: 1.6350 - val_accuracy: 0.3510\n",
      "Epoch 6/11\n",
      "1385/1385 [==============================] - 0s 308us/step - loss: 1.4929 - accuracy: 0.4058 - val_loss: 1.6261 - val_accuracy: 0.3592\n",
      "Epoch 7/11\n",
      "1385/1385 [==============================] - 0s 317us/step - loss: 1.4641 - accuracy: 0.4065 - val_loss: 1.6385 - val_accuracy: 0.3714\n",
      "Epoch 8/11\n",
      "1385/1385 [==============================] - 0s 303us/step - loss: 1.4319 - accuracy: 0.4181 - val_loss: 1.6523 - val_accuracy: 0.3673\n",
      "Epoch 9/11\n",
      "1385/1385 [==============================] - 0s 302us/step - loss: 1.4133 - accuracy: 0.4289 - val_loss: 1.6749 - val_accuracy: 0.3796\n",
      "Epoch 10/11\n",
      "1385/1385 [==============================] - 0s 311us/step - loss: 1.3917 - accuracy: 0.4448 - val_loss: 1.6761 - val_accuracy: 0.3755\n",
      "Epoch 11/11\n",
      "1385/1385 [==============================] - 0s 309us/step - loss: 1.3541 - accuracy: 0.4527 - val_loss: 1.6926 - val_accuracy: 0.3673\n"
     ]
    }
   ],
   "source": [
    "model=keras.models.load_model(\"./model1.h5\")\n",
    "model.summary()\n",
    "model.layers.pop()\n",
    "model.layers.pop()\n",
    "model.add(Dense(150,activation='relu',name='dense1'))\n",
    "model.add(Dense(64,activation='relu',name='dense2'))\n",
    "model.add(Dense(7,activation='softmax',name='dense3'))\n",
    "model.summary()\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "history = model.fit(x_train_7, y_train_7,   validation_data=(x_val_7,y_val_7), epochs=11, batch_size=50)\n",
    "model.save(\"./model3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e02yEAVrNvKm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n"
     ]
    }
   ],
   "source": [
    "corpora_test_7=\"./train_3_3.csv\"\n",
    "corpora_test_7= pd.read_csv(corpora_test_7,sep='\\t',names=['id','sentiment','data'])\n",
    "\n",
    "tweets_test_7,sentiments_test_3,corpora_test_7 =  data_preprocessing(corpora_test_7)\n",
    "sequences_test_7 = tokenizer.texts_to_sequences(tweets_test_7)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
