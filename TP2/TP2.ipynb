{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_data(filename, columns, sep=\"\\t\"):\n",
    "    pd.read_csv(filename, sep=sep, names=columns)\n",
    "\n",
    "def map_column(df, columns, function):\n",
    "    df = df.copy()\n",
    "    for column in columns:\n",
    "        df[column] = df[column].apply(function)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/beuvry_j/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def tokenize(texts):\n",
    "    tokenizer = text.Tokenizer(filters=' ')\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    return tokenizer, tokenizer.word_index\n",
    "\n",
    "def to_sequences(tokenizer, texts):\n",
    "    return tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "def max_sequences_length(texts_list):\n",
    "    return max([ max(map(lambda text: len(text), texts)) for texts in texts_list ])\n",
    "\n",
    "def pad_sequences(texts_list, max_length=None):\n",
    "    if max_length is None:\n",
    "        max_length = max_sequences_length(texts_list)\n",
    "    return [ sequence.pad_sequences(texts, maxlen=max_length) for texts in texts_list ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#replace emojy\n",
    "emojis = {\n",
    "    u\":‑\\)\":\"☺️\", u\":\\)\":\"☺️\", u\":-\\]\":\"☺️\", u\":\\]\":\"☺️\", u\":-3\":\"☺️\",\n",
    "    u\":3\":\"☺️\",\n",
    "    u\":->\":\"☺️\",\n",
    "    u\":>\":\"☺️\",\n",
    "    u\"8-\\)\":\"☺️\",\n",
    "    u\":o\\)\":\"☺️\",\n",
    "    u\":-\\}\":\"☺️\",\n",
    "    u\":\\}\":\"☺️\",\n",
    "    u\":-\\)\":\"☺️\",\n",
    "    u\":c\\)\":\"☺️\",\n",
    "    u\":\\^\\)\":\"☺️\",\n",
    "    u\"=\\]\":\"☺️\",\n",
    "    u\"=\\)\":\"☺️\",\n",
    "    u\":‑D\":\"😃\",\n",
    "    u\":D\":\"😃\",\n",
    "    u\"8‑D\":\"😃\",\n",
    "    u\"8D\":\"😃\",\n",
    "    u\"X‑D\":\"😃\",\n",
    "    u\"XD\":\"😃\",\n",
    "    u\"=D\":\"😃\",\n",
    "    u\"=3\":\"😃\",\n",
    "    u\"B\\^D\":\"😃\",\n",
    "    u\":-\\)\\)\":\"😃\",\n",
    "    u\":‑\\(\":\"☹️\",\n",
    "    u\":-\\(\":\"☹️\",\n",
    "    u\":\\(\":\"☹️\",\n",
    "    u\":‑c\":\"☹️\",\n",
    "    u\":c\":\"☹️\",\n",
    "    u\":‑<\":\"☹️\",\n",
    "    u\":<\":\"☹️\",\n",
    "    u\":‑\\[\":\"☹️\",\n",
    "    u\":\\[\":\"☹️\",\n",
    "    u\":-\\|\\|\":\"☹️\",\n",
    "    u\">:\\[\":\"☹️\",\n",
    "    u\":\\{\":\"☹️\",\n",
    "    u\":@\":\"☹️\",\n",
    "    u\">:\\(\":\"☹️\",\n",
    "    u\":'‑\\(\":\"😭\",\n",
    "    u\":'\\(\":\"😭\",\n",
    "    u\":'‑\\)\":\"😃\",\n",
    "    u\":'\\)\":\"😃\",\n",
    "    u\"D‑':\":\"😨\",\n",
    "    u\"D:<\":\"😨\",\n",
    "    u\"D:\":\"😧\",\n",
    "    u\"D8\":\"😧\",\n",
    "    u\"D;\":\"😧\",\n",
    "    u\"D=\":\"😧\",\n",
    "    u\"DX\":\"😧\",\n",
    "    u\":‑O\":\"😮\",\n",
    "    u\":O\":\"😮\",\n",
    "    u\":‑o\":\"😮\",\n",
    "    u\":o\":\"😮\",\n",
    "    u\":-0\":\"😮\",\n",
    "    u\"8‑0\":\"😮\",\n",
    "    u\">:O\":\"😮\",\n",
    "    u\":-\\*\":\"😗\",\n",
    "    u\":\\*\":\"😗\",\n",
    "    u\":X\":\"😗\",\n",
    "    u\";‑\\)\":\"😉\",\n",
    "    u\";\\)\":\"😉\",\n",
    "    u\"\\*-\\)\":\"😉\",\n",
    "    u\"\\*\\)\":\"😉\",\n",
    "    u\";‑\\]\":\"😉\",\n",
    "    u\";\\]\":\"😉\",\n",
    "    u\";\\^\\)\":\"😉\",\n",
    "    u\":‑,\":\"😉\",\n",
    "    u\";D\":\"😉\",\n",
    "    u\":‑P\":\"😛\",\n",
    "    u\":P\":\"😛\",\n",
    "    u\"X‑P\":\"😛\",\n",
    "    u\"XP\":\"😛\",\n",
    "    u\":‑Þ\":\"😛\",\n",
    "    u\":Þ\":\"😛\",\n",
    "    u\":b\":\"😛\",\n",
    "    u\"d:\":\"😛\",\n",
    "    u\"=p\":\"😛\",\n",
    "    u\">:P\":\"😛\",\n",
    "    u\":‑/\":\"😕\",\n",
    "    u\":/\":\"😕\",\n",
    "    u\":-[.]\":\"😕\",\n",
    "    u\">:[(\\\\\\)]\":\"😕\",\n",
    "    u\">:/\":\"😕\",\n",
    "    u\":[(\\\\\\)]\":\"😕\",\n",
    "    u\"=/\":\"😕\",\n",
    "    u\"=[(\\\\\\)]\":\"😕\",\n",
    "    u\":L\":\"😕\",\n",
    "    u\"=L\":\"😕\",\n",
    "    u\":S\":\"😕\",\n",
    "    u\":‑\\|\":\"😐\",\n",
    "    u\":\\|\":\"😐\",\n",
    "    u\":$\":\"😳\",\n",
    "    u\":‑x\":\"🤐\",\n",
    "    u\":x\":\"🤐\",\n",
    "    u\":‑#\":\"🤐\",\n",
    "    u\":#\":\"🤐\",\n",
    "    u\":‑&\":\"🤐\",\n",
    "    u\":&\":\"🤐\",\n",
    "    u\"O:‑\\)\":\"😇\",\n",
    "    u\"O:\\)\":\"😇\",\n",
    "    u\"0:‑3\":\"😇\",\n",
    "    u\"0:3\":\"😇\",\n",
    "    u\"0:‑\\)\":\"😇\",\n",
    "    u\"0:\\)\":\"😇\",\n",
    "    u\":‑b\":\"😛\",\n",
    "    u\"0;\\^\\)\":\"😇\",\n",
    "    u\">:‑\\)\":\"😈\",\n",
    "    u\">:\\)\":\"😈\",\n",
    "    u\"\\}:‑\\)\":\"😈\",\n",
    "    u\"\\}:\\)\":\"😈\",\n",
    "    u\"3:‑\\)\":\"😈\",\n",
    "    u\"3:\\)\":\"😈\",\n",
    "    u\">;\\)\":\"😈\",\n",
    "    u\"\\|;‑\\)\":\"😎\",\n",
    "    u\"\\|‑O\":\"😏\",\n",
    "    u\":‑J\":\"😏\",\n",
    "    u\"%‑\\)\":\"😵\",\n",
    "    u\"%\\)\":\"😵\",\n",
    "    u\":-###..\":\"🤒\",\n",
    "    u\":###..\":\"🤒\",\n",
    "    u\"\\(>_<\\)\":\"😣\",\n",
    "    u\"\\(>_<\\)>\":\"😣\",\n",
    "    u\"\\(';'\\)\":\"👶\",\n",
    "    u\"\\(\\^\\^>``\":\"😓\",\n",
    "    u\"\\(\\^_\\^;\\)\":\"😓\",\n",
    "    u\"\\(-_-;\\)\":\"😓\",\n",
    "    u\"\\(~_~;\\) \\(・\\.・;\\)\":\"😓\",\n",
    "    u\"\\(-_-\\)zzz\":\"😴\",\n",
    "    u\"\\(\\^_-\\)\":\"😉\",\n",
    "    u\"\\(\\(\\+_\\+\\)\\)\":\"😕\",\n",
    "    u\"\\(\\+o\\+\\)\":\"😕\",\n",
    "    u\"\\^_\\^\":\"😃\",\n",
    "    u\"\\(\\^_\\^\\)/\":\"😃\",\n",
    "    u\"\\(\\^O\\^\\)／\":\"😃\",\n",
    "    u\"\\(\\^o\\^\\)／\":\"😃\",\n",
    "    u\"\\(__\\)\":\"🙇\",\n",
    "    u\"_\\(\\._\\.\\)_\":\"🙇\",\n",
    "    u\"<\\(_ _\\)>\":\"🙇\",\n",
    "    u\"<m\\(__\\)m>\":\"🙇\",\n",
    "    u\"m\\(__\\)m\":\"🙇\",\n",
    "    u\"m\\(_ _\\)m\":\"🙇\",\n",
    "    u\"\\('_'\\)\":\"😭\",\n",
    "    u\"\\(/_;\\)\":\"😭\",\n",
    "    u\"\\(T_T\\) \\(;_;\\)\":\"😭\",\n",
    "    u\"\\(;_;\":\"😭\",\n",
    "    u\"\\(;_:\\)\":\"😭\",\n",
    "    u\"\\(;O;\\)\":\"😭\",\n",
    "    u\"\\(:_;\\)\":\"😭\",\n",
    "    u\"\\(ToT\\)\":\"😭\",\n",
    "    u\";_;\":\"😭\",\n",
    "    u\";-;\":\"😭\",\n",
    "    u\";n;\":\"😭\",\n",
    "    u\";;\":\"😭\",\n",
    "    u\"Q\\.Q\":\"😭\",\n",
    "    u\"T\\.T\":\"😭\",\n",
    "    u\"QQ\":\"😭\",\n",
    "    u\"Q_Q\":\"😭\",\n",
    "    u\"\\(-\\.-\\)\":\"😞\",\n",
    "    u\"\\(-_-\\)\":\"😞\",\n",
    "    u\"\\(一一\\)\":\"😞\",\n",
    "    u\"\\(；一_一\\)\":\"😞\",\n",
    "    u\"\\(=_=\\)\":\"😩\",\n",
    "    u\"\\(=\\^\\·\\^=\\)\":\"😺\",\n",
    "    u\"\\(=\\^\\·\\·\\^=\\)\":\"😺\",\n",
    "    u\"=_\\^=\t\":\"😺\",\n",
    "    u\"\\(\\.\\.\\)\":\"😔\",\n",
    "    u\"\\(\\._\\.\\)\":\"😔\",\n",
    "    u\"\\(\\・\\・?\":\"😕\",\n",
    "    u\"\\(?_?\\)\":\"😕\",\n",
    "    u\">\\^_\\^<\":\"😃\",\n",
    "    u\"<\\^!\\^>\":\"😃\",\n",
    "    u\"\\^/\\^\":\"😃\",\n",
    "    u\"\\（\\*\\^_\\^\\*）\" :\"😃\",\n",
    "    u\"\\(\\^<\\^\\) \\(\\^\\.\\^\\)\":\"😃\",\n",
    "    u\"\\(^\\^\\)\":\"😃\",\n",
    "    u\"\\(\\^\\.\\^\\)\":\"😃\",\n",
    "    u\"\\(\\^_\\^\\.\\)\":\"😃\",\n",
    "    u\"\\(\\^_\\^\\)\":\"😃\",\n",
    "    u\"\\(\\^\\^\\)\":\"😃\",\n",
    "    u\"\\(\\^J\\^\\)\":\"😃\",\n",
    "    u\"\\(\\*\\^\\.\\^\\*\\)\":\"😃\",\n",
    "    u\"\\(\\^—\\^\\）\":\"😃\",\n",
    "    u\"\\(#\\^\\.\\^#\\)\":\"😃\",\n",
    "    u\"\\（\\^—\\^\\）\":\"👋\",\n",
    "    u\"\\(;_;\\)/~~~\":\"👋\",\n",
    "    u\"\\(\\^\\.\\^\\)/~~~\":\"👋\",\n",
    "    u\"\\(T_T\\)/~~~\":\"👋\",\n",
    "    u\"\\(ToT\\)/~~~\":\"👋\",\n",
    "    u\"\\(\\*\\^0\\^\\*\\)\":\"😍\",\n",
    "    u\"\\(\\*_\\*\\)\":\"😍\",\n",
    "    u\"\\(\\*_\\*;\":\"😍\",\n",
    "    u\"\\(\\+_\\+\\) \\(@_@\\)\":\"😍\",\n",
    "    u\"\\(\\*\\^\\^\\)v\":\"😂\",\n",
    "    u\"\\(\\^_\\^\\)v\":\"😂\",\n",
    "    u'\\(-\"-\\)':\"😓\",\n",
    "    u\"\\(ーー;\\)\":\"😓\",\n",
    "    u\"\\(\\^0_0\\^\\)\":\"😎\",\n",
    "    u\"\\(\\＾ｖ\\＾\\)\":\"😀\",\n",
    "    u\"\\(\\＾ｕ\\＾\\)\":\"😀\",\n",
    "    u\"\\(\\^\\)o\\(\\^\\)\":\"😀\",\n",
    "    u\"\\(\\^O\\^\\)\":\"😀\",\n",
    "    u\"\\(\\^o\\^\\)\":\"😀\",\n",
    "    u\"\\)\\^o\\^\\(\":\"😀\",\n",
    "    u\":O o_O\":\"😮\",\n",
    "    u\"o_0\":\"😮\",\n",
    "    u\"o\\.O\":\"😮\",\n",
    "    u\"\\(o\\.o\\)\":\"😮\",\n",
    "    u\"oO\":\"😮\",\n",
    "    u\"\\(\\*￣m￣\\)\":\"😠\",\n",
    "    u\":‑)\":\"☺️\",\n",
    "    u\":)\":\"☺️\",\n",
    "    u\":-]\":\"☺️\",\n",
    "    u\":]\":\"☺️\",\n",
    "    u\":-3\":\"☺️\",\n",
    "    u\":3\":\"☺️\",\n",
    "    u\":->\":\"☺️\",\n",
    "    u\":>\":\"☺️\",\n",
    "    u\"8-)\":\"☺️\",\n",
    "    u\":o)\":\"☺️\",\n",
    "    u\":-}\":\"☺️\",\n",
    "    u\":}\":\"☺️\",\n",
    "    u\":-)\":\"☺️\",\n",
    "    u\":c)\":\"☺️\",\n",
    "    u\":^)\":\"☺️\",\n",
    "    u\"=]\":\"☺️\",\n",
    "    u\"=)\":\"☺️\",\n",
    "    u\":‑D\":\"😃\",\n",
    "    u\":D\":\"😃\",\n",
    "    u\"8‑D\":\"😃\",\n",
    "    u\"8D\":\"😃\",\n",
    "    u\"X‑D\":\"😃\",\n",
    "    u\"XD\":\"😃\",\n",
    "    u\"=D\":\"😃\",\n",
    "    u\"=3\":\"😃\",\n",
    "    u\"B^D\":\"😃\",\n",
    "    u\":-))\":\"😃\",\n",
    "    u\":-(\":\"☹️\",\n",
    "    u\":‑(\":\"☹️\",\n",
    "    u\":(\":\"☹️\",\n",
    "    u\":‑c\":\"☹️\",\n",
    "    u\":c\":\"☹️\",\n",
    "    u\":‑<\":\"☹️\",\n",
    "    u\":<\":\"☹️\",\n",
    "    u\":‑[\":\"☹️\",\n",
    "    u\":[\":\"☹️\",\n",
    "    u\":-||\":\"☹️\",\n",
    "    u\">:[\":\"☹️\",\n",
    "    u\":{\":\"☹️\",\n",
    "    u\":@\":\"☹️\",\n",
    "    u\">:(\":\"☹️\",\n",
    "    u\":'‑(\":\"😭\",\n",
    "    u\":'(\":\"😭\",\n",
    "    u\":'‑)\":\"😃\",\n",
    "    u\":')\":\"😃\",\n",
    "    u\"D‑':\":\"😧\",\n",
    "    u\"D:<\":\"😨\",\n",
    "    u\"D:\":\"😧\",\n",
    "    u\"D8\":\"😧\",\n",
    "    u\"D;\":\"😧\",\n",
    "    u\"D=\":\"😧\",\n",
    "    u\"DX\":\"😧\",\n",
    "    u\":‑O\":\"😮\",\n",
    "    u\":O\":\"😮\",\n",
    "    u\":‑o\":\"😮\",\n",
    "    u\":o\":\"😮\",\n",
    "    u\":-0\":\"😮\",\n",
    "    u\"8‑0\":\"😮\",\n",
    "    u\">:O\":\"😮\",\n",
    "    u\":-*\":\"😗\",\n",
    "    u\":*\":\"😗\",\n",
    "    u\":X\":\"😗\",\n",
    "    u\";‑)\":\"😉\",\n",
    "    u\";)\":\"😉\",\n",
    "    u\"*-)\":\"😉\",\n",
    "    u\"*)\":\"😉\",\n",
    "    u\";‑]\":\"😉\",\n",
    "    u\";]\":\"😉\",\n",
    "    u\";^)\":\"😉\",\n",
    "    u\":‑,\":\"😉\",\n",
    "    u\";D\":\"😉\",\n",
    "    u\":‑P\":\"😛\",\n",
    "    u\":P\":\"😛\",\n",
    "    u\"X‑P\":\"😛\",\n",
    "    u\"XP\":\"😛\",\n",
    "    u\":‑Þ\":\"😛\",\n",
    "    u\":Þ\":\"😛\",\n",
    "    u\":b\":\"😛\",\n",
    "    u\"d:\":\"😛\",\n",
    "    u\"=p\":\"😛\",\n",
    "    u\">:P\":\"😛\",\n",
    "    u\":‑/\":\"😕\",\n",
    "    u\":/\":\"😕\",\n",
    "    u\":-[.]\":\"😕\",\n",
    "    u\">:[(\\)]\":\"😕\",\n",
    "    u\">:/\":\"😕\",\n",
    "    u\":[(\\)]\":\"😕\",\n",
    "    u\"=/\":\"😕\",\n",
    "    u\"=[(\\)]\":\"😕\",\n",
    "    u\":L\":\"😕\",\n",
    "    u\"=L\":\"😕\",\n",
    "    u\":S\":\"😕\",\n",
    "    u\":‑|\":\"😐\",\n",
    "    u\":|\":\"😐\",\n",
    "    u\":$\":\"😳\",\n",
    "    u\":‑x\":\"🤐\",\n",
    "    u\":x\":\"🤐\",\n",
    "    u\":‑#\":\"🤐\",\n",
    "    u\":#\":\"🤐\",\n",
    "    u\":‑&\":\"🤐\",\n",
    "    u\":&\":\"🤐\",\n",
    "    u\"O:‑)\":\"😇\",\n",
    "    u\"O:)\":\"😇\",\n",
    "    u\"0:‑3\":\"😇\",\n",
    "    u\"0:3\":\"😇\",\n",
    "    u\"0:‑)\":\"😇\",\n",
    "    u\"0:)\":\"😇\",\n",
    "    u\":‑b\":\"😛\",\n",
    "    u\"0;^)\":\"😇\",\n",
    "    u\">:‑)\":\"😈\",\n",
    "    u\">:)\":\"😈\",\n",
    "    u\"}:‑)\":\"😈\",\n",
    "    u\"}:)\":\"😈\",\n",
    "    u\"3:‑)\":\"😈\",\n",
    "    u\"3:)\":\"😈\",\n",
    "    u\">;)\":\"😈\",\n",
    "    u\"|;‑)\":\"😎\",\n",
    "    u\"|‑O\":\"😏\",\n",
    "    u\":‑J\":\"😏\",\n",
    "    u\"%‑)\":\"😵\",\n",
    "    u\"%)\":\"😵\",\n",
    "    u\":-###..\":\"🤒\",\n",
    "    u\":###..\":\"🤒\",\n",
    "    u\"(>_<)\":\"😣\",\n",
    "    u\"(>_<)>\":\"😣\",\n",
    "    u\"(';')\":\"Baby\",\n",
    "    u\"(^^>``\":\"😓\",\n",
    "    u\"(^_^;)\":\"😓\",\n",
    "    u\"(-_-;)\":\"😓\",\n",
    "    u\"(~_~;) (・.・;)\":\"😓\",\n",
    "    u\"(-_-)zzz\":\"😴\",\n",
    "    u\"(^_-)\":\"😉\",\n",
    "    u\"((+_+))\":\"😕\",\n",
    "    u\"(+o+)\":\"😕\",\n",
    "    u\"^_^\":\"😃\",\n",
    "    u\"(^_^)/\":\"😃\",\n",
    "    u\"(^O^)／\":\"😃\",\n",
    "    u\"(^o^)／\":\"😃\",\n",
    "    u\"(__)\":\"🙇\",\n",
    "    u\"_(._.)_\":\"🙇\",\n",
    "    u\"<(_ _)>\":\"🙇\",\n",
    "    u\"<m(__)m>\":\"🙇\",\n",
    "    u\"m(__)m\":\"🙇\",\n",
    "    u\"m(_ _)m\":\"🙇\",\n",
    "    u\"('_')\":\"😭\",\n",
    "    u\"(/_;)\":\"😭\",\n",
    "    u\"(T_T) (;_;)\":\"😭\",\n",
    "    u\"(;_;\":\"😭\",\n",
    "    u\"(;_:)\":\"😭\",\n",
    "    u\"(;O;)\":\"😭\",\n",
    "    u\"(:_;)\":\"😭\",\n",
    "    u\"(ToT)\":\"😭\",\n",
    "    u\";_;\":\"😭\",\n",
    "    u\";-;\":\"😭\",\n",
    "    u\";n;\":\"😭\",\n",
    "    u\";;\":\"😭\",\n",
    "    u\"Q.Q\":\"😭\",\n",
    "    u\"T.T\":\"😭\",\n",
    "    u\"QQ\":\"😭\",\n",
    "    u\"Q_Q\":\"😭\",\n",
    "    u\"(-.-)\":\"😞\",\n",
    "    u\"(-_-)\":\"😞\",\n",
    "    u\"(一一)\":\"😞\",\n",
    "    u\"(；一_一)\":\"😞\",\n",
    "    u\"(=_=)\":\"😩\",\n",
    "    u\"(=^·^=)\":\"😺\",\n",
    "    u\"(=^··^=)\":\"😺\",\n",
    "    u\"=_^= \":\"😺\",\n",
    "    u\"(..)\":\"😔\",\n",
    "    u\"(._.)\":\"😔\",\n",
    "    u\"(・・?\":\"😕\",\n",
    "    u\"(?_?)\":\"😕\",\n",
    "    u\">^_^<\":\"😃\",\n",
    "    u\"<^!^>\":\"😃\",\n",
    "    u\"^/^\":\"😃\",\n",
    "    u\"（*^_^*）\" :\"😃\",\n",
    "    u\"(^<^) (^.^)\":\"😃\",\n",
    "    u\"(^^)\":\"😃\",\n",
    "    u\"(^.^)\":\"😃\",\n",
    "    u\"(^_^.)\":\"😃\",\n",
    "    u\"(^_^)\":\"😃\",\n",
    "    u\"(^^)\":\"😃\",\n",
    "    u\"(^J^)\":\"😃\",\n",
    "    u\"(*^.^*)\":\"😃\",\n",
    "    u\"(^—^）\":\"😃\",\n",
    "    u\"(#^.^#)\":\"😃\",\n",
    "    u\"（^—^）\":\"👋\",\n",
    "    u\"(;_;)/~~~\":\"👋\",\n",
    "    u\"(^.^)/~~~\":\"👋\",\n",
    "    u\"(-_-)/~~~ ($··)/~~~\":\"👋\",\n",
    "    u\"(T_T)/~~~\":\"👋\",\n",
    "    u\"(ToT)/~~~\":\"👋\",\n",
    "    u\"(*^0^*)\":\"😍\",\n",
    "    u\"(*_*)\":\"😍\",\n",
    "    u\"(*_*;\":\"😍\",\n",
    "    u\"(+_+) (@_@)\":\"😍\",\n",
    "    u\"(*^^)v\":\"😂\",\n",
    "    u\"(^_^)v\":\"😂\",\n",
    "    u'(-\"-)':\"😓\",\n",
    "    u\"(ーー;)\":\"😓\",\n",
    "    u\"(^0_0^)\":\"😎\",\n",
    "    u\"(＾ｖ＾)\":\"😀\",\n",
    "    u\"(＾ｕ＾)\":\"😀\",\n",
    "    u\"(^)o(^)\":\"😀\",\n",
    "    u\"(^O^)\":\"😀\", u\"(^o^)\":\"😀\", u\")^o^(\":\"😀\", u\":O o_O\":\"😮\",\n",
    "    u\"o_0\":\"😮\", u\"o.O\":\"😮\", u\"(o.o)\":\"😮\", u\"oO\":\"😮\",\n",
    "}\n",
    "\n",
    "\n",
    "def str2emoji(text):\n",
    "\n",
    "    for pos,ej in enumerate(text):\n",
    "        if ej in emojis:\n",
    "            text[pos]=emojis[ej]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from unidecode import unidecode\n",
    "\n",
    "def norm_text(text):\n",
    "    text = re.sub(r\"\\\\u2019\", \"'\", text)\n",
    "    text = re.sub(r\"\\\\u002c\", \",\", text)\n",
    "    text = ' '.join(str2emoji(unidecode(text).lower().split()))\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\" can\\'t\", \" cannot\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'s\", \"\", text)\n",
    "    text = re.sub(r\"\\'n\", \"\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    text = re.sub(r\"@\\w+\", r' ',text)\n",
    "    text = re.sub(r\"#\\w+\", r' ',text)\n",
    "    text = re.sub(r\"[.]+\",\" \",text)\n",
    "    # Remove stopwords and punctuations\n",
    "    text = ' '.join([ word for word in text.split() if (word not in stopwords) and (word not in string.punctuation ) ])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def load_embeddings_matrix(filename, word_index, embedding_dim=300):\n",
    "    nb_words=len(word_index)+1\n",
    "    \n",
    "    embedding_matrix = np.zeros((nb_words, embedding_dim))\n",
    "    word2vec = KeyedVectors.load_word2vec_format(filename, binary=True)\n",
    "    \n",
    "    # Unknown vocabulary will be replaced as random vector\n",
    "    oov = [np.random.rand(embedding_dim) * 2.0 - 1.0]\n",
    "    oov = oov / np.linalg.norm(oov)\n",
    "    \n",
    "    # Create the resulting embedding_matrix\n",
    "    for word, i in word_index.items():\n",
    "        if word in word2vec.vocab:\n",
    "            embedding_matrix[i] = word2vec.word_vec(word)\n",
    "        else:\n",
    "            embedding_matrix[i] = oov\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>turn1</th>\n",
       "      <th>turn2</th>\n",
       "      <th>turn3</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Then dont ask me</td>\n",
       "      <td>YOURE A GUY NOT AS IF YOU WOULD UNDERSTAND</td>\n",
       "      <td>IM NOT A GUY FUCK OFF</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Mixed things  such as??</td>\n",
       "      <td>the things you do.</td>\n",
       "      <td>Have you seen minions??</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Today I'm very happy</td>\n",
       "      <td>and I'm happy for you ❤</td>\n",
       "      <td>I will be marry</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Woah bring me some</td>\n",
       "      <td>left it there oops</td>\n",
       "      <td>Brb</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>it is thooooo</td>\n",
       "      <td>I said soon master.</td>\n",
       "      <td>he is pressuring me</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750</th>\n",
       "      <td>2750</td>\n",
       "      <td>U are my book</td>\n",
       "      <td>book for what? ugliness? THANK YOU</td>\n",
       "      <td>U like ur self</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2751</th>\n",
       "      <td>2751</td>\n",
       "      <td>I'll be crying</td>\n",
       "      <td>You just want to make ppl cry:P</td>\n",
       "      <td>ppl</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2752</th>\n",
       "      <td>2752</td>\n",
       "      <td>Thanks for sending</td>\n",
       "      <td>hahaha you're welcome! 😤😤</td>\n",
       "      <td>Why are u not sending</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2753</th>\n",
       "      <td>2753</td>\n",
       "      <td>Write it</td>\n",
       "      <td>Mr. F</td>\n",
       "      <td>U understand me?</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2754</th>\n",
       "      <td>2754</td>\n",
       "      <td>Yes</td>\n",
       "      <td>okay I'll give you a ticket</td>\n",
       "      <td>Ohk</td>\n",
       "      <td>others</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2755 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                    turn1  \\\n",
       "0        0         Then dont ask me   \n",
       "1        1  Mixed things  such as??   \n",
       "2        2     Today I'm very happy   \n",
       "3        3       Woah bring me some   \n",
       "4        4            it is thooooo   \n",
       "...    ...                      ...   \n",
       "2750  2750            U are my book   \n",
       "2751  2751           I'll be crying   \n",
       "2752  2752       Thanks for sending   \n",
       "2753  2753                 Write it   \n",
       "2754  2754                      Yes   \n",
       "\n",
       "                                           turn2                    turn3  \\\n",
       "0     YOURE A GUY NOT AS IF YOU WOULD UNDERSTAND    IM NOT A GUY FUCK OFF   \n",
       "1                             the things you do.  Have you seen minions??   \n",
       "2                        and I'm happy for you ❤          I will be marry   \n",
       "3                             left it there oops                      Brb   \n",
       "4                            I said soon master.      he is pressuring me   \n",
       "...                                          ...                      ...   \n",
       "2750          book for what? ugliness? THANK YOU           U like ur self   \n",
       "2751             You just want to make ppl cry:P                      ppl   \n",
       "2752                   hahaha you're welcome! 😤😤    Why are u not sending   \n",
       "2753                                       Mr. F         U understand me?   \n",
       "2754                 okay I'll give you a ticket                      Ohk   \n",
       "\n",
       "       label  \n",
       "0      angry  \n",
       "1     others  \n",
       "2      happy  \n",
       "3     others  \n",
       "4     others  \n",
       "...      ...  \n",
       "2750  others  \n",
       "2751  others  \n",
       "2752  others  \n",
       "2753  others  \n",
       "2754  others  \n",
       "\n",
       "[2755 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"train.txt\", sep=\"\\t\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace empty columns with a balise to prevent mistakes\n",
    "\n",
    "CLASSES = [\"angry\", \"happy\", \"sad\", \"others\"]\n",
    "\n",
    "def transform_text(data):    \n",
    "    data = map_column(data, [\"turn1\", \"turn2\", \"turn3\"], lambda text:  \" <b> \" + text)\n",
    "    data = map_column(data, [\"turn1\", \"turn2\", \"turn3\"], norm_text)\n",
    "    data = map_column(data, [\"label\"], lambda label: CLASSES.index(label))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = transform_text(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<b>': 1,\n",
       " 'u': 2,\n",
       " 'know': 3,\n",
       " 'yes': 4,\n",
       " 'ok': 5,\n",
       " 'like': 6,\n",
       " 'tell': 7,\n",
       " 'good': 8,\n",
       " 'love': 9,\n",
       " 'want': 10,\n",
       " 'r': 11,\n",
       " '☺️': 12,\n",
       " 'one': 13,\n",
       " 'say': 14,\n",
       " 'go': 15,\n",
       " 'think': 16,\n",
       " 'ur': 17,\n",
       " 'get': 18,\n",
       " 'send': 19,\n",
       " 'what?': 20,\n",
       " 'see': 21,\n",
       " 'yeah': 22,\n",
       " 'something': 23,\n",
       " 'oh': 24,\n",
       " 'thank': 25,\n",
       " 'nothing': 26,\n",
       " 'talk': 27,\n",
       " 'dont': 28,\n",
       " 'would': 29,\n",
       " 'give': 30,\n",
       " 'please': 31,\n",
       " 'really': 32,\n",
       " 'time': 33,\n",
       " 'let': 34,\n",
       " 'sorry': 35,\n",
       " 'okay': 36,\n",
       " 'you?': 37,\n",
       " 'funny': 38,\n",
       " 'understand': 39,\n",
       " 'well': 40,\n",
       " 'ask': 41,\n",
       " 'friend': 42,\n",
       " 'thanks': 43,\n",
       " 'haha': 44,\n",
       " 'cool': 45,\n",
       " 'much': 46,\n",
       " 'going': 47,\n",
       " 'mean': 48,\n",
       " 'said': 49,\n",
       " 'life': 50,\n",
       " 'cannot': 51,\n",
       " ':p': 52,\n",
       " 'come': 53,\n",
       " 'sleep': 54,\n",
       " 'name': 55,\n",
       " 'got': 56,\n",
       " 'also': 57,\n",
       " 'need': 58,\n",
       " 'im': 59,\n",
       " 'number': 60,\n",
       " 'make': 61,\n",
       " 'nice': 62,\n",
       " 'lol': 63,\n",
       " 'bye': 64,\n",
       " 'welcome': 65,\n",
       " 'pic': 66,\n",
       " ':d': 67,\n",
       " 'still': 68,\n",
       " 'hate': 69,\n",
       " 'day': 70,\n",
       " 'bad': 71,\n",
       " 'better': 72,\n",
       " 'fuck': 73,\n",
       " 'happy': 74,\n",
       " 'best': 75,\n",
       " 'night': 76,\n",
       " 'always': 77,\n",
       " 'movie': 78,\n",
       " 'hey': 79,\n",
       " 'never': 80,\n",
       " 'work': 81,\n",
       " 'great': 82,\n",
       " 'me?': 83,\n",
       " 'right': 84,\n",
       " 'call': 85,\n",
       " 'sure': 86,\n",
       " 'meet': 87,\n",
       " 'question': 88,\n",
       " 'fine': 89,\n",
       " 'hmm': 90,\n",
       " 'busy': 91,\n",
       " 'answer': 92,\n",
       " 'wish': 93,\n",
       " 'things': 94,\n",
       " 'show': 95,\n",
       " 'talking': 96,\n",
       " 'anything': 97,\n",
       " 'today': 98,\n",
       " 'back': 99,\n",
       " 'wanna': 100,\n",
       " 'wow': 101,\n",
       " 'girl': 102,\n",
       " 'people': 103,\n",
       " 'thing': 104,\n",
       " 'angry': 105,\n",
       " 'watch': 106,\n",
       " '??': 107,\n",
       " 'already': 108,\n",
       " 'asking': 109,\n",
       " 'even': 110,\n",
       " 'play': 111,\n",
       " 'chat': 112,\n",
       " 'someone': 113,\n",
       " 'take': 114,\n",
       " 'baby': 115,\n",
       " 'english': 116,\n",
       " 'many': 117,\n",
       " 'fun': 118,\n",
       " 'way': 119,\n",
       " 'help': 120,\n",
       " 'first': 121,\n",
       " 'ai': 122,\n",
       " 'song': 123,\n",
       " 'feel': 124,\n",
       " 'why?': 125,\n",
       " 'sad': 126,\n",
       " 'game': 127,\n",
       " 'girlfriend': 128,\n",
       " 'believe': 129,\n",
       " 'else': 130,\n",
       " 'phone': 131,\n",
       " 'could': 132,\n",
       " 'ohh': 133,\n",
       " 'stupid': 134,\n",
       " 'thats': 135,\n",
       " 'thought': 136,\n",
       " 'stop': 137,\n",
       " 'ya': 138,\n",
       " 'you!': 139,\n",
       " 'course': 140,\n",
       " 'person': 141,\n",
       " 'yet': 142,\n",
       " 'done': 143,\n",
       " 'nope': 144,\n",
       " 'guess': 145,\n",
       " '😉': 146,\n",
       " '☹️': 147,\n",
       " 'heart': 148,\n",
       " 'waiting': 149,\n",
       " 'yup': 150,\n",
       " 'means': 151,\n",
       " 'speak': 152,\n",
       " 'n': 153,\n",
       " 'sex': 154,\n",
       " 'may': 155,\n",
       " 'man': 156,\n",
       " 'getting': 157,\n",
       " 'look': 158,\n",
       " 'plz': 159,\n",
       " 'home': 160,\n",
       " 'it?': 161,\n",
       " 'hell': 162,\n",
       " 'sweet': 163,\n",
       " '2': 164,\n",
       " 'place': 165,\n",
       " 'pls': 166,\n",
       " 'lot': 167,\n",
       " 'na': 168,\n",
       " 'lost': 169,\n",
       " 'saying': 170,\n",
       " 'single': 171,\n",
       " 'actually': 172,\n",
       " 'dear': 173,\n",
       " 'new': 174,\n",
       " 'language': 175,\n",
       " 'find': 176,\n",
       " 'whats': 177,\n",
       " 'us': 178,\n",
       " 'wrong': 179,\n",
       " 'feeling': 180,\n",
       " 'k': 181,\n",
       " 'next': 182,\n",
       " 'leave': 183,\n",
       " 'job': 184,\n",
       " 'ever': 185,\n",
       " 'use': 186,\n",
       " 'real': 187,\n",
       " 'wait': 188,\n",
       " 'hahaha': 189,\n",
       " 'hope': 190,\n",
       " 'check': 191,\n",
       " 'city': 192,\n",
       " 'fast': 193,\n",
       " 'human': 194,\n",
       " 'face': 195,\n",
       " 'watching': 196,\n",
       " 'you,': 197,\n",
       " 'last': 198,\n",
       " 'keep': 199,\n",
       " 'live': 200,\n",
       " 'told': 201,\n",
       " 'seen': 202,\n",
       " 'try': 203,\n",
       " 'ready': 204,\n",
       " 'though': 205,\n",
       " 'tomorrow': 206,\n",
       " 'join': 207,\n",
       " 'mind': 208,\n",
       " 'kiss': 209,\n",
       " 'mine': 210,\n",
       " 'beautiful': 211,\n",
       " 'wht': 212,\n",
       " 'yes,': 213,\n",
       " 'nd': 214,\n",
       " 'yea': 215,\n",
       " 'favorite': 216,\n",
       " 'soon': 217,\n",
       " 'video': 218,\n",
       " 'reply': 219,\n",
       " 'office': 220,\n",
       " 'everything': 221,\n",
       " 'food': 222,\n",
       " 'remember': 223,\n",
       " 'making': 224,\n",
       " 'artificial': 225,\n",
       " 'learn': 226,\n",
       " 'world': 227,\n",
       " 'free': 228,\n",
       " 'two': 229,\n",
       " 'sent': 230,\n",
       " 'rude': 231,\n",
       " 'pretty': 232,\n",
       " 'bored': 233,\n",
       " 'care': 234,\n",
       " 'that?': 235,\n",
       " 'seriously': 236,\n",
       " 'photo': 237,\n",
       " 'made': 238,\n",
       " 'maybe': 239,\n",
       " 'gonna': 240,\n",
       " 'link': 241,\n",
       " 'b': 242,\n",
       " 'boy': 243,\n",
       " 'asked': 244,\n",
       " 'everyone': 245,\n",
       " 'start': 246,\n",
       " 'boyfriend': 247,\n",
       " 'read': 248,\n",
       " 'every': 249,\n",
       " '1': 250,\n",
       " '<3': 251,\n",
       " 'not?': 252,\n",
       " 'coming': 253,\n",
       " 'guy': 254,\n",
       " 'wo': 255,\n",
       " 'friends': 256,\n",
       " 'house': 257,\n",
       " 'pics': 258,\n",
       " 'india': 259,\n",
       " 'yaar': 260,\n",
       " 'another': 261,\n",
       " 'days': 262,\n",
       " 'listen': 263,\n",
       " 'morning': 264,\n",
       " 'year': 265,\n",
       " 'do?': 266,\n",
       " 'trying': 267,\n",
       " 'anyone': 268,\n",
       " 'picture': 269,\n",
       " 'shall': 270,\n",
       " 'right?': 271,\n",
       " 'cute': 272,\n",
       " 'wanted': 273,\n",
       " 'text': 274,\n",
       " 'sometimes': 275,\n",
       " 'long': 276,\n",
       " 'so?': 277,\n",
       " 'awesome': 278,\n",
       " 'intelligence': 279,\n",
       " 'kind': 280,\n",
       " 'sing': 281,\n",
       " 'alone': 282,\n",
       " 'money': 283,\n",
       " 'dance': 284,\n",
       " 'hear': 285,\n",
       " 'pizza': 286,\n",
       " 'mobile': 287,\n",
       " 'old': 288,\n",
       " 'called': 289,\n",
       " 'working': 290,\n",
       " 'miss': 291,\n",
       " 'second': 292,\n",
       " 'joke': 293,\n",
       " 'must': 294,\n",
       " 'later': 295,\n",
       " 'different': 296,\n",
       " '😕': 297,\n",
       " 'iam': 298,\n",
       " 'boring': 299,\n",
       " 'dinner': 300,\n",
       " 'makes': 301,\n",
       " 'none': 302,\n",
       " 'shut': 303,\n",
       " 'know?': 304,\n",
       " 'interesting': 305,\n",
       " 'reason': 306,\n",
       " 'list': 307,\n",
       " 'type': 308,\n",
       " 'lie': 309,\n",
       " 'wt': 310,\n",
       " 'serious': 311,\n",
       " 'watched': 312,\n",
       " 'eat': 313,\n",
       " 'online': 314,\n",
       " 'point': 315,\n",
       " 'cold': 316,\n",
       " 'beach': 317,\n",
       " 'share': 318,\n",
       " 'looks': 319,\n",
       " 'suggest': 320,\n",
       " 'date': 321,\n",
       " 'word': 322,\n",
       " 'no,': 323,\n",
       " 'ok~~ay~~': 324,\n",
       " '😃': 325,\n",
       " '!!': 326,\n",
       " 'fight': 327,\n",
       " 'relationship': 328,\n",
       " 'self': 329,\n",
       " 'ohhh': 330,\n",
       " 'sense': 331,\n",
       " 'interested': 332,\n",
       " 'confused': 333,\n",
       " 'didnt': 334,\n",
       " 'away': 335,\n",
       " 'lunch': 336,\n",
       " 'gf': 337,\n",
       " 'true': 338,\n",
       " 'soul': 339,\n",
       " 'birthday': 340,\n",
       " '3': 341,\n",
       " 'chatting': 342,\n",
       " 'idea': 343,\n",
       " 'mad': 344,\n",
       " 'hours': 345,\n",
       " 'series': 346,\n",
       " 'thinking': 347,\n",
       " 'lets': 348,\n",
       " 'dude': 349,\n",
       " 'ill': 350,\n",
       " 'three': 351,\n",
       " 'worry': 352,\n",
       " 'story': 353,\n",
       " 'big': 354,\n",
       " 'weird': 355,\n",
       " 'back!!': 356,\n",
       " 'change': 357,\n",
       " 'quite': 358,\n",
       " 'anymore': 359,\n",
       " 'study': 360,\n",
       " 'buddy': 361,\n",
       " 'babe': 362,\n",
       " 'tv': 363,\n",
       " 'movies': 364,\n",
       " 'yr': 365,\n",
       " 'amazing': 366,\n",
       " 'yeah,': 367,\n",
       " 'full': 368,\n",
       " 'stay': 369,\n",
       " 'married': 370,\n",
       " 'sleeping': 371,\n",
       " 'problem': 372,\n",
       " 'u?': 373,\n",
       " 'cant': 374,\n",
       " 'favourite': 375,\n",
       " 'valentine': 376,\n",
       " 'marry': 377,\n",
       " 'either': 378,\n",
       " 'id': 379,\n",
       " 'bro': 380,\n",
       " 'girls': 381,\n",
       " 'clearly': 382,\n",
       " 'belong': 383,\n",
       " 'dnt': 384,\n",
       " 'kidding': 385,\n",
       " 'exam': 386,\n",
       " 'exactly': 387,\n",
       " 'google': 388,\n",
       " 'yesterday': 389,\n",
       " 'hello': 390,\n",
       " 'engineering': 391,\n",
       " 'what??': 392,\n",
       " 'xd': 393,\n",
       " 'up?': 394,\n",
       " 'liked': 395,\n",
       " 'songs': 396,\n",
       " 'nobody': 397,\n",
       " 'clear': 398,\n",
       " 'special': 399,\n",
       " 'felt': 400,\n",
       " 'pictures': 401,\n",
       " 'questions': 402,\n",
       " 'family': 403,\n",
       " 'knw': 404,\n",
       " 'school': 405,\n",
       " 'future': 406,\n",
       " 'abt': 407,\n",
       " 'times': 408,\n",
       " 'looking': 409,\n",
       " 'month': 410,\n",
       " 'dream': 411,\n",
       " 'telling': 412,\n",
       " 'feelings': 413,\n",
       " 'topic': 414,\n",
       " 'purple': 415,\n",
       " 'coz': 416,\n",
       " 'teach': 417,\n",
       " 'probably': 418,\n",
       " 'to?': 419,\n",
       " 'gone': 420,\n",
       " 'alright': 421,\n",
       " 'wat': 422,\n",
       " 'meaning': 423,\n",
       " 'bt': 424,\n",
       " 'idiot': 425,\n",
       " 'dumb': 426,\n",
       " 'saw': 427,\n",
       " 'forget': 428,\n",
       " 'party': 429,\n",
       " 'rule': 430,\n",
       " 'sry': 431,\n",
       " 'book': 432,\n",
       " 'uh': 433,\n",
       " 'loves': 434,\n",
       " 'gud': 435,\n",
       " 'together': 436,\n",
       " 'stuff': 437,\n",
       " 'personal': 438,\n",
       " 'nah': 439,\n",
       " 'used': 440,\n",
       " 'app': 441,\n",
       " 'hot': 442,\n",
       " 'exams': 443,\n",
       " 'really?': 444,\n",
       " 'conversation': 445,\n",
       " 'cricket': 446,\n",
       " 'thanks!': 447,\n",
       " 'music': 448,\n",
       " 'tired': 449,\n",
       " 'forever': 450,\n",
       " 'huh': 451,\n",
       " 'yep': 452,\n",
       " 'matter': 453,\n",
       " 'mate': 454,\n",
       " 'opinion': 455,\n",
       " 'women': 456,\n",
       " 'explain': 457,\n",
       " 'anywhere': 458,\n",
       " 'x': 459,\n",
       " 'supposed': 460,\n",
       " 'cuz': 461,\n",
       " 'it!': 462,\n",
       " 'glad': 463,\n",
       " 'ppl': 464,\n",
       " 'whatever': 465,\n",
       " 'broken': 466,\n",
       " 'lonely': 467,\n",
       " 'weekend': 468,\n",
       " 'buy': 469,\n",
       " 'college': 470,\n",
       " 'address': 471,\n",
       " 'scary': 472,\n",
       " 'vacation': 473,\n",
       " 'problems': 474,\n",
       " 'mood': 475,\n",
       " 'g': 476,\n",
       " 'chocolate': 477,\n",
       " 'improve': 478,\n",
       " '4': 479,\n",
       " 'bollywood': 480,\n",
       " 'bf': 481,\n",
       " 'scared': 482,\n",
       " 'bore': 483,\n",
       " 'surprise': 484,\n",
       " '😭': 485,\n",
       " 'nw': 486,\n",
       " 'possible': 487,\n",
       " 'happen': 488,\n",
       " 'now?': 489,\n",
       " 'father': 490,\n",
       " 'don&apos;t': 491,\n",
       " 'btw': 492,\n",
       " 'meant': 493,\n",
       " 'smart': 494,\n",
       " 'uu': 495,\n",
       " 'near': 496,\n",
       " 'oops': 497,\n",
       " 'bit': 498,\n",
       " 'guys': 499,\n",
       " 'random': 500,\n",
       " 'block': 501,\n",
       " 'gave': 502,\n",
       " 'nt': 503,\n",
       " 'hard': 504,\n",
       " 'plan': 505,\n",
       " 'sitting': 506,\n",
       " '5': 507,\n",
       " '8': 508,\n",
       " 'jokes': 509,\n",
       " 'write': 510,\n",
       " 'cry': 511,\n",
       " 'robot': 512,\n",
       " 'so,': 513,\n",
       " 'fake': 514,\n",
       " 'rest': 515,\n",
       " 'disappointed': 516,\n",
       " 'about?': 517,\n",
       " 'heard': 518,\n",
       " 'mean?': 519,\n",
       " 'happened': 520,\n",
       " 'memories': 521,\n",
       " 'far': 522,\n",
       " 'culture': 523,\n",
       " 'around': 524,\n",
       " 'hi': 525,\n",
       " 'says': 526,\n",
       " 'top': 527,\n",
       " 'mail': 528,\n",
       " 'software': 529,\n",
       " 'pain': 530,\n",
       " 'cause': 531,\n",
       " 'definitely': 532,\n",
       " 'doing?': 533,\n",
       " 'well,': 534,\n",
       " 'vote': 535,\n",
       " 'pay': 536,\n",
       " 'studying': 537,\n",
       " 'welcome!': 538,\n",
       " 'suck': 539,\n",
       " 'depressed': 540,\n",
       " 'sick': 541,\n",
       " 'pleasure': 542,\n",
       " 'dead': 543,\n",
       " 'name?': 544,\n",
       " 'finished': 545,\n",
       " 'broke': 546,\n",
       " 'program': 547,\n",
       " 'like?': 548,\n",
       " 'secret': 549,\n",
       " 'match': 550,\n",
       " 'yess': 551,\n",
       " 'store': 552,\n",
       " 'pc': 553,\n",
       " 'hoo': 554,\n",
       " 'break': 555,\n",
       " 'yaa': 556,\n",
       " 'location': 557,\n",
       " 'fav': 558,\n",
       " 'memory': 559,\n",
       " 'answers': 560,\n",
       " 'crazy': 561,\n",
       " 'saturday': 562,\n",
       " 'kinda': 563,\n",
       " 'missing': 564,\n",
       " 'started': 565,\n",
       " 'humans': 566,\n",
       " 'cook': 567,\n",
       " 'time?': 568,\n",
       " 'country': 569,\n",
       " 'somewhere': 570,\n",
       " 'texting': 571,\n",
       " 'choose': 572,\n",
       " 'came': 573,\n",
       " 'giving': 574,\n",
       " 'anytime': 575,\n",
       " 'damn': 576,\n",
       " 'little': 577,\n",
       " 'romantic': 578,\n",
       " 'walk': 579,\n",
       " 'message': 580,\n",
       " 'years': 581,\n",
       " 'dp': 582,\n",
       " 'fantastic': 583,\n",
       " 'red': 584,\n",
       " '???': 585,\n",
       " 'news': 586,\n",
       " 'waste': 587,\n",
       " 'whatsapp': 588,\n",
       " 'normal': 589,\n",
       " 'ohk': 590,\n",
       " 'star': 591,\n",
       " 'seem': 592,\n",
       " 'darling': 593,\n",
       " 'microsoft': 594,\n",
       " 'f': 595,\n",
       " 'company': 596,\n",
       " 'living': 597,\n",
       " 'choice': 598,\n",
       " 'marriage': 599,\n",
       " 'okey': 600,\n",
       " 'mm': 601,\n",
       " 'smile': 602,\n",
       " '[?]': 603,\n",
       " 'high': 604,\n",
       " 'fan': 605,\n",
       " 'actor': 606,\n",
       " 'class': 607,\n",
       " 'who?': 608,\n",
       " '9': 609,\n",
       " 'luv': 610,\n",
       " 'dn': 611,\n",
       " 'totally': 612,\n",
       " 'youre': 613,\n",
       " 'computer': 614,\n",
       " 'sending': 615,\n",
       " 'happened?': 616,\n",
       " 'knows': 617,\n",
       " 'behind': 618,\n",
       " 'great,': 619,\n",
       " 'agree': 620,\n",
       " 'truth': 621,\n",
       " 'might': 622,\n",
       " '(:': 623,\n",
       " 'goin': 624,\n",
       " 'do!': 625,\n",
       " '!!!': 626,\n",
       " 'ones': 627,\n",
       " 'me!': 628,\n",
       " 'exception': 629,\n",
       " 'happening': 630,\n",
       " 'photos': 631,\n",
       " 'wear': 632,\n",
       " 'winter': 633,\n",
       " 'correct': 634,\n",
       " 'perfect': 635,\n",
       " 'sexy': 636,\n",
       " 'bitch': 637,\n",
       " 'kill': 638,\n",
       " 'from?': 639,\n",
       " 'seeing': 640,\n",
       " 'plans': 641,\n",
       " 'using': 642,\n",
       " 'prince': 643,\n",
       " 'replying': 644,\n",
       " 'missed': 645,\n",
       " 'strong': 646,\n",
       " 'refer': 647,\n",
       " 'hmmm': 648,\n",
       " 'chicken': 649,\n",
       " 'cross': 650,\n",
       " 'yo': 651,\n",
       " 'zombie': 652,\n",
       " 'created': 653,\n",
       " 'typing': 654,\n",
       " 'town': 655,\n",
       " 'kno': 656,\n",
       " 'today?': 657,\n",
       " 'silly': 658,\n",
       " 'car': 659,\n",
       " 'march': 660,\n",
       " 'gotta': 661,\n",
       " 'c': 662,\n",
       " 'recently': 663,\n",
       " 'trust': 664,\n",
       " 'mee': 665,\n",
       " 'simple': 666,\n",
       " 'lots': 667,\n",
       " 'impress': 668,\n",
       " 'tomato': 669,\n",
       " 'wtf': 670,\n",
       " 'bot': 671,\n",
       " '24': 672,\n",
       " 'allowed': 673,\n",
       " 'shy': 674,\n",
       " 'alot': 675,\n",
       " 'math': 676,\n",
       " 'animal': 677,\n",
       " 'basically': 678,\n",
       " 'ignore': 679,\n",
       " 'bed': 680,\n",
       " 'met': 681,\n",
       " 'mom': 682,\n",
       " 'brother': 683,\n",
       " 'sister': 684,\n",
       " 'wife': 685,\n",
       " 'question?': 686,\n",
       " 'movie?': 687,\n",
       " 'comedy': 688,\n",
       " 'aww': 689,\n",
       " 'one?': 690,\n",
       " 'mr': 691,\n",
       " 'film': 692,\n",
       " 'data': 693,\n",
       " 'planning': 694,\n",
       " 'yeahh': 695,\n",
       " 'aint': 696,\n",
       " 'films': 697,\n",
       " 'whom?': 698,\n",
       " 'interest': 699,\n",
       " 'dint': 700,\n",
       " 'rather': 701,\n",
       " 'know,': 702,\n",
       " 'small': 703,\n",
       " 'whole': 704,\n",
       " '????': 705,\n",
       " 'books': 706,\n",
       " 'win': 707,\n",
       " 'left': 708,\n",
       " '2nd': 709,\n",
       " 'sky': 710,\n",
       " 'snow': 711,\n",
       " 'sharing': 712,\n",
       " 'sleepy': 713,\n",
       " 'fun?': 714,\n",
       " 'ab': 715,\n",
       " 'rn': 716,\n",
       " 'creators': 717,\n",
       " 'friendly': 718,\n",
       " 'hair': 719,\n",
       " 'leaving': 720,\n",
       " 'chuck': 721,\n",
       " '10': 722,\n",
       " 'moment': 723,\n",
       " 'dad': 724,\n",
       " 'daily': 725,\n",
       " 'shows': 726,\n",
       " 'frnd': 727,\n",
       " 'week': 728,\n",
       " 'tym': 729,\n",
       " 'pass': 730,\n",
       " 'visit': 731,\n",
       " 'oh!': 732,\n",
       " 'woman': 733,\n",
       " 'luck': 734,\n",
       " 'breaking': 735,\n",
       " 'anime': 736,\n",
       " 'save': 737,\n",
       " 'mi': 738,\n",
       " '1st': 739,\n",
       " 'developer': 740,\n",
       " 'something?': 741,\n",
       " 'playing': 742,\n",
       " 'chess': 743,\n",
       " 'easy': 744,\n",
       " 'parents': 745,\n",
       " 'open': 746,\n",
       " 'indian': 747,\n",
       " 'forgot': 748,\n",
       " 'languages': 749,\n",
       " 'gibberish': 750,\n",
       " 'angry?': 751,\n",
       " 'enjoy': 752,\n",
       " 'dreams': 753,\n",
       " 'come?': 754,\n",
       " '😗': 755,\n",
       " 'version': 756,\n",
       " 'part': 757,\n",
       " 'afternoon': 758,\n",
       " 'able': 759,\n",
       " 'related': 760,\n",
       " 'press': 761,\n",
       " 'early': 762,\n",
       " 'intelligent': 763,\n",
       " 'sarcasm': 764,\n",
       " 'laugh': 765,\n",
       " 'hahah': 766,\n",
       " 'follow': 767,\n",
       " 'acting': 768,\n",
       " 'pinch': 769,\n",
       " 'not,': 770,\n",
       " 'least': 771,\n",
       " 'age': 772,\n",
       " 'x-d': 773,\n",
       " 'depends': 774,\n",
       " 'indeed': 775,\n",
       " 'late': 776,\n",
       " 'hello!': 777,\n",
       " 'minute': 778,\n",
       " 'too!': 779,\n",
       " ';-)': 780,\n",
       " 'day?': 781,\n",
       " 'dunno': 782,\n",
       " 'pink': 783,\n",
       " 'now,': 784,\n",
       " 'how??': 785,\n",
       " 'fall': 786,\n",
       " 'sorry,': 787,\n",
       " 'regret': 788,\n",
       " 'sort': 789,\n",
       " 'handle': 790,\n",
       " 'half': 791,\n",
       " 'yay': 792,\n",
       " 'given': 793,\n",
       " 'um': 794,\n",
       " 'ticket': 795,\n",
       " 'took': 796,\n",
       " '7': 797,\n",
       " 'there!': 798,\n",
       " 'ji': 799,\n",
       " 'gets': 800,\n",
       " 'ask?': 801,\n",
       " 'hm': 802,\n",
       " 'ok,': 803,\n",
       " '😮': 804,\n",
       " 'blocked': 805,\n",
       " 'monday': 806,\n",
       " 'end': 807,\n",
       " 'dare': 808,\n",
       " 'good,': 809,\n",
       " 'per': 810,\n",
       " 'turn': 811,\n",
       " 'life?': 812,\n",
       " 'too,': 813,\n",
       " 'woke': 814,\n",
       " 'yah': 815,\n",
       " 'no!': 816,\n",
       " 'sounds': 817,\n",
       " '6': 818,\n",
       " 'except': 819,\n",
       " 'help?': 820,\n",
       " 'be?': 821,\n",
       " 'dog': 822,\n",
       " 'fucking': 823,\n",
       " 'bring': 824,\n",
       " 'wont': 825,\n",
       " 'tht': 826,\n",
       " 'sports': 827,\n",
       " 'l': 828,\n",
       " 'kk': 829,\n",
       " 'dng': 830,\n",
       " 'pleased': 831,\n",
       " 'gym': 832,\n",
       " 'skydiving': 833,\n",
       " 'somebody': 834,\n",
       " 'expected': 835,\n",
       " 'mean,': 836,\n",
       " 'else?': 837,\n",
       " 'lovely': 838,\n",
       " 'ho': 839,\n",
       " 'front': 840,\n",
       " 'ahh': 841,\n",
       " 'episode': 842,\n",
       " 'meeting': 843,\n",
       " 'personality': 844,\n",
       " 'min': 845,\n",
       " 'add': 846,\n",
       " 'calling': 847,\n",
       " 'speed': 848,\n",
       " 'crush': 849,\n",
       " 'apple': 850,\n",
       " 'drink': 851,\n",
       " 'bear': 852,\n",
       " 'birth': 853,\n",
       " 'heck': 854,\n",
       " 'dancing': 855,\n",
       " 'space': 856,\n",
       " 'really,': 857,\n",
       " 'idk': 858,\n",
       " 'noon': 859,\n",
       " 'delete': 860,\n",
       " 'wake': 861,\n",
       " 'profile': 862,\n",
       " 'hv': 863,\n",
       " 'have?': 864,\n",
       " 'ability': 865,\n",
       " 'whts': 866,\n",
       " 'stories': 867,\n",
       " 'irritating': 868,\n",
       " 'knowledge': 869,\n",
       " 'dress': 870,\n",
       " 'responding': 871,\n",
       " 'machine': 872,\n",
       " 'reasons': 873,\n",
       " 'salary': 874,\n",
       " 'distance': 875,\n",
       " 'road': 876,\n",
       " 'hehe': 877,\n",
       " 'event': 878,\n",
       " 'sun': 879,\n",
       " 'web': 880,\n",
       " 'cream': 881,\n",
       " 'blushing': 882,\n",
       " 'dates': 883,\n",
       " 'upset': 884,\n",
       " 'hang': 885,\n",
       " 'hardware': 886,\n",
       " 'breakfast': 887,\n",
       " 'malayalam': 888,\n",
       " 'science': 889,\n",
       " 'words': 890,\n",
       " 'side': 891,\n",
       " 'butter': 892,\n",
       " 'byy': 893,\n",
       " 'piece': 894,\n",
       " 'prefer': 895,\n",
       " 'justin': 896,\n",
       " 'yellow': 897,\n",
       " 'biggest': 898,\n",
       " 'state': 899,\n",
       " 'noo': 900,\n",
       " 'i&apos;m': 901,\n",
       " 'umm': 902,\n",
       " 'bahubali': 903,\n",
       " 'oo': 904,\n",
       " 'tech': 905,\n",
       " 'infinity': 906,\n",
       " 'videos': 907,\n",
       " 'tries': 908,\n",
       " 'chating': 909,\n",
       " 'health': 910,\n",
       " 'ans': 911,\n",
       " 'purpose': 912,\n",
       " 'drawing': 913,\n",
       " 'god': 914,\n",
       " 'boys': 915,\n",
       " 'history': 916,\n",
       " 'describe': 917,\n",
       " 'wbu': 918,\n",
       " 'e': 919,\n",
       " 'frnds': 920,\n",
       " 'found': 921,\n",
       " 'important': 922,\n",
       " 'teddy': 923,\n",
       " 'caring': 924,\n",
       " 'secrets': 925,\n",
       " 'shop': 926,\n",
       " 'hour': 927,\n",
       " 'emojis': 928,\n",
       " 'desperate': 929,\n",
       " 'single?': 930,\n",
       " 'eye': 931,\n",
       " 'sake': 932,\n",
       " 'haa': 933,\n",
       " 'team': 934,\n",
       " 'taste': 935,\n",
       " 'weather': 936,\n",
       " 'season': 937,\n",
       " 'according': 938,\n",
       " 'wherever': 939,\n",
       " 'added': 940,\n",
       " 'norris': 941,\n",
       " 'checking': 942,\n",
       " 'ok?': 943,\n",
       " 'windows': 944,\n",
       " 'boss': 945,\n",
       " 'thn': 946,\n",
       " 'programmer': 947,\n",
       " 'will!': 948,\n",
       " 'shit': 949,\n",
       " 'kid': 950,\n",
       " 'curious': 951,\n",
       " 'shock': 952,\n",
       " 'creepy': 953,\n",
       " 'bout': 954,\n",
       " 'enough': 955,\n",
       " 'eminem': 956,\n",
       " 'smith': 957,\n",
       " 'final': 958,\n",
       " 'what,': 959,\n",
       " 'popular': 960,\n",
       " 'sooo': 961,\n",
       " 'nightcore': 962,\n",
       " 'is?': 963,\n",
       " 'size': 964,\n",
       " 'energy': 965,\n",
       " 'saving': 966,\n",
       " 'uhh': 967,\n",
       " 'upto': 968,\n",
       " 'hide': 969,\n",
       " 'on?': 970,\n",
       " 'me,': 971,\n",
       " 'tonight': 972,\n",
       " 'haha,': 973,\n",
       " 'masters': 974,\n",
       " 'others': 975,\n",
       " 'close': 976,\n",
       " 'internet': 977,\n",
       " 'pick': 978,\n",
       " 'where?': 979,\n",
       " 'date?': 980,\n",
       " 'calls': 981,\n",
       " 'programme': 982,\n",
       " 'completely': 983,\n",
       " 'now??': 984,\n",
       " 'planned': 985,\n",
       " 'thanx': 986,\n",
       " 'non': 987,\n",
       " 'long?': 988,\n",
       " 'white': 989,\n",
       " 'everytime': 990,\n",
       " 'friend?': 991,\n",
       " 'use?': 992,\n",
       " 'doctor': 993,\n",
       " 'building': 994,\n",
       " 'nowadays': 995,\n",
       " 'university': 996,\n",
       " 'weeks': 997,\n",
       " 'without': 998,\n",
       " 'knew': 999,\n",
       " 'nyc': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer, word_index = tokenize(data[\"turn1\"].append(data[\"turn2\"]).append(data[\"turn3\"]))\n",
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[   0,    0,    0, ...,    1,   28,   41],\n",
       "        [   0,    0,    0, ..., 1739,   94, 1740],\n",
       "        [   0,    0,    0, ...,    1,   98,   74],\n",
       "        ...,\n",
       "        [   0,    0,    0, ...,    1,   43,  615],\n",
       "        [   0,    0,    0, ...,    0,    1,  510],\n",
       "        [   0,    0,    0, ...,    0,    1,    4]], dtype=int32),\n",
       " array([[  0,   0,   0, ..., 254,  29,  39],\n",
       "        [  0,   0,   0, ...,   0,   1,  94],\n",
       "        [  0,   0,   0, ...,   0,   1,  74],\n",
       "        ...,\n",
       "        [  0,   0,   0, ...,   1, 189, 538],\n",
       "        [  0,   0,   0, ...,   1, 691, 595],\n",
       "        [  0,   0,   0, ...,  36,  30, 795]], dtype=int32),\n",
       " array([[   0,    0,    0, ...,   59,  254,   73],\n",
       "        [   0,    0,    0, ...,    1,  202, 3913],\n",
       "        [   0,    0,    0, ...,    0,    1,  377],\n",
       "        ...,\n",
       "        [   0,    0,    0, ...,    1,    2,  615],\n",
       "        [   0,    0,    0, ...,    2,   39,   83],\n",
       "        [   0,    0,    0, ...,    0,    1,  590]], dtype=int32))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[seq_turn_1, seq_turn_2, seq_turn_3] = pad_sequences([to_sequences(tokenizer, data[turn]) for turn in [\"turn1\", \"turn2\", \"turn3\"]])\n",
    "seq_turn_1, seq_turn_2, seq_turn_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = load_embeddings_matrix(\"../GoogleNews-vectors-negative300.bin\", word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(predictions, ground):\n",
    "\n",
    "    \"\"\"Given predicted labels and the respective ground truth labels, display some metrics\n",
    "    Input: shape [# of samples, NUM_CLASSES]\n",
    "        predictions : Model output. Every row has 4 decimal values, with the highest belonging to the predicted class\n",
    "        ground : Ground truth labels, converted to one-hot encodings. A sample belonging to Happy class will be [0, 1, 0, 0]\n",
    "    Output:\n",
    "        accuracy : Average accuracy\n",
    "        microPrecision : Precision calculated on a micro level. Ref - https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin/16001\n",
    "        microRecall : Recall calculated on a micro level\n",
    "        microF1 : Harmonic mean of microPrecision and microRecall. Higher value implies better classification  \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # [0.1, 0.3 , 0.2, 0.1] -> [0, 1, 0, 0]\n",
    "    discretePredictions = to_categorical(predictions.argmax(axis=1))\n",
    "    truePositives = np.sum(discretePredictions*ground, axis=0)\n",
    "    falsePositives = np.sum(np.clip(discretePredictions - ground, 0, 1), axis=0)\n",
    "    falseNegatives = np.sum(np.clip(ground-discretePredictions, 0, 1), axis=0)\n",
    "\n",
    "    print(\"True Positives per class : \", truePositives)\n",
    "    print(\"False Positives per class : \", falsePositives)\n",
    "    print(\"False Negatives per class : \", falseNegatives)    \n",
    "\n",
    "    # ------------- Macro level calculation ---------------\n",
    "\n",
    "    macroPrecision = 0\n",
    "    macroRecall = 0\n",
    "\n",
    "    # We ignore the \"Others\" class during the calculation of Precision, Recall and F1\n",
    "    for c in range(1, NUM_CLASSES):\n",
    "        precision = truePositives[c] / (truePositives[c] + falsePositives[c])\n",
    "        macroPrecision += precision\n",
    "        recall = truePositives[c] / (truePositives[c] + falseNegatives[c])\n",
    "        macroRecall += recall\n",
    "        f1 = ( 2 * recall * precision ) / (precision + recall) if (precision+recall) > 0 else 0\n",
    "        print(\"Class %s : Precision : %.3f, Recall : %.3f, F1 : %.3f\" % ([\"angry\", \"happy\", \"sad\", \"others\"][c], precision, recall, f1))\n",
    "    \n",
    "    macroPrecision /= 3\n",
    "    macroRecall /= 3\n",
    "    macroF1 = (2 * macroRecall * macroPrecision ) / (macroPrecision + macroRecall) if (macroPrecision+macroRecall) > 0 else 0\n",
    "    print(\"Ignoring the Others class, Macro Precision : %.4f, Macro Recall : %.4f, Macro F1 : %.4f\" % (macroPrecision, macroRecall, macroF1))   \n",
    "\n",
    "    # ------------- Micro level calculation ---------------\n",
    "\n",
    "    truePositives = truePositives[1:].sum()\n",
    "    falsePositives = falsePositives[1:].sum()\n",
    "    falseNegatives = falseNegatives[1:].sum()        \n",
    "\n",
    "    print(\"Ignoring the Others class, Micro TP : %d, FP : %d, FN : %d\" % (truePositives, falsePositives, falseNegatives))\n",
    "\n",
    "    microPrecision = truePositives / (truePositives + falsePositives)\n",
    "    microRecall = truePositives / (truePositives + falseNegatives)    \n",
    "\n",
    "    microF1 = ( 2 * microRecall * microPrecision ) / (microPrecision + microRecall) if (microPrecision+microRecall) > 0 else 0\n",
    "    print(\"Ignoring the Others class, Micro Precision : %.4f, Micro Recall : %.4f, Micro F1 : %.4f\" % (microPrecision, microRecall, microF1))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "\n",
    "class TestCallback(Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        r = self.model.predict(x)\n",
    "        # Evaluation return nothing\n",
    "        evaluation(r, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import LSTM, Dropout, Dense, Bidirectional,  Flatten, Input, GRU, Concatenate, Reshape\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCES_LENGTH = seq_turn_1.shape[1]\n",
    "EMBEDDING_DIM = embedding_matrix.shape[1]\n",
    "NUM_CLASSES = len(data[\"label\"].unique()) \n",
    "DROPOUT = 0.2\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main_input1 (InputLayer)        (None, 21)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "main_input2 (InputLayer)        (None, 21)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "main_input3 (InputLayer)        (None, 21)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 21, 300)      1392600     main_input1[0][0]                \n",
      "                                                                 main_input2[0][0]                \n",
      "                                                                 main_input3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 600)          1442400     embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "                                                                 embedding_1[2][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1800)         0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_1[1][0]            \n",
      "                                                                 bidirectional_1[2][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 4)            7204        concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 2,842,204\n",
      "Trainable params: 1,449,604\n",
      "Non-trainable params: 1,392,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model_1():\n",
    "    turn1 = Input(shape=(MAX_SEQUENCES_LENGTH,), dtype='int32', name='main_input1')\n",
    "    turn2 = Input(shape=(MAX_SEQUENCES_LENGTH,), dtype='int32', name='main_input2')\n",
    "    turn3 = Input(shape=(MAX_SEQUENCES_LENGTH,), dtype='int32', name='main_input3')\n",
    "    embedding_layer = Embedding(\n",
    "        embedding_matrix.shape[0],\n",
    "        EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_SEQUENCES_LENGTH,\n",
    "        trainable=False\n",
    "    )\n",
    "    emb1 = embedding_layer(turn1)\n",
    "    emb2 = embedding_layer(turn2)\n",
    "    emb3 = embedding_layer(turn3)\n",
    "    lstm = Bidirectional(LSTM(EMBEDDING_DIM, dropout=DROPOUT))\n",
    "    lstm1 = lstm(emb1)\n",
    "    lstm2 = lstm(emb2)\n",
    "    lstm3 = lstm(emb3)\n",
    "    inp = Concatenate(axis=-1)([lstm1, lstm2, lstm3])\n",
    "    out = Dense(NUM_CLASSES, activation='softmax')(inp)\n",
    "    adam = optimizers.adam(lr=LEARNING_RATE)\n",
    "    model1 = Model([turn1, turn2, turn3], out)\n",
    "    model1.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['acc'])\n",
    "    model1.summary()\n",
    "\n",
    "    return model1\n",
    "\n",
    "model1 = create_model_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main_input1 (InputLayer)        (None, 21)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "main_input2 (InputLayer)        (None, 21)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "main_input3 (InputLayer)        (None, 21)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 21, 300)      1392600     main_input1[0][0]                \n",
      "                                                                 main_input2[0][0]                \n",
      "                                                                 main_input3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 600)          1442400     embedding_2[0][0]                \n",
      "                                                                 embedding_2[1][0]                \n",
      "                                                                 embedding_2[2][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1800)         0           bidirectional_2[0][0]            \n",
      "                                                                 bidirectional_2[1][0]            \n",
      "                                                                 bidirectional_2[2][0]            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 3, 600)       0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 300)          1081200     reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 4)            1204        lstm_3[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 3,917,404\n",
      "Trainable params: 2,524,804\n",
      "Non-trainable params: 1,392,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model_2():\n",
    "    turn1 = Input(shape=(MAX_SEQUENCES_LENGTH,), dtype='int32', name='main_input1')\n",
    "    turn2 = Input(shape=(MAX_SEQUENCES_LENGTH,), dtype='int32', name='main_input2')\n",
    "    turn3 = Input(shape=(MAX_SEQUENCES_LENGTH,), dtype='int32', name='main_input3')\n",
    "    embedding_layer = Embedding(\n",
    "        embedding_matrix.shape[0],\n",
    "        EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_SEQUENCES_LENGTH,\n",
    "        trainable=False\n",
    "    )\n",
    "    emb1 = embedding_layer(turn1)\n",
    "    emb2 = embedding_layer(turn2)\n",
    "    emb3 = embedding_layer(turn3)\n",
    "    lstm = Bidirectional(LSTM(EMBEDDING_DIM, dropout=DROPOUT))\n",
    "    lstm1 = lstm(emb1)\n",
    "    lstm2 = lstm(emb2)\n",
    "    lstm3 = lstm(emb3)\n",
    "    inp = Concatenate(axis=-1)([lstm1, lstm2, lstm3])\n",
    "    inp = Reshape((3, 2 * EMBEDDING_DIM))(inp)\n",
    "    lstm_up = LSTM(EMBEDDING_DIM, dropout=DROPOUT)\n",
    "    out = lstm_up(inp)\n",
    "    out = Dense(NUM_CLASSES, activation='softmax')(out)\n",
    "    adam = optimizers.adam(lr=LEARNING_RATE)\n",
    "    model2 = Model([turn1, turn2, turn3], out)\n",
    "    model2.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['acc'])\n",
    "    model2.summary()\n",
    "\n",
    "    return model1\n",
    "\n",
    "model2 = create_model_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One hotting all the label\n",
    "Y = to_categorical(data[\"label\"], NUM_CLASSES)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.]], dtype=float32), array([[0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        ...,\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.]], dtype=float32), [array([[   0,    0,    0, ...,    1,   28,   41],\n",
       "         [   0,    0,    0, ..., 1739,   94, 1740],\n",
       "         [   0,    0,    0, ...,    1,   98,   74],\n",
       "         ...,\n",
       "         [   0,    0,    0, ...,    3, 1355,  271],\n",
       "         [   0,    0,    0, ...,    0,    1, 2359],\n",
       "         [   0,    0,    0, ...,    1,   17,  376]], dtype=int32),\n",
       "  array([[   0,    0,    0, ...,  254,   29,   39],\n",
       "         [   0,    0,    0, ...,    0,    1,   94],\n",
       "         [   0,    0,    0, ...,    0,    1,   74],\n",
       "         ...,\n",
       "         [   0,    0,    0, ...,    0,    1,  519],\n",
       "         [   0,    0,    0, ..., 1045,  552,   18],\n",
       "         [   0,    0,    0, ...,    0,    1,  268]], dtype=int32),\n",
       "  array([[   0,    0,    0, ...,   59,  254,   73],\n",
       "         [   0,    0,    0, ...,    1,  202, 3913],\n",
       "         [   0,    0,    0, ...,    0,    1,  377],\n",
       "         ...,\n",
       "         [   0,    0,    0, ...,   10,  342, 1355],\n",
       "         [   0,    0,    0, ...,    1,  552, 4501],\n",
       "         [   0,    0,    0, ...,    0,    1,   48]], dtype=int32)], [array([[   0,    0,    0, ...,    1, 1356,  515],\n",
       "         [   0,    0,    0, ...,    0,    1,  988],\n",
       "         [   0,    0,    0, ...,  921,  128,   83],\n",
       "         ...,\n",
       "         [   0,    0,    0, ...,    1,   43,  615],\n",
       "         [   0,    0,    0, ...,    0,    1,  510],\n",
       "         [   0,    0,    0, ...,    0,    1,    4]], dtype=int32),\n",
       "  array([[   0,    0,    0, ...,    0,    1,  125],\n",
       "         [   0,    0,    0, ...,  250,  265, 1413],\n",
       "         [   0,    0,    0, ...,    0,    1, 3640],\n",
       "         ...,\n",
       "         [   0,    0,    0, ...,    1,  189,  538],\n",
       "         [   0,    0,    0, ...,    1,  691,  595],\n",
       "         [   0,    0,    0, ...,   36,   30,  795]], dtype=int32),\n",
       "  array([[   0,    0,    0, ...,  163, 4502,  210],\n",
       "         [   0,    0,    0, ...,    2,    2, 1069],\n",
       "         [   0,    0,    0, ...,    0,    1,  698],\n",
       "         ...,\n",
       "         [   0,    0,    0, ...,    1,    2,  615],\n",
       "         [   0,    0,    0, ...,    2,   39,   83],\n",
       "         [   0,    0,    0, ...,    0,    1,  590]], dtype=int32)])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting as test, train dataset\n",
    "test_ratio = 0.20\n",
    "split_index = int(len(Y) * (1 - test_ratio))\n",
    "\n",
    "Y_train = Y[:split_index]\n",
    "Y_val= Y[split_index:]\n",
    "\n",
    "X_train = [seq_turn_1[:split_index], seq_turn_2[:split_index], seq_turn_3[:split_index]]\n",
    "X_val = [seq_turn_1[split_index:], seq_turn_2[split_index:], seq_turn_3[split_index:]]\n",
    "\n",
    "Y_train, Y_val, X_train, X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2204 samples, validate on 551 samples\n",
      "Epoch 1/6\n",
      "2204/2204 [==============================] - 16s 7ms/step - loss: 0.7028 - acc: 0.8330 - val_loss: 0.5675 - val_acc: 0.8530\n",
      "True Positives per class :  [  0.   0.   0. 470.]\n",
      "False Positives per class :  [ 0.  0.  0. 81.]\n",
      "False Negatives per class :  [24. 22. 35.  0.]\n",
      "Class happy : Precision : nan, Recall : 0.000, F1 : 0.000\n",
      "Class sad : Precision : nan, Recall : 0.000, F1 : 0.000\n",
      "Class others : Precision : 0.853, Recall : 1.000, F1 : 0.921\n",
      "Ignoring the Others class, Macro Precision : nan, Macro Recall : 0.3333, Macro F1 : 0.0000\n",
      "Ignoring the Others class, Micro TP : 470, FP : 81, FN : 57\n",
      "Ignoring the Others class, Micro Precision : 0.8530, Micro Recall : 0.8918, Micro F1 : 0.8720\n",
      "Epoch 2/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/ipykernel_launcher.py:32: RuntimeWarning: invalid value encountered in float_scalars\n",
      "/usr/lib/python3.7/site-packages/ipykernel_launcher.py:36: RuntimeWarning: invalid value encountered in greater\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2204/2204 [==============================] - 15s 7ms/step - loss: 0.5057 - acc: 0.8544 - val_loss: 0.5071 - val_acc: 0.8548\n",
      "True Positives per class :  [  1.   0.   0. 470.]\n",
      "False Positives per class :  [ 0.  0.  0. 80.]\n",
      "False Negatives per class :  [23. 22. 35.  0.]\n",
      "Class happy : Precision : nan, Recall : 0.000, F1 : 0.000\n",
      "Class sad : Precision : nan, Recall : 0.000, F1 : 0.000\n",
      "Class others : Precision : 0.855, Recall : 1.000, F1 : 0.922\n",
      "Ignoring the Others class, Macro Precision : nan, Macro Recall : 0.3333, Macro F1 : 0.0000\n",
      "Ignoring the Others class, Micro TP : 470, FP : 80, FN : 57\n",
      "Ignoring the Others class, Micro Precision : 0.8545, Micro Recall : 0.8918, Micro F1 : 0.8728\n",
      "Epoch 3/6\n",
      "2204/2204 [==============================] - 15s 7ms/step - loss: 0.4305 - acc: 0.8680 - val_loss: 0.4089 - val_acc: 0.8875\n",
      "True Positives per class :  [ 15.   4.   7. 463.]\n",
      "False Positives per class :  [ 9.  0.  0. 53.]\n",
      "False Negatives per class :  [ 9. 18. 28.  7.]\n",
      "Class happy : Precision : 1.000, Recall : 0.182, F1 : 0.308\n",
      "Class sad : Precision : 1.000, Recall : 0.200, F1 : 0.333\n",
      "Class others : Precision : 0.897, Recall : 0.985, F1 : 0.939\n",
      "Ignoring the Others class, Macro Precision : 0.9658, Macro Recall : 0.4556, Macro F1 : 0.6192\n",
      "Ignoring the Others class, Micro TP : 474, FP : 53, FN : 53\n",
      "Ignoring the Others class, Micro Precision : 0.8994, Micro Recall : 0.8994, Micro F1 : 0.8994\n",
      "Epoch 4/6\n",
      "2204/2204 [==============================] - 15s 7ms/step - loss: 0.3680 - acc: 0.8848 - val_loss: 0.3776 - val_acc: 0.9002\n",
      "True Positives per class :  [ 17.   5.  12. 462.]\n",
      "False Positives per class :  [ 7.  1.  1. 46.]\n",
      "False Negatives per class :  [ 7. 17. 23.  8.]\n",
      "Class happy : Precision : 0.833, Recall : 0.227, F1 : 0.357\n",
      "Class sad : Precision : 0.923, Recall : 0.343, F1 : 0.500\n",
      "Class others : Precision : 0.909, Recall : 0.983, F1 : 0.945\n",
      "Ignoring the Others class, Macro Precision : 0.8886, Macro Recall : 0.5177, Macro F1 : 0.6542\n",
      "Ignoring the Others class, Micro TP : 479, FP : 48, FN : 48\n",
      "Ignoring the Others class, Micro Precision : 0.9089, Micro Recall : 0.9089, Micro F1 : 0.9089\n",
      "Epoch 5/6\n",
      "2204/2204 [==============================] - 15s 7ms/step - loss: 0.3257 - acc: 0.8993 - val_loss: 0.3525 - val_acc: 0.9020\n",
      "True Positives per class :  [ 17.   8.  12. 460.]\n",
      "False Positives per class :  [ 6.  3.  3. 42.]\n",
      "False Negatives per class :  [ 7. 14. 23. 10.]\n",
      "Class happy : Precision : 0.727, Recall : 0.364, F1 : 0.485\n",
      "Class sad : Precision : 0.800, Recall : 0.343, F1 : 0.480\n",
      "Class others : Precision : 0.916, Recall : 0.979, F1 : 0.947\n",
      "Ignoring the Others class, Macro Precision : 0.8145, Macro Recall : 0.5617, Macro F1 : 0.6649\n",
      "Ignoring the Others class, Micro TP : 480, FP : 48, FN : 47\n",
      "Ignoring the Others class, Micro Precision : 0.9091, Micro Recall : 0.9108, Micro F1 : 0.9100\n",
      "Epoch 6/6\n",
      "2204/2204 [==============================] - 16s 7ms/step - loss: 0.2807 - acc: 0.9047 - val_loss: 0.3535 - val_acc: 0.8984\n",
      "True Positives per class :  [ 17.   7.   8. 463.]\n",
      "False Positives per class :  [ 6.  2.  0. 48.]\n",
      "False Negatives per class :  [ 7. 15. 27.  7.]\n",
      "Class happy : Precision : 0.778, Recall : 0.318, F1 : 0.452\n",
      "Class sad : Precision : 1.000, Recall : 0.229, F1 : 0.372\n",
      "Class others : Precision : 0.906, Recall : 0.985, F1 : 0.944\n",
      "Ignoring the Others class, Macro Precision : 0.8946, Macro Recall : 0.5106, Macro F1 : 0.6502\n",
      "Ignoring the Others class, Micro TP : 478, FP : 50, FN : 49\n",
      "Ignoring the Others class, Micro Precision : 0.9053, Micro Recall : 0.9070, Micro F1 : 0.9062\n"
     ]
    }
   ],
   "source": [
    "history = model1.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    validation_data=(X_val, Y_val),\n",
    "    epochs=6,\n",
    "    batch_size=50,\n",
    "    callbacks=[TestCallback((X_val, Y_val))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2204 samples, validate on 551 samples\n",
      "Epoch 1/6\n",
      "2204/2204 [==============================] - 15s 7ms/step - loss: 0.2442 - acc: 0.9206 - val_loss: 0.3663 - val_acc: 0.8929\n",
      "True Positives per class :  [ 16.   7.  15. 454.]\n",
      "False Positives per class :  [ 7.  8.  2. 42.]\n",
      "False Negatives per class :  [ 8. 15. 20. 16.]\n",
      "Class happy : Precision : 0.467, Recall : 0.318, F1 : 0.378\n",
      "Class sad : Precision : 0.882, Recall : 0.429, F1 : 0.577\n",
      "Class others : Precision : 0.915, Recall : 0.966, F1 : 0.940\n",
      "Ignoring the Others class, Macro Precision : 0.7548, Macro Recall : 0.5709, Macro F1 : 0.6501\n",
      "Ignoring the Others class, Micro TP : 476, FP : 52, FN : 51\n",
      "Ignoring the Others class, Micro Precision : 0.9015, Micro Recall : 0.9032, Micro F1 : 0.9024\n",
      "Epoch 2/6\n",
      "2204/2204 [==============================] - 15s 7ms/step - loss: 0.2306 - acc: 0.9251 - val_loss: 0.3953 - val_acc: 0.8929\n",
      "True Positives per class :  [ 14.   2.   8. 468.]\n",
      "False Positives per class :  [ 2.  0.  0. 57.]\n",
      "False Negatives per class :  [10. 20. 27.  2.]\n",
      "Class happy : Precision : 1.000, Recall : 0.091, F1 : 0.167\n",
      "Class sad : Precision : 1.000, Recall : 0.229, F1 : 0.372\n",
      "Class others : Precision : 0.891, Recall : 0.996, F1 : 0.941\n",
      "Ignoring the Others class, Macro Precision : 0.9638, Macro Recall : 0.4384, Macro F1 : 0.6027\n",
      "Ignoring the Others class, Micro TP : 478, FP : 57, FN : 49\n",
      "Ignoring the Others class, Micro Precision : 0.8935, Micro Recall : 0.9070, Micro F1 : 0.9002\n",
      "Epoch 3/6\n",
      "2204/2204 [==============================] - 16s 7ms/step - loss: 0.1992 - acc: 0.9333 - val_loss: 0.3763 - val_acc: 0.8875\n",
      "True Positives per class :  [ 17.   6.   8. 458.]\n",
      "False Positives per class :  [ 4.  7.  1. 50.]\n",
      "False Negatives per class :  [ 7. 16. 27. 12.]\n",
      "Class happy : Precision : 0.462, Recall : 0.273, F1 : 0.343\n",
      "Class sad : Precision : 0.889, Recall : 0.229, F1 : 0.364\n",
      "Class others : Precision : 0.902, Recall : 0.974, F1 : 0.937\n",
      "Ignoring the Others class, Macro Precision : 0.7507, Macro Recall : 0.4919, Macro F1 : 0.5944\n",
      "Ignoring the Others class, Micro TP : 472, FP : 58, FN : 55\n",
      "Ignoring the Others class, Micro Precision : 0.8906, Micro Recall : 0.8956, Micro F1 : 0.8931\n",
      "Epoch 4/6\n",
      "2204/2204 [==============================] - 15s 7ms/step - loss: 0.1687 - acc: 0.9469 - val_loss: 0.4059 - val_acc: 0.8929\n",
      "True Positives per class :  [ 15.   6.  12. 459.]\n",
      "False Positives per class :  [ 4.  7.  0. 48.]\n",
      "False Negatives per class :  [ 9. 16. 23. 11.]\n",
      "Class happy : Precision : 0.462, Recall : 0.273, F1 : 0.343\n",
      "Class sad : Precision : 1.000, Recall : 0.343, F1 : 0.511\n",
      "Class others : Precision : 0.905, Recall : 0.977, F1 : 0.940\n",
      "Ignoring the Others class, Macro Precision : 0.7890, Macro Recall : 0.5307, Macro F1 : 0.6346\n",
      "Ignoring the Others class, Micro TP : 477, FP : 55, FN : 50\n",
      "Ignoring the Others class, Micro Precision : 0.8966, Micro Recall : 0.9051, Micro F1 : 0.9008\n",
      "Epoch 5/6\n",
      "2204/2204 [==============================] - 16s 7ms/step - loss: 0.1491 - acc: 0.9501 - val_loss: 0.4299 - val_acc: 0.8820\n",
      "True Positives per class :  [ 16.   7.   6. 457.]\n",
      "False Positives per class :  [ 8.  7.  0. 50.]\n",
      "False Negatives per class :  [ 8. 15. 29. 13.]\n",
      "Class happy : Precision : 0.500, Recall : 0.318, F1 : 0.389\n",
      "Class sad : Precision : 1.000, Recall : 0.171, F1 : 0.293\n",
      "Class others : Precision : 0.901, Recall : 0.972, F1 : 0.936\n",
      "Ignoring the Others class, Macro Precision : 0.8005, Macro Recall : 0.4873, Macro F1 : 0.6058\n",
      "Ignoring the Others class, Micro TP : 470, FP : 57, FN : 57\n",
      "Ignoring the Others class, Micro Precision : 0.8918, Micro Recall : 0.8918, Micro F1 : 0.8918\n",
      "Epoch 6/6\n",
      "2204/2204 [==============================] - 16s 7ms/step - loss: 0.1430 - acc: 0.9560 - val_loss: 0.4532 - val_acc: 0.8893\n",
      "True Positives per class :  [ 17.   8.   6. 459.]\n",
      "False Positives per class :  [ 6.  7.  0. 48.]\n",
      "False Negatives per class :  [ 7. 14. 29. 11.]\n",
      "Class happy : Precision : 0.533, Recall : 0.364, F1 : 0.432\n",
      "Class sad : Precision : 1.000, Recall : 0.171, F1 : 0.293\n",
      "Class others : Precision : 0.905, Recall : 0.977, F1 : 0.940\n",
      "Ignoring the Others class, Macro Precision : 0.8129, Macro Recall : 0.5039, Macro F1 : 0.6221\n",
      "Ignoring the Others class, Micro TP : 473, FP : 55, FN : 54\n",
      "Ignoring the Others class, Micro Precision : 0.8958, Micro Recall : 0.8975, Micro F1 : 0.8967\n"
     ]
    }
   ],
   "source": [
    "history = model2.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    validation_data=(X_val, Y_val),\n",
    "    epochs=6,\n",
    "    batch_size=50,\n",
    "    callbacks=[TestCallback((X_val, Y_val))]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_index_classes(Y_pred):\n",
    "    return Y_pred.argmax(axis=1)\n",
    "\n",
    "def index_to_classes(Y_pred_index):\n",
    "    return np.vectorize(lambda index: CLASSES[index])(Y_pred_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['angry', 'others', 'others', ..., 'others', 'others', 'others'],\n",
       "       dtype='<U6'),\n",
       " array(['angry', 'others', 'others', ..., 'others', 'others', 'others'],\n",
       "       dtype='<U6'))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = pd.read_csv(\"train.txt\", sep=\"\\t\")\n",
    "\n",
    "# Transform all the test data\n",
    "data_test = transform_text(data_test)\n",
    "[seq_test_turn_1, seq_test_turn_2, seq_test_turn_3] = pad_sequences([to_sequences(tokenizer, data_test[turn]) for turn in [\"turn1\", \"turn2\", \"turn3\"]])\n",
    "X_test = [seq_test_turn_1, seq_test_turn_2, seq_test_turn_3]\n",
    "\n",
    "# Make the prediction\n",
    "Y_pred_test_1, Y_pred_test_2 = model1.predict(X_test), model2.predict(X_test)\n",
    "\n",
    "# Get back the predicted classes\n",
    "Y_pred_test_1 = index_to_classes(to_index_classes(Y_pred_test_1))\n",
    "Y_pred_test_2 = index_to_classes(to_index_classes(Y_pred_test_2))\n",
    "\n",
    "Y_pred_test_1, Y_pred_test_2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
